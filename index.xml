<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zero.xu blog</title>
    <link>https://xujianhai.fun/</link>
    <description>Recent content on zero.xu blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 07 Mar 2020 09:35:43 +0800</lastBuildDate>
    
	<atom:link href="https://xujianhai.fun/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Rocketmq Heartbeat Timeout</title>
      <link>https://xujianhai.fun/posts/rocketmq-heartbeat-timeout/</link>
      <pubDate>Sat, 07 Mar 2020 09:35:43 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq-heartbeat-timeout/</guid>
      <description>背景 最近线上发现了一些报警: &amp;ldquo;send heart beat to broker add: xxx error: request timeout&amp;rdquo;, 同时伴随着服务重启, 会出现consumer 流量短时间降低, 同时 consumer的连接创建也很缓慢
排查 通过关键字匹配, 发现这个是 rocektmq-golang-sdk 的一处错误打印, 是心跳命令请求broker超时的场景下打印的
既然是请求rocketmq超时了, 直接登录到线上rocketmq broker查看负载, 但是通过top执行发现cpu和内存占比都比较正常, 同时 netstat -anp | grep pid 扫描的socket的数量也只有几千个,没有异常点.
没有线索的情况下, 我们继续排查日志内容, 通过 tailf broker.log 一段时间后, 发现有一些类似 &amp;ldquo;event queue size 10000 enough, so drop this event CLOSE&amp;rdquo; 和 &amp;ldquo;event queue size 10000 enough, so drop this event CONNECT&amp;rdquo; 的日志, 同样进行关键字匹配, 发现这段逻辑是 rocketmq 对event的抽象处理, event比如: CONNECT/CLOSE/IDLE/EXCEPTION, 以 CLOSE 为例, 当rocketmq netty server 监听到 主动关闭或者被动关闭 连接的时候, 会实例化一个 CLOSE 类型的 event信息 投递到了 eventQueue, 这个 eventQueue 大小是 1w, 当大小大于 1w 的时候, 就会投递失败 (这里有个坑), eventQueue 投递后的消息是由一个单线程异步处理的, 线程会回调根据注册的listener进行回调, 这一块逻辑参考 NettyRemotingServer#channelInactive、NettyRemotingAbstract#putNettyEvent 和 NettyRemotingAbstract#run, 注册的回调逻辑实现在 ClientHousekeepingService#onChannelClose, 回调都做了什么事情呢?</description>
    </item>
    
    <item>
      <title>Protobuf</title>
      <link>https://xujianhai.fun/posts/protobuf/</link>
      <pubDate>Sat, 11 Jan 2020 15:16:40 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/protobuf/</guid>
      <description>gogo-protobuf 扩展了 protobuf 的使用姿势, 不仅添加了丰富的插件: string/euqla/marshal/unmarshal, 性能上还进行了优化. gogo-protobuf 的插件体系相对于原生的protobuf-go的插件实现(虽然只有一个grpc), 不仅丰富, 而且支持开关. 开关是借助于 描述符中extension 实现的.
gogo-protobuf 因为支持的插件体系比较多, 为此, 将插件分成了几种启用级别, 对外是不同的使用入口. 比如 protoc-gen-gogofast、protoc-gen-gogofaster、protoc-gen-gogoslick. 除了通过不同的入口, 还可以通过不同proto文件的参数定制, 比如 option (gogoproto.gostring_all) = true; 实现给每个message添加string的方法.
补充:  extension  extension 是 proto2 中支持的语法, 在新的pb文件中, 使用了 Any 进行了替代. 更多关于extension可以参考: https://developers.google.com/protocol-buffers/docs/proto#extensions, Any可以参照 https://developers.google.com/protocol-buffers/docs/proto3#any . 但是在 gogo的使用实现中, 还是用 extend 机制, proto2 用extend, proto3 使用本地登记的方式
2.validator
validator插件 https://github.com/mwitkow/go-proto-validators 提供了字段检查的功能, 会根据proto文件生成goalng validator代码文件.
protoc插件  protoc是支持插件的, 比如gogo-out其实就是去找gogo的插件, govalidators_out就是找 govalidators插件
其他深入的点 union group 类型.</description>
    </item>
    
    <item>
      <title>Kip 2018 12</title>
      <link>https://xujianhai.fun/posts/kip-2018-12/</link>
      <pubDate>Wed, 11 Dec 2019 19:49:58 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kip-2018-12/</guid>
      <description> reassignment 限流   摘要: 对 reassignment 的 replication 进行限流, 避免全局限流的导致isr落后的partition 的无法追上。这里的限流是动态的. 以前的通用的限流不进行废弃, 因为存在无isr而且 follower 用光带宽的时候, 这个限制还是需要的。新增加了两个配置: leader.reassignment.throttled.rate 和 follower.reassignment.throttled.rate, 前者是 leader broker 统一限流, 但是因为kafka reassignment 的 partition follower 可以有很多个, leader限流的话需要计算每个followerd的比例, 所以添加了 follower限流 kip: https://cwiki.apache.org/confluence/display/KAFKA/KIP-542%3A+Partition+Reassignment+Throttling  replication quota   摘要: client 的限流是通过延迟实现的, replica原来也有类似的限流 replica.fetch.max.bytes , 不过是 partition 级别的。新的方案, 添加了 LeaderQuotaRate 和 FollowerQuotaRate. kip: https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas  </description>
    </item>
    
    <item>
      <title>Rocketmq Admin</title>
      <link>https://xujianhai.fun/posts/rocketmq-admin/</link>
      <pubDate>Sun, 17 Nov 2019 21:50:30 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq-admin/</guid>
      <description>查看消费进度:
sh mqadmin consumerProgress -g $groupName -n ${ip:port} 查看客户端的连接信息
sh mqadmin consumerConnection -g $group -n ${ip:port} 查看topic状态
sh mqadmin topicStatus -t $topic -n ${ip:port} 按照时间重置offset
sh mqadmin resetOffsetByTime -t $topic -n ${ip:port} -g $group -s $ms 创建topic
sh mqadmin updateTopic -t $topic -n ${ip:port} -c $cluster -r 1 -w 1 -o true </description>
    </item>
    
    <item>
      <title>Kafka Controller Redesign</title>
      <link>https://xujianhai.fun/posts/kafka-controller-redesign/</link>
      <pubDate>Sun, 10 Nov 2019 11:10:24 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-controller-redesign/</guid>
      <description>最近学习 kafka 相关的kip, 发现了一个 kafka controller redesign 的设计的文章, 这里叙述一下:
  kip: https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#
  kip: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Redesign
  里面主要就 kafka controller 当前遇到的问题 进行了总结并提出了 部分解决方案:
 zk异步写入 控制请求和 数据请求使用优先级队列分离 使用 generation 区分 controller -&amp;gt; broker 的请求信息 清晰的代码组织: 逻辑简化收敛 使用单线程的事件处理 简化 controller 的并发实现 (1.1.0)  </description>
    </item>
    
    <item>
      <title>Kafka Group Codereview</title>
      <link>https://xujianhai.fun/posts/kafka-group-codereview/</link>
      <pubDate>Sat, 09 Nov 2019 21:52:25 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-group-codereview/</guid>
      <description>最近学习 kafka-connect 的设计和实现. 其中设计到 group member protocol 的内容. 这里展开学习.
kafka group member 协议 主要参考 AbstractCoordinator 的实现流程 以及 ConsumerCoordinator 的实现.
整体生命流程: 找到一个Node -&amp;gt; find coordinator protocol -&amp;gt; onJoinPrepare(子类) -&amp;gt; join group protocol -&amp;gt; sync group protocol(onJoinLeader 包含了任务分配的结果/follower 空的assignment) -&amp;gt; enable heartbeat -&amp;gt;onJoinComplete(子类处理分配结果). 心跳线程处理 coordinator的网络连接. leader 是 coordinator 选举的. ConsumerCoordinator子类 从上面的流程中可以知道, 继承 AbstractCoordinator 的子类, 需要实现 onJoinPrepare、metadata、onLeavePrepare、performAssignment、onJoinComplete
 onJoinPrepare: 在 eager 模式下, 上次分配的内容全部 revoked; 在 COOPERATIVE 模式下, 只撤回不在定于的 topic 的 partition. metadata: sendJoinGroupRequest使用的数据信息, 用于后面的分配.</description>
    </item>
    
    <item>
      <title>Kafka Group Kip</title>
      <link>https://xujianhai.fun/posts/kafka-group-kip/</link>
      <pubDate>Thu, 07 Nov 2019 09:54:35 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-group-kip/</guid>
      <description>这里主要讨论 kafka group 相关的协议: rebalance, partition 等
https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner
https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol
https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request
https://cwiki.apache.org/confluence/display/KAFKA/KIP-379%3A+Multiple+Consumer+Group+Management
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=89070828
https://cwiki.apache.org/confluence/display/KAFKA/KIP-341%3A+Update+Sticky+Assignor%27s+User+Data+Protocol
https://cwiki.apache.org/confluence/display/KAFKA/KIP-389%3A+Introduce+a+configurable+consumer+group+size+limit
在 group非常大的时候, rebalance 次数就会增加; rebalance 时间取决于最慢的consumer, group 越大, 慢consumer出现的概率就越大. 除此之外, group coordinator 可能多个 group 共享的, 所以彼此会影响. 这个提案中, 提出了 `consumer.group.max.size` 的概念, 对 server端进行了保护. 当有超过数量的member加入, 将会收到 异常. https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request
之前都是 broker 在 收到 joinGroup request 的时候, 返回 uuid 给 client 作为 member.id, 在边缘case中(client不断重启加入), 可能导致内存膨胀. 这个 proposal 中, 就是需要用户手动提交 memebr.id https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances
为了避免rebalance导致 有状态的应用程序的数据迁移. 目前的状态 broker group status: Running -&amp;gt; member JoinGroupRequest -&amp;gt; broker group status: PREPARE_REBALANCE -&amp;gt; broker group status: COMPLETING_REBALANCE -&amp;gt; sync group request (group members) -&amp;gt; SyncGroupResponse (broker send to memebrs) 其中, 第一个加入的 member 就是 group leader.</description>
    </item>
    
    <item>
      <title>Kafka Mirror Review</title>
      <link>https://xujianhai.fun/posts/kafka-mirror-review/</link>
      <pubDate>Sat, 02 Nov 2019 23:33:49 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-mirror-review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Kafka Connect Codereview</title>
      <link>https://xujianhai.fun/posts/kafka-connect-codereview/</link>
      <pubDate>Sat, 02 Nov 2019 23:32:49 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-connect-codereview/</guid>
      <description>kafka 支持两种模式: standalone 和 distributed. 分开来讲.
组件 kafka-connect 主要的组件如下
 Herder: 管理 worker、statusBackingStore、configBackingStore、offsetBackingStore. Connect: kafka-connect 组件的生命周期的管理 statusBackingStore: task/worker 状态变化, 都需要调用这个 offsetBackingStore: 管理offset configBackingStore: 管理配置 ConfigProvider: 管理配置信息  standalone standalone 的实现比较简单, 这里简单说下 生命周期.
生命周期:
流程: ConnectStandalone#main -&amp;gt; StandaloneHerder#putConnectorConfig -&amp;gt; StandaloneHerder#startConnector -&amp;gt; 启动 connector + 启动 task 启动 connector: Worker#startConnector -&amp;gt; WorkerConnector#start (管理connector的生命周期) -&amp;gt; Connector#start 启动task: Worer#startTask -&amp;gt; WorkerTask#run...execute (executorService执行) StatusBackingStore: MemoryStatusBackingStore MemoryConfigBackingStore: MemoryConfigBackingStore standalone的依赖 XXXBackingStore 都是 memory 的实现, 不赘述.
学习 kafka-connect 很喜欢用 callback 参数, 将结果callback 出去, 一定程度上能够增强 并发度</description>
    </item>
    
    <item>
      <title>Kafka Connect Design Kip</title>
      <link>https://xujianhai.fun/posts/kafka-connect-design-kip/</link>
      <pubDate>Sat, 02 Nov 2019 23:31:22 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-connect-design-kip/</guid>
      <description>早期的设计 最早的设计可以看: KIP-26 - Add Kafka Connect framework for data import/export,
通过kip, 可以发现, connect设计的目标是 导入和导出数据. 设计目标:
 只聚焦数据处理 并行度的支持. 能够支持大量数据的拷贝 尽可能提供 准确一次的分发. 管理元数据 设计良好的connector api. 易于扩展 流式和批处理的支持 Strandalone 和 集群的支持  为什么是基于 kafka 构建一套connect, 而不是其他框架呢?
 kafka 本身就有并行度的概念 kafka 本身良好的 容错能力, 使得编码简单 kafka 提供了 准确一次、最多一次、最少一次  除此之外, 其他的框架 本身也是从一个具体的case进行 泛化, 不能很好的利用kafka本身的优势。 学习成本和复杂度很高. 部分依赖于 yarn, 对于大集群是好处, 但是应该是 利用而不是依赖. 最后, 其他框架的技术栈 和 kafka 不匹配.
那么, 为什么 kafka-connect 和 kafka 放在一起呢?
 文档入口友好度 生态化.</description>
    </item>
    
  </channel>
</rss>