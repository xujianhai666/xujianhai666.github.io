<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zero.xu blog</title>
    <link>https://xujianhai.fun/</link>
    <description>Recent content on zero.xu blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 08 Mar 2020 12:00:05 +0800</lastBuildDate>
    
	<atom:link href="https://xujianhai.fun/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Rocketmq_article</title>
      <link>https://xujianhai.fun/posts/rocketmq_article/</link>
      <pubDate>Sun, 08 Mar 2020 12:00:05 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq_article/</guid>
      <description> 这里主要是收集一些比较不错的rocketmq 相关的文章
 使用和优化 rocketmq官方文档的优化使用: irqbalance 关闭、中断聚合、numa: 链接
双机房 源码分析 </description>
    </item>
    
    <item>
      <title>GOMAXPROCS</title>
      <link>https://xujianhai.fun/posts/max_proc/</link>
      <pubDate>Sat, 07 Mar 2020 20:04:32 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/max_proc/</guid>
      <description>这个是年前的一个案例, 通过团队成员解决, 因为比较经典, 还是写下blog进行记录.
 背景 随着业务接入, 服务的集群cpu逐渐上涨到 40%, 有时候流量一段时间上涨, 就会触发cpu 80% 报警, 常规情况下一般是简单扩容就好了, 但是本着cpu优化的角度, 开始进行了profile.
因为我们的实现基于go的, 直接用 go pprof 进行cpu 分析 就可以了. 结果发现, runtime.findrunnable 的cpu占比比较高, 通过搜索发现, 可能是因为 cpu设置的问题. 于是进行了环境变量的打印, golang默认的 cpu 是获取物理机的cpu: 128, 但是我们的服务是部署在 k8s 上的, cpu 的数量应该是通过 MY_CPU_LIMIT 获取所在容器的cpu:16, 中间差了7倍的数量
但是为什么 cpu 数量设置的不正确会影响 cpu 利用率呢？
这个需要讲到 GOMAXPROCS 的参数了, 这个参数规定了 P 的最大数量, 默认取值是 cpu数量, 通过设置 最大并行度(GOMAXPROCS) 为 cpu 数量, 可以充分利用每个cpu, 避免线程切换间的代价. 如果说将 GOMAXPROCS 设置成了128, 首先并行执行go代码的线程数膨胀, 但是由于 k8s 容器对于cpu的约束，导致只有 16个cpu 运行 128个线程 (至少128个, 因为系统调用的线程是不受 GOMAXPROCS 约束的)</description>
    </item>
    
    <item>
      <title>Rocketmq Heartbeat Timeout</title>
      <link>https://xujianhai.fun/posts/rocketmq-heartbeat-timeout/</link>
      <pubDate>Sat, 07 Mar 2020 09:35:43 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq-heartbeat-timeout/</guid>
      <description>背景 最近线上发现了一些报警: &amp;ldquo;send heart beat to broker add: xxx error: request timeout&amp;rdquo;, 同时伴随着服务重启, 会出现consumer 流量短时间降低, 同时 consumer的连接创建也很缓慢
排查 通过关键字匹配, 发现这个是 rocektmq-golang-sdk 的一处错误打印, 是心跳命令请求broker超时的场景下打印的
既然是请求rocketmq超时了, 直接登录到线上rocketmq broker查看负载, 但是通过top执行发现cpu和内存占比都比较正常, 同时 netstat -anp | grep pid 扫描的socket的数量也只有几千个,没有异常点.
没有线索的情况下, 我们继续排查日志内容, 通过 tailf broker.log 一段时间后, 发现有一些类似 &amp;ldquo;event queue size 10000 enough, so drop this event CLOSE&amp;rdquo; 和 &amp;ldquo;event queue size 10000 enough, so drop this event CONNECT&amp;rdquo; 的日志, 同样进行关键字匹配, 发现这段逻辑是 rocketmq 对event的抽象处理, event比如: CONNECT/CLOSE/IDLE/EXCEPTION, 以 CLOSE 为例, 当rocketmq netty server 监听到 主动关闭或者被动关闭 连接的时候, 会实例化一个 CLOSE 类型的 event信息 投递到了 eventQueue, 这个 eventQueue 大小是 1w, 当大小大于 1w 的时候, 就会投递失败 (这里有个坑), eventQueue 投递后的消息是由一个单线程异步处理的, 线程会回调根据注册的listener进行回调, 这一块逻辑参考 NettyRemotingServer#channelInactive、NettyRemotingAbstract#putNettyEvent 和 NettyRemotingAbstract#run, 注册的回调逻辑实现在 ClientHousekeepingService#onChannelClose, 回调都做了什么事情呢?</description>
    </item>
    
    <item>
      <title>Protobuf</title>
      <link>https://xujianhai.fun/posts/protobuf/</link>
      <pubDate>Sat, 11 Jan 2020 15:16:40 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/protobuf/</guid>
      <description>gogo-protobuf 扩展了 protobuf 的使用姿势, 不仅添加了丰富的插件: string/euqla/marshal/unmarshal, 性能上还进行了优化. gogo-protobuf 的插件体系相对于原生的protobuf-go的插件实现(虽然只有一个grpc), 不仅丰富, 而且支持开关. 开关是借助于 描述符中extension 实现的.
gogo-protobuf 因为支持的插件体系比较多, 为此, 将插件分成了几种启用级别, 对外是不同的使用入口. 比如 protoc-gen-gogofast、protoc-gen-gogofaster、protoc-gen-gogoslick. 除了通过不同的入口, 还可以通过不同proto文件的参数定制, 比如 option (gogoproto.gostring_all) = true; 实现给每个message添加string的方法.
补充:  extension  extension 是 proto2 中支持的语法, 在新的pb文件中, 使用了 Any 进行了替代. 更多关于extension可以参考: https://developers.google.com/protocol-buffers/docs/proto#extensions, Any可以参照 https://developers.google.com/protocol-buffers/docs/proto3#any . 但是在 gogo的使用实现中, 还是用 extend 机制, proto2 用extend, proto3 使用本地登记的方式
2.validator
validator插件 https://github.com/mwitkow/go-proto-validators 提供了字段检查的功能, 会根据proto文件生成goalng validator代码文件.
protoc插件  protoc是支持插件的, 比如gogo-out其实就是去找gogo的插件, govalidators_out就是找 govalidators插件
其他深入的点 union group 类型.</description>
    </item>
    
    <item>
      <title>Kip 2018 12</title>
      <link>https://xujianhai.fun/posts/kip-2018-12/</link>
      <pubDate>Wed, 11 Dec 2019 19:49:58 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kip-2018-12/</guid>
      <description> reassignment 限流   摘要: 对 reassignment 的 replication 进行限流, 避免全局限流的导致isr落后的partition 的无法追上。这里的限流是动态的. 以前的通用的限流不进行废弃, 因为存在无isr而且 follower 用光带宽的时候, 这个限制还是需要的。新增加了两个配置: leader.reassignment.throttled.rate 和 follower.reassignment.throttled.rate, 前者是 leader broker 统一限流, 但是因为kafka reassignment 的 partition follower 可以有很多个, leader限流的话需要计算每个followerd的比例, 所以添加了 follower限流 kip: https://cwiki.apache.org/confluence/display/KAFKA/KIP-542%3A+Partition+Reassignment+Throttling  replication quota   摘要: client 的限流是通过延迟实现的, replica原来也有类似的限流 replica.fetch.max.bytes , 不过是 partition 级别的。新的方案, 添加了 LeaderQuotaRate 和 FollowerQuotaRate. kip: https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas  </description>
    </item>
    
    <item>
      <title>Rocketmq Admin</title>
      <link>https://xujianhai.fun/posts/rocketmq-admin/</link>
      <pubDate>Sun, 17 Nov 2019 21:50:30 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq-admin/</guid>
      <description>查看消费进度:
sh mqadmin consumerProgress -g $groupName -n ${ip:port} 查看客户端的连接信息
sh mqadmin consumerConnection -g $group -n ${ip:port} 查看topic状态
sh mqadmin topicStatus -t $topic -n ${ip:port} 按照时间重置offset
sh mqadmin resetOffsetByTime -t $topic -n ${ip:port} -g $group -s $ms 创建topic
sh mqadmin updateTopic -t $topic -n ${ip:port} -c $cluster -r 1 -w 1 -o true </description>
    </item>
    
    <item>
      <title>Kafka Controller Redesign</title>
      <link>https://xujianhai.fun/posts/kafka-controller-redesign/</link>
      <pubDate>Sun, 10 Nov 2019 11:10:24 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-controller-redesign/</guid>
      <description>最近学习 kafka 相关的kip, 发现了一个 kafka controller redesign 的设计的文章, 这里叙述一下:
  kip: https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#
  kip: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Redesign
  里面主要就 kafka controller 当前遇到的问题 进行了总结并提出了 部分解决方案:
 zk异步写入 控制请求和 数据请求使用优先级队列分离 使用 generation 区分 controller -&amp;gt; broker 的请求信息 清晰的代码组织: 逻辑简化收敛 使用单线程的事件处理 简化 controller 的并发实现 (1.1.0)  </description>
    </item>
    
    <item>
      <title>Kafka Group Codereview</title>
      <link>https://xujianhai.fun/posts/kafka-group-codereview/</link>
      <pubDate>Sat, 09 Nov 2019 21:52:25 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-group-codereview/</guid>
      <description>最近学习 kafka-connect 的设计和实现. 其中设计到 group member protocol 的内容. 这里展开学习.
kafka group member 协议 主要参考 AbstractCoordinator 的实现流程 以及 ConsumerCoordinator 的实现.
整体生命流程: 找到一个Node -&amp;gt; find coordinator protocol -&amp;gt; onJoinPrepare(子类) -&amp;gt; join group protocol -&amp;gt; sync group protocol(onJoinLeader 包含了任务分配的结果/follower 空的assignment) -&amp;gt; enable heartbeat -&amp;gt;onJoinComplete(子类处理分配结果). 心跳线程处理 coordinator的网络连接. leader 是 coordinator 选举的. ConsumerCoordinator子类 从上面的流程中可以知道, 继承 AbstractCoordinator 的子类, 需要实现 onJoinPrepare、metadata、onLeavePrepare、performAssignment、onJoinComplete
 onJoinPrepare: 在 eager 模式下, 上次分配的内容全部 revoked; 在 COOPERATIVE 模式下, 只撤回不在定于的 topic 的 partition. metadata: sendJoinGroupRequest使用的数据信息, 用于后面的分配.</description>
    </item>
    
    <item>
      <title>Kafka Group Kip</title>
      <link>https://xujianhai.fun/posts/kafka-group-kip/</link>
      <pubDate>Thu, 07 Nov 2019 09:54:35 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-group-kip/</guid>
      <description>这里主要讨论 kafka group 相关的协议: rebalance, partition 等
https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner
https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol
https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request
https://cwiki.apache.org/confluence/display/KAFKA/KIP-379%3A+Multiple+Consumer+Group+Management
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=89070828
https://cwiki.apache.org/confluence/display/KAFKA/KIP-341%3A+Update+Sticky+Assignor%27s+User+Data+Protocol
https://cwiki.apache.org/confluence/display/KAFKA/KIP-389%3A+Introduce+a+configurable+consumer+group+size+limit
在 group非常大的时候, rebalance 次数就会增加; rebalance 时间取决于最慢的consumer, group 越大, 慢consumer出现的概率就越大. 除此之外, group coordinator 可能多个 group 共享的, 所以彼此会影响. 这个提案中, 提出了 `consumer.group.max.size` 的概念, 对 server端进行了保护. 当有超过数量的member加入, 将会收到 异常. https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request
之前都是 broker 在 收到 joinGroup request 的时候, 返回 uuid 给 client 作为 member.id, 在边缘case中(client不断重启加入), 可能导致内存膨胀. 这个 proposal 中, 就是需要用户手动提交 memebr.id https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances
为了避免rebalance导致 有状态的应用程序的数据迁移. 目前的状态 broker group status: Running -&amp;gt; member JoinGroupRequest -&amp;gt; broker group status: PREPARE_REBALANCE -&amp;gt; broker group status: COMPLETING_REBALANCE -&amp;gt; sync group request (group members) -&amp;gt; SyncGroupResponse (broker send to memebrs) 其中, 第一个加入的 member 就是 group leader.</description>
    </item>
    
    <item>
      <title>Kafka Mirror Review</title>
      <link>https://xujianhai.fun/posts/kafka-mirror-review/</link>
      <pubDate>Sat, 02 Nov 2019 23:33:49 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka-mirror-review/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>