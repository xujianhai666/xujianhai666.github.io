<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zero.xu blog</title>
    <link>https://xujianhai.fun/</link>
    <description>Recent content on zero.xu blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 11 Apr 2020 20:51:59 +0800</lastBuildDate>
    
	<atom:link href="https://xujianhai.fun/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mysql_basic</title>
      <link>https://xujianhai.fun/posts/mysql_basic/</link>
      <pubDate>Sat, 11 Apr 2020 20:51:59 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/mysql_basic/</guid>
      <description>背景 假设一个面试者过来面试, 想要考察mysql方面, 我该怎么考察呢? 从整体上而言, mysql 还是偏基础概念, 比如 事务和锁.
理解与概念 首先需要确保候选人理解以下的基本概念:
 索引  InnoDB支持的几种索引类型:
 hash索引 B+索引 全文索引  B+数中的索引类型:
 聚簇索引: 节点页只包含了索引列，叶子页包含了行的全部数据 覆盖索引: 一个查询语句的执行只用从索引中就能够取得，不必从数据表中读 联合索引: 表上的多个列进行索引. 最左前缀: 最常用的在最左边  慢查询优化经验. 索引没有被使用的情况 遇到过? 如何判断索引生效?
 explain 的使用  事务  什么是事务 以及 事务的四个特征: acid
事务的隔离级别: 读未提交 、读已提交(解决脏读)、可重复读(解决不可重复读)、可串行化(幻读: 插入了新数据)
InnoDB 怎么解决幻读的? mvcc + 间隙锁(gap lock)、next-key lock, mysql的读已提交
什么是gap lock、next-key lock?
当我们用范围条件进行查询, InnoDB 除了给 符合条件的数据的索引加锁, 还会给 在条件范围内但并不存在的记录 进行加锁,
代理  如何自己设计一个代理, 怎么设计?</description>
    </item>
    
    <item>
      <title>Redis_basic</title>
      <link>https://xujianhai.fun/posts/redis_basic/</link>
      <pubDate>Wed, 08 Apr 2020 09:07:46 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/redis_basic/</guid>
      <description>背景 最近和小伙伴重新讨论了下 redis 相关的一些知识点, 发现自己很多已经遗忘了, 今天重新学习下. (最早的redis知识点还是从 “” 学习的)
原理 数据类型/对象 redis支持的数据结构:
 string (sds 采用预分配冗余空间的方式来减少内存的频繁分配, 获取字符串长度也更快) list (双向列表, 可以模拟队列/堆栈; 存储: ziplist, 元素较少使用一块连续的内存空间, 普通列表的附加指针空间大) hash (渐进式rehash, hash碰撞扩容在大数据量的时候耗时很长, 渐进式rehash, 不是立即迁移, 而是在后续的定时任务以及hash结构的读写指令进行迁移, golang sync.map 类似) set: 基于hash实现, value 指向的都是同一个对象 zset/sortedset: hash+跳跃列表, hash 保存的是权重, 跳跃列表给value排序, zrange 的排序方式是 升序, 部分场景是希望最新的排前面, 需要降序 bitmap: bit的操作, 上层 布隆过滤器. 用于验证场景, 签到、是否已经执行. 最大 2^32 Geo: Redis 3.2, 处理地理位置信息 HyperLogLog: 统计计数: 统计uv. 是一种概率数据结构，它使用概率算法来统计集合的近似基数. 参考分析: https://www.zhihu.com/question/53416615 Streams: 流式pub/sub  编码&amp;amp;数据结构 目前支持的编码格式:</description>
    </item>
    
    <item>
      <title>Cpp_basic</title>
      <link>https://xujianhai.fun/posts/cpp_basic/</link>
      <pubDate>Sun, 05 Apr 2020 19:36:11 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/cpp_basic/</guid>
      <description>背景 最近准备写 cpp 的一些项目, 学习下不太了解的领域.
基础 锁 std::mutex 是基础的锁, 但是使用 std::mutex 很容易经常忘记解锁, 而且缺乏一些常用的特性: 延迟加锁、尝试加锁. 为此, cpp提供了 std::unique_lock 和 std::lock_gurad , 都是简化锁的使用, std::local_guard 是通过栈上变量的 构造和析构 解决加锁和解锁逻辑, std::unique_lock 则提供了更多的特性: 延迟锁定、递归锁定、有时限尝试、所有权转移等.
std::scoped_lock: raii风格的互斥包装器, 创建对象的时候 会获取 互斥的所有权, 离开的时候 逆序释放. 相当于两步操作的封装 (std::lock(a, b), std::lock_quard g1(a1, std::adopt_lock), std::lock_gurad g2(a2, std::adopt_lock))
https://zh.cppreference.com/w/cpp/thread/lock https://zh.cppreference.com/w/cpp/thread/mutex https://zh.cppreference.com/w/cpp/thread/lock_guard https://zh.cppreference.com/w/cpp/thread/unique_lock http://www.cplusplus.com/reference/mutex/unique_lock/ https://zh.cppreference.com/w/cpp/thread/scoped_lock
补充, std::lock 是一个函数, 用来对多个互斥量上锁, 还有尝试加锁的函数: std::try_lock 更多线程相关的信息: https://zh.cppreference.com/w/cpp/thread
智能指针 为了避免在处理指针导致指针对象泄露, cpp提供了多种智能指针的处理方式. 除了 auto, 更常见的是 std::shared_pter 和 std::unique_ptr.
std::shared_ptr: 通过指针保持对象共享所有权的智能指针, 多个 shared_ptr 对象可占有同一对象, 当最后的shared_ptr 被销毁或者被赋值为其他指针就会释放、销毁 对象 并释放内存.</description>
    </item>
    
    <item>
      <title>Proxy_sam</title>
      <link>https://xujianhai.fun/posts/proxy_sam/</link>
      <pubDate>Sun, 05 Apr 2020 09:42:10 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/proxy_sam/</guid>
      <description>背景 最近在研究service mesh相关的实现, 除了知名的 envoy-proxy(c++写的), go写的知名mesh就是 mosn, 在饿了么, 也实现了mesh的产品: samaritan
设计 整体设计 samaritan 本质上是一个proxy, 通过 unix domain socket 和应用进程交互. samaritan-proxy 的设计有以下几个重要模块:
 instance: 一个samaritan-proxy就是一个实例 controller: 负责配置信息的变更和处理, 配置变更会启动一个proc Proc: 服务名绑定的 流量处理和转发抽象, 每一个 procName 对应一个Proc. 比如 redis的实现 就是一个 Proc. Proc 独立监听 网络端口和网络包处理 admin: 运维管理接口, 比如 获取配置、stats统计、pprof. admin端口是重用的. config: 动态配置实现, 获取动态配置 放到 event channel, 订阅者监听 event channel 处理配置变更  需要注意的是, proc 的协议相关实现 需要实现注册到 controller#procs 的map中, 注册目前依赖 Bootstrap#StaticServices 的配置信息.
主流程 先关注主流程, 协议启动:
Controller#handleEvent -&amp;gt; handleSvcAdd -&amp;gt; #tryEnsureProc -&amp;gt; newProc(controller.</description>
    </item>
    
    <item>
      <title>Qmq_delay</title>
      <link>https://xujianhai.fun/posts/qmq_delay/</link>
      <pubDate>Fri, 03 Apr 2020 14:02:08 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/qmq_delay/</guid>
      <description>背景 最近在调研延迟消息的设计和研发, 其中, 目前开源并在线上大规模使用的主要是 去哪儿的qmq, 本文主要针对 qmq 中的延迟消息实现进行分析
问题 作为一款延迟消息服务, 需要解决一下问题:
 延迟消息的底层存储 延迟消息的投递方式 延迟消息怎么保证稳定的到期 延迟消息到期怎么投递 延迟消息是否可以取消 主从同步  设计 qmq 的延迟设计中, 所有的消息投递到 message log, message log 相当于 wal, 会有一个IterateOffsetManager 异步读取 message log 构建 schedule log 和 hash wheel, schedule log 是每个目标到期时间的文件, 默认每分钟一个文件, 时间间隔可以定制. hash wheel 是内存的多层时间轮, 所有后续的延迟消息 以及 到期的schedule log文件的消息 都会加载到内存中. 消息到期后, 会投递给 正常的消息服务的broker (投递会目标subject)
底层存储 qmq 本身设计了一套文件存储形式, 延迟功能基于这套存储进行了封装和实现, 除此之外, 延迟功能在内存中也维护了一份 时间轮的消息索引, 这里主要分析文件侧实现. 在延迟消息存储的设计中, 主要是以下模块:
  LogSegment&amp;amp;LogManager: qmq底层文件存储 和 文件目录管理(以及recover) 的实现, 延迟消息和普通消息基于这个做定制化.</description>
    </item>
    
    <item>
      <title>Mesh_file</title>
      <link>https://xujianhai.fun/posts/mesh_file/</link>
      <pubDate>Sat, 28 Mar 2020 23:36:12 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/mesh_file/</guid>
      <description>记录 service mesh 的一些相关资料:
 http://www.360doc.com/content/19/0907/06/46368139_859593650.shtml  </description>
    </item>
    
    <item>
      <title>Rocketmq_nomessage</title>
      <link>https://xujianhai.fun/posts/rocketmq_nomessage/</link>
      <pubDate>Thu, 26 Mar 2020 23:33:04 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq_nomessage/</guid>
      <description>背景 最近在修rocketmq-golang-client的问题的时候, 发现在处理 PullNoNewMsg 的时候会导致 offset 被自动提交, 但是用户并没有设置自动ack, 并且也没有手动ack
注:
rocketmq 开源的版本并没有ack的概念
排查 于是, 通过日志打印调试, 发现是在 rocketmq-client-go 拉取消息处理 primitive.PullNoNewMsg 的状态的时候, 直接将 result.NextBeginOffset 替换为 request.nextOffset, 并且还 更新了本地offsetStore的offset 信息, 因为 rocektmq-client-go 是 周期性提交offset, 所以导致了 offset被ack 了
解决 在rocketmq-client-go的内部开发版本中, 直接将 offset 的本地存储更新给注释掉就可以了, 因为内部开发中, 是异步处理处理消息的, 并且offset的提交不需要满足递增的特性 (考虑到很多场景中可能存在 offset被移动到 更小的情况)
在开源的版本中, 对齐java的实现, 判断 processQueue是否有消息, 如果没有消息, 在更新本地offsetStore, 避免提交了 正在消费的消息
更多的理解 乘这次机会, 重新梳理了 rocketmq 在 pullMessage 的响应逻辑的处理. 根据客户端处理的逻辑, 区分如下 (不涉及到transaction)
1.NO_NEW_MSG
当broker返回 ResponseCode.PULL_NOT_FOUND 的时候, 客户端会转义成 PullStatus.NO_NEW_MSG, 会执行如下操作:</description>
    </item>
    
    <item>
      <title>Rocketmq_search</title>
      <link>https://xujianhai.fun/posts/rocketmq_search/</link>
      <pubDate>Wed, 25 Mar 2020 18:22:56 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq_search/</guid>
      <description>背景 最近基于golang做了 消息查询的功能, 这里做一些记录
原理 rocketmq的消息查询, 支持两种模式: offsetMsgId 和 msgkey、uniqueKey, 我这里避免了 msgid 的命名, 因为在 rocketmq client的实现过程中, msgid 存在很大的差异.
基本概念 offsetMsgId
offsetMsgId 本质上是 rocketmq commitLog 生成的, 生成格式如下:
 public static String createMessageId(final ByteBuffer input, final ByteBuffer addr, final long offset) { input.flip(); int msgIDLength = addr.limit() == 8 ? 16 : 28; input.limit(msgIDLength); input.put(addr); input.putLong(offset); return UtilAll.bytes2string(input.array()); } 可以发现, offsetMsgId 是基于commitLog 所在的地址 + 消息的offset 组成, 保证了 唯一性. 因此通过offsetMsgId 可以借助这个特性.
msgkey
在消息发送的时候, 发送的消息是可以指定消息的key的, 需要注意的是, msgKey可以设置多个.</description>
    </item>
    
    <item>
      <title>Golang_timer</title>
      <link>https://xujianhai.fun/posts/golang_timer/</link>
      <pubDate>Wed, 25 Mar 2020 10:23:01 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/golang_timer/</guid>
      <description>背景 新版发布了新的定时器实现, 声称性能优化了很多. 这里特意记录. 同时, 也偶尔看到有同事写错了相关实现, 这里补充
历史 1.10 之前, 一个独立的timerproc通过小顶堆和futexsleep 管理定时任务 1.10 ~ 1.13: 64个timerproc协程 + 四插堆 1.14: 不再有timerproc, 基于 netpoll的epoll wait来做就近时间的休眠等待, 在每次runtime.findrinnable 调度时都检查运行到期的定时器
基本结构:   p 里面存储了 timers []*timer (runtime2.go), 其他辅助结构 timersLock 和 其他timer状态的计数, 使用 四插堆维护 timer, 使用锁避免插入冲突
  netpoller 支持 超时等待, 这样p可以利用这个特性进行timer的等待
  基本流程 timer 支持 Timer.NewTimer、 Timer.Stop 和 Timer.Reset 三种接口.
下面分析三个接口
  Timer.NewTimer 其实就是 创建了 timer对象, 然后调用了 runtime/time.addtimer 方法, 将timer对象添加到p的timers四插堆, 需要注意的是 这里执行了两个特殊的操作: cleantimers 和 wakeNetPoller, cleantimers 是删除 timer0(最早到期的那个); wakeNetPoller 只有在 新增timer的时间比较早的时候才会触发, 个人猜想是为了 解决 fundrunnable 阻塞在timer的设计 (后面细讲).</description>
    </item>
    
    <item>
      <title>Rocketmq_recover</title>
      <link>https://xujianhai.fun/posts/rocketmq_recover/</link>
      <pubDate>Tue, 24 Mar 2020 23:43:23 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/rocketmq_recover/</guid>
      <description>背景 最近要开发延迟消息, 这里记录下 recover相关的逻辑实现
原理 之前知道, rocketmq是所有的消息统一投递到 commitlog, 然后异步构建 consumer queue, 那么, 如果机器正常重启/异常宕机的情况下, 又是怎么恢复的呢?
前菜 rocketmq 使用了 checkpoint 文件记录了 physicMsgTimestamp logicsMsgTimestamp indexMsgTimestamp 三个字段, 分别表示 commitlog 的flush的时间点、comsumer queue的flush的时间点、index file 刷新的时间点. 也就是 已经落地磁盘的时间点. (通过fileChannel#force)
那么 这些时间点什么场景下会被更新, 什么时候checkpoint会flush呢?
 physicMsgTimestamp  首先, CommitLog 本身既有一个定时flush的任务, 根据flush方式的不同, 有两种实现: GroupCommitService 和 FlushRealTimeService(后面单独分析), 无论是同步还是异步, 每次flush之后都会设置 physicMsgTimestamp.
除此之外, 在 dledger模式中, slave构建 consumer queue的时候 也会设置 physicMsgTimestamp
logicsMsgTimestamp  在定时flush consumer queue 以及 追加consumer queue消息的时候, 都会更新. (因此, logicsMsgTimestamp 并不是 consumer queue flush的时间)</description>
    </item>
    
  </channel>
</rss>