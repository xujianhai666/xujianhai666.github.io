<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zero.xu blog</title>
    <link>https://xujianhai.fun/</link>
    <description>Recent content on zero.xu blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 13 Jun 2020 00:06:10 +0800</lastBuildDate>
    
	<atom:link href="https://xujianhai.fun/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Redis_debug</title>
      <link>https://xujianhai.fun/posts/redis_debug/</link>
      <pubDate>Sat, 13 Jun 2020 00:06:10 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/redis_debug/</guid>
      <description>Preface 最近小伙伴讨论到了如何调试c/cpp 应用, 其中讲到了 compile db 这个组件, 不是很了解, 这里学习下
redis 先用redis调试, 其中主要参考了 https://my.oschina.net/icebergxty/blog/4309023, 整个过程都成功了, 但是需要注意的是, 在创建调试的配置的时候, 选择的是二进制的应用, 而不是 server.c, 虽然选择的是二进制, 但是调试还是可以的, 打的断点可以执行到.
使用 compile db 调试, 避免了 在每个模块添加 CMakeList.txt 的操作, 快速很多
参考的blog是中文的, 可以参考英文官方文档: https://www.jetbrains.com/help/clion/custom-build-targets.html#.
mysql cmake
-DCMAKE_INSTALL_PREFIX=/Users/jianhaixu/secrect/opensource/mysql-server/debug
-DMYSQL_DATADIR=/Users/jianhaixu/secrect/opensource/mysql-server/debug/data
-DSYSCONFDIR=/Users/jianhaixu/secrect/opensource/mysql-server/debug
-DMYSQL_UNIX_ADDR=/Users/jianhaixu/secrect/opensource/mysql-server/debug/data/mysql.sock
-DWITH_DEBUG=1 -DFORCE_INSOURCE_BUILD=1
-DDOWNLOAD_BOOST=1
-DWITH_BOOST=/Users/jianhaixu/Downloads/boost_1_70_0
make -j 4
make install -j 4
bin/mysqld &amp;ndash;initialize-insecure &amp;ndash;user=root &amp;ndash;datadir=/Users/jianhaixu/secrect/opensource/mysql-server/debug/data
bin/mysqld &amp;ndash;defaults-file=/Users/jianhaixu/secrect/opensource/mysql-server/debug/etc/my.cnf</description>
    </item>
    
    <item>
      <title>Python_tool</title>
      <link>https://xujianhai.fun/posts/python_tool/</link>
      <pubDate>Mon, 08 Jun 2020 13:11:08 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/python_tool/</guid>
      <description>火焰图 pyframe https://github.com/uber/pyflame https://pyflame.readthedocs.io/en/latest/
不侵入代码, 支持诊断多线程. 支持 火焰图、线程应用、stack trace. 但是不支持web导出, 需要自己做
vprof https://github.com/nvdv/vprof
支持cpu火焰图、内存火焰图，代码执行时间、web导出, 看上去很丰富
profile_online https://github.com/rfyiamcool/profiler_online
比较简单, 只支持火焰图, 支持web导出
py-spy https://github.com/benfred/py-spy
打印堆栈、火焰图、top</description>
    </item>
    
    <item>
      <title>Mit_6</title>
      <link>https://xujianhai.fun/posts/mit_6.824/</link>
      <pubDate>Sat, 06 Jun 2020 10:23:10 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/mit_6.824/</guid>
      <description>序 记录学习 mit 6.824 课程的经历
MapReduce 1 目标: 将任务拆解成map 和 reduce 两个阶段, 进行 大规模的数据处理, 比如 页面爬取、词频统计
模型 如上图, map/reduce 架构会将用户输入切成若干份数据输入(map的个数), 由map进行处理, 按照论文的说法, map读取文件是本地读取操作, map计算后得到的结果, 会按照 hash到若干份(reduce个数)本地文件存储, 并将存储位置上报给 master, master启动reduce worker, reduce worker会远程获取 每个map机器上的文件, 本地计算后输出到 gfs(分布式文件存储)上. 为了更好的性能和效果, 在map输出后以及reduce输入前, 会有一个combiner任务, 对map的结果进行预处理, 减少网络传输, 但是和reduce不同, combiner 的输出结果是存储在磁盘上的.
分布式场景下, 容易出现一些坏的机器导致map/reduce 执行慢, 对此, map/reduce 架构会重新执行任务. 对于执行完宕机的场景, map会触发重新执行(结果存放在本地), reduce 不需要重新执行(结果存放在gfs)
map/reduce的场景中, 需要处理 热点倾斜的问题, 因为会出现大量数据集中在一台reduce机器上, 对于这种问题, 需要自定义良好的 partition 函数, 将数据尽可能的平均打散
hadoop 架构
将论文中 partition/combiner 的抽象成 shuffle, 进行分区、排序、分割, 将属于同一划分（分区）的输出合并在一起并写在磁盘上，最终得到一个分区有序的文件.</description>
    </item>
    
    <item>
      <title>Kafka_group_coordinator</title>
      <link>https://xujianhai.fun/posts/kafka_group_coordinator/</link>
      <pubDate>Sat, 30 May 2020 21:27:58 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka_group_coordinator/</guid>
      <description>preface 因为工作内容涉及 kafka, 今天梳理下 kafka group coordinator
coordinator coordinator 的主要任务是: 1. consumer rebalance 2. offset 管理
coordinator rebalance 模块比较简单, 之前客户端开发的时候总结过一波. 如下:
客户端找到一个Node -&amp;gt; find coordinator protocol -&amp;gt; onJoinPrepare(子类) -&amp;gt; join group protocol -&amp;gt; sync group protocol(onJoinLeader 包含了任务分配的结果/follower 空的assignment) -&amp;gt; enable heartbeat -&amp;gt;onJoinComplete(子类处理分配结果). 心跳线程处理 coordinator的网络连接. leader 是 coordinator 选举的. 在group coordinator的kafka server端, 主要处理 join group protocol 和 sync group protocol. 在处理join操作的时候, 一个重点就是维护参加 rebalane 的consumer, 为此, Kafka 抽象了 MemberMetadata 表示 member 的状态, MemberMetadata 维护了 joinCallback 和 syncCallback, 这样join/sync结束的时候可以通过 joinCallback 通知consumer.</description>
    </item>
    
    <item>
      <title>Linux_iouring</title>
      <link>https://xujianhai.fun/posts/linux_iouring/</link>
      <pubDate>Tue, 26 May 2020 22:32:44 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/linux_iouring/</guid>
      <description>Preface 最近内部经常讨论 io_uring, 这块不是很了解, 特别记录下
回顾io linux io 模型中主要区分四种类型区分: 同步io 非同步io 和 阻塞io 非阻塞io, 通过不同的组合, 有不同的模型. 如下:
 同步io + 阻塞io: 用户进程会被阻塞在 recvfrom. 同步io + 非阻塞io: recvfrom 会返回 错误表示数据还没有到来, 不会阻塞. O_NONBLOCK 参数 同步io + 阻塞io: io多路复用: select/poll/epoll, 虽然读写不会阻塞在recvfrom, 但是会阻塞在select调用. 信号驱动: 接收到信号之后需要自己操作 异步io: 内核操作完通知. 操作系统提供了 libaio  有人还将 异步io + epoll 进行了实现: http://m.blog.chinaunix.net/uid-16979052-id-3840266.html
io_uring   定位: 更高 IOPS 的 async syscall api. (io层的异步api, 主打高性能)
  特点&amp;amp;主要概念
 基于ringbuffer 的设计, 提交队列和完成队列只存储索引, SQEs(submission queue entries) 存储请求, 这样提交的请求可以内存不连续 用户态和系统态 通过 mmap 共享 提交队列 和 完成队列, 减少地址映射开销 在poll模式下, IO提交和收割 可以由 kernel 完成, 不需要系统调用, 系统会启动一个 SQ Poll 的内核线程不断poll (没有请求会睡眠), 处理 sq 和 cq 非poll模式下, io_uring_enter 会阻塞, 完成 SQ Poll 线程的任务 提供了polling和非polling两种模式, 和底层实现有关, 非polling性能比 libaio 提升不了多少, polling 模式和 SPDK 非常接近.</description>
    </item>
    
    <item>
      <title>Kafka_controller</title>
      <link>https://xujianhai.fun/posts/kafka_controller/</link>
      <pubDate>Tue, 26 May 2020 11:45:58 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka_controller/</guid>
      <description>Preface kafka controller 是 kafka 设计中重要的一环, 负责 kafka 集群状态 、topic 元数据的缓存和管理(topic、replica的管理), 下面分成两部分分析 kafka controller. 一部分是 kafka controller 的redesign 设计, 第二部分是从源代码分析和思考
Redesign 网上关于 redesign(https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko) 翻译的版本很多, 但是在看这个内容之前, 我们需要思考几个问题: 1. 为什么要redesign, 存在什么问题 2. 如何解决这些问题.
从文章内容上来看, 主要存在以下问题:
 每个partition的zk写入是同步, 并发度不够(比如broker宕机导致的 partition leader重新选举、replica摘除) controller-broker 的请求也是每个partition 顺序的 (StopReplicaRequests LeaderAndIsrRequest UpdateMetadataRequest 这些并发量很大的请求, broker宕机的时候触发) 并发管理复杂 (controller-broker channel、zk、kafkaApi 都会操作) controller 代码组织混乱 (replicaStateMachine 和 topicStateMachine 的状态边界分的不是很清晰, 需要一些状态同步) 控制面和数据面没有分离, 控制面的命令不能及时下达, 会导致什么问题呢? (新选举的leader无法及时通知正在忙于处理用户数据的旧的leader, ack=1 和0 的会导致数据丢失) controller-broker 的请求 没有 broker generation, 会导致什么问题呢? (broker收到之前的controller过期的请求) zkClinet 没有状态管理 导致了什么问题?</description>
    </item>
    
    <item>
      <title>Kafka_socket</title>
      <link>https://xujianhai.fun/posts/kafka_socket/</link>
      <pubDate>Sat, 23 May 2020 17:13:38 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka_socket/</guid>
      <description>Preface 最近IO层面的深入比较多, 顺便研究下 Kafka 底层IO实现,
深入 直接看下 socketServer 的注释 学习下理论.
acceptor /***Handles new connections, requests and responses to and from broker. *Kafka supports two types of request planes : *- data-plane : *- Handles requests from clients and other brokers in the cluster. *- The threading model is *1 Acceptor thread per listener, that handles new connections. *It is possible to configure multiple data-planes by specifying multiple &amp;#34;,&amp;#34; separated endpoints for &amp;#34;listeners&amp;#34; in KafkaConfig.</description>
    </item>
    
    <item>
      <title>Conn_close</title>
      <link>https://xujianhai.fun/posts/conn_close/</link>
      <pubDate>Sat, 23 May 2020 11:08:50 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/conn_close/</guid>
      <description>preface 最近讨论解决服务状态不正确的问题, 涉及到一个连接关闭的手段, 但是网上说法很多, 很难有一篇完整的手段描述, 特别记忆
实现 首先, 我们使用nc模拟网络的收发. 启动server:
nc -l -p 4444 启动client:
nc localhost 4444 -p 5555 使用tcpdump 查看效果:
sudo tcpdump port 4444 -i lo -xnn -S sudo tcpdump port 5555 -i lo -xnn -S 结果如下:
查看下连接的状态:
ss -ant | grep -E &#39;4444|5555&#39; 如下图: 根据参考的文档, 正确关闭的姿势如下(亲测有效):
sudo iptables -A INPUT -p tcp --dport 4444 -j REJECT --reject-with tcp-reset iptables -nL // n 数字化输出地址和端口, L 列出所有规则 iptables -F // 删除所有规则 iptables -A OUTPUT -p tcp --dport 5555 -j REJECT --reject-with tcp-reset iptables -nL 注意:</description>
    </item>
    
    <item>
      <title>Kafka_seq</title>
      <link>https://xujianhai.fun/posts/kafka_seq/</link>
      <pubDate>Fri, 22 May 2020 18:07:04 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/kafka_seq/</guid>
      <description>Preface 最近发现sarama的收发延迟很高, 内部研究发现 sarama抽象的broker交互 底层竟然是同步阻塞的调用: sendAndReceive, 比如:
func (b *Broker) Produce(request *ProduceRequest) (*ProduceResponse, error) { var response *ProduceResponse var err error if request.RequiredAcks == NoResponse { err = b.sendAndReceive(request, nil) } else { response = new(ProduceResponse) err = b.sendAndReceive(request, response) // 同步阻塞了 } if err != nil { return nil, err } return response, nil } 这里的broker对象就是 sarma 对 kafka broker 连接的抽象, 从上面可以发现, 对于每个生产请求, 都是顺序发送, 并且下一个请求必须等待上各个请求接收到相应 才能发送. 于是直观的想法就是, 异步的send 和 receive, 也就是说下一个请求并不需要等待上一个请求收到响应.</description>
    </item>
    
    <item>
      <title>Docker_net</title>
      <link>https://xujianhai.fun/posts/docker_net/</link>
      <pubDate>Thu, 21 May 2020 21:38:15 +0800</pubDate>
      
      <guid>https://xujianhai.fun/posts/docker_net/</guid>
      <description>Preface 最近小组讨论到 &amp;ldquo;too many open files&amp;rdquo; 和 网络模式, 说 bridger 可以实现pod间socket文件句柄的隔离, 引发了一些思考
network 这里讲到的 network 是 docker 层面的, docker 一共支持四种模式:
 bridger: docker 默认的方式, docker容器有独立的 network namespace、ip和子网, 这种模式下, 主机上会启动一个docker0的虚拟网桥(占有一个网段), 类似物理交换机, 所有的docker容器都会连接到这个网桥上, 分配网段中的ip. host: 使用宿主机的ip和端口, 能看到host上所有的设备 none: 没有网络配置、网卡、ip、路由 container: 容器之间共享一个 network namespace  参考 https://www.docker.org.cn/dockerppt/111.html</description>
    </item>
    
  </channel>
</rss>