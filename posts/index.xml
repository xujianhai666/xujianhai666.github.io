<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on zero.xu blog</title>
        <link>https://xujianhai.fun/posts/</link>
        <description>Recent content in Posts on zero.xu blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Thu, 26 Mar 2020 23:33:04 +0800</lastBuildDate>
        <atom:link href="https://xujianhai.fun/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Rocketmq_nomessage</title>
            <link>https://xujianhai.fun/posts/rocketmq_nomessage/</link>
            <pubDate>Thu, 26 Mar 2020 23:33:04 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_nomessage/</guid>
            <description>背景 最近在修rocketmq-golang-client的问题的时候, 发现在处理 PullNoNewMsg 的时候会导致 offset 被自动提交, 但是用户并没有设置自动ack, 并且也没有手动ack
注:
rocketmq 开源的版本并没有ack的概念
排查 于是, 通过日志打印调试, 发现是在 rocketmq-client-go 拉取消息处理 primitive.PullNoNewMsg 的状态的时候, 直接将 result.NextBeginOffset 替换为 request.nextOffset, 并且还 更新了本地offsetStore的offset 信息, 因为 rocektmq-client-go 是 周期性提交offset, 所以导致了 offset被ack 了
解决 在rocketmq-client-go的内部开发版本中, 直接将 offset 的本地存储更新给注释掉就可以了, 因为内部开发中, 是异步处理处理消息的, 并且offset的提交不需要满足递增的特性 (考虑到很多场景中可能存在 offset被移动到 更小的情况)
在开源的版本中, 对齐java的实现, 判断 processQueue是否有消息, 如果没有消息, 在更新本地offsetStore, 避免提交了 正在消费的消息
更多的理解 乘这次机会, 重新梳理了 rocketmq 在 pullMessage 的响应逻辑的处理. 根据客户端处理的逻辑, 区分如下 (不涉及到transaction)
1.NO_NEW_MSG
当broker返回 ResponseCode.PULL_NOT_FOUND 的时候, 客户端会转义成 PullStatus.NO_NEW_MSG, 会执行如下操作:</description>
            <content type="html"><![CDATA[<h2 id="背景">背景</h2>
<p>最近在修rocketmq-golang-client的问题的时候, 发现在处理 PullNoNewMsg 的时候会导致 offset 被自动提交, 但是用户并没有设置自动ack, 并且也没有手动ack</p>
<p><strong>注:</strong></p>
<p>rocketmq 开源的版本并没有ack的概念</p>
<h2 id="排查">排查</h2>
<p>于是, 通过日志打印调试, 发现是在 rocketmq-client-go 拉取消息处理 <code>primitive.PullNoNewMsg</code> 的状态的时候, 直接将 result.NextBeginOffset 替换为 request.nextOffset, 并且还 更新了本地offsetStore的offset 信息, 因为 rocektmq-client-go 是 周期性提交offset, 所以导致了 offset被ack 了</p>
<h2 id="解决">解决</h2>
<p>在rocketmq-client-go的内部开发版本中, 直接将 offset 的本地存储更新给注释掉就可以了, 因为内部开发中, 是异步处理处理消息的, 并且offset的提交不需要满足递增的特性 (考虑到很多场景中可能存在 offset被移动到 更小的情况)</p>
<p>在开源的版本中, 对齐java的实现, 判断 processQueue是否有消息, 如果没有消息, 在更新本地offsetStore, 避免提交了 正在消费的消息</p>
<h2 id="更多的理解">更多的理解</h2>
<p>乘这次机会, 重新梳理了 rocketmq 在 pullMessage 的响应逻辑的处理. 根据客户端处理的逻辑, 区分如下 (不涉及到transaction)</p>
<p><strong>1.NO_NEW_MSG</strong></p>
<p>当broker返回 <code>ResponseCode.PULL_NOT_FOUND</code> 的时候, 客户端会转义成 <code>PullStatus.NO_NEW_MSG</code>, 会执行如下操作:</p>
<ol>
<li>设置下次pullMessage的request的nextOffset 为 <code>pullResult.getNextBeginOffset()</code></li>
<li>如果这个queue本地的processQueue没有消息, 就更新本地offsetStore的offset到nextOffset, 这个会通过定期提交offset最终提交到 broker的记录中</li>
<li>重新调度拉取消息</li>
</ol>
<p>那么, broker 什么时候会返回 <code>PULL_NOT_FOUND</code> 呢？</p>
<ol>
<li>NO_MESSAGE_IN_QUEUE: queue没有数据, 可能是确实没有数据写入, 也可能是被删除了, 没数据写入的场景下会返回 <code>PULL_NOT_FOUND</code>, nextoffset=0, client的操作没有任何意义(甚至重新调度的时间也是可以延迟的). 如果是删除的场景, 会返回 <code>PULL_OFFSET_MOVED</code>, 客户端转义成 <code>OFFSET_ILLEGAL</code>, 具体操作参考下面的分析  (对应broker内部 NO_MESSAGE_IN_QUEUE)</li>
<li>在读取consumer queue file 时候失败了, 一般是文件出问题了, 直接跳转到下一个文件 (broker内部 OFFSET_FOUND_NULL)</li>
<li>consumer queue已经读取完毕了 (broker内部OFFSET_OVERFLOW_ONE)</li>
</ol>
<p><strong>2.NO_MATCHED_MSG</strong></p>
<p>当broker返回 <code>PULL_RETRY_IMMEDIATELY</code>, 客户端会转义成 <code>NO_MATCHED_MSG</code>, 执行如下操作:</p>
<ol>
<li>重置 pullRequest#nextOffset 为 pullResult#nextBeginOffset</li>
<li>如果queue对应的processQueue没有消息, 则重置offset</li>
<li>重新调度拉取消息</li>
</ol>
<p>那么, broker 什么时候会返回 <code>PULL_RETRY_IMMEDIATELY</code> 呢？</p>
<ol>
<li>从 从节点 读取消息但是 BrokerConfig().isSlaveReadEnable() 不允许 从读取</li>
<li>一轮拉取行为中, 第一次从 commitlog 没有读取到消息, 并且随后也都没有读取到消息. 这种情况下,是 consumer queue的消息已经是过期的了, 下次拉取的offset 就是 当前offset + 拉取数量的offset. (对应 MESSAGE_WAS_REMOVING)</li>
<li>consumer queue 的tag过滤不满足 或者 类过滤不满足 (对应NO_MATCHED_MESSAGE)</li>
</ol>
<p><strong>3.OFFSET_ILLEGAL</strong></p>
<p>当broker返回 <code>PULL_OFFSET_MOVED</code>, client会转义成 <code>OFFSET_ILLEGAL</code>, 并执行如下操作:</p>
<ol>
<li>更新 processQueue 的 nextOffset 字段为 <code>pullResult.getNextBeginOffset()</code>, 并设置 <code>dropped=true</code>, 丢弃这个queue</li>
<li>10s 后执行: 更新offsetStore的queue对应的offset并持久化到 broker, 然后从 本地删除 messageQueue (等待下次rebalance之后queue的owner进行拉取操作)</li>
</ol>
<p>不理解: 为什么要等待10s, 理解执行问题也不大</p>
<p>那么, broker 什么时候会返回 <code>PULL_OFFSET_MOVED</code> 呢</p>
<ol>
<li>consumer queue文件被删除了, 导致需要从0开始读取consumer queue文件, 需要client重置offset (对应broker NO_MESSAGE_IN_QUEUE)</li>
<li>client offset 大于 consumer queue maxOffset, 原因是 consumer queue文件消息丢失: 比如 主从切换, commitlog文件最近文件损害导致consumer queue文件截断等   (broker内部 OFFSET_OVERFLOW_BADLY)</li>
<li>consumer offset 小于 consumer queue minoffset, 很可能是 consumer queue 老的数据文件被清理了, 而 cosnumer group 的消费能力还没有消费完 之前被清理的数据 (或者consumer group被暂停了一段时间) (对应broker的OFFSET_TOO_SMALL)</li>
</ol>
<p><strong>4.FOUND</strong></p>
<p>有消息, 不赘述. 会返回下次拉取的起点offset字段: nextOffset</p>
<p>需要注意的是, 只要能够拉取到一条消息，返回的状态码都会是 <code>FOUND</code></p>
<h2 id="总结">总结</h2>
<ol>
<li>还是很复杂的, 自定义逻辑还是需要避免采坑</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq_search</title>
            <link>https://xujianhai.fun/posts/rocketmq_search/</link>
            <pubDate>Wed, 25 Mar 2020 18:22:56 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_search/</guid>
            <description>背景 最近基于golang做了 消息查询的功能, 这里做一些记录
原理 rocketmq的消息查询, 支持两种模式: offsetMsgId 和 msgkey、uniqueKey, 我这里避免了 msgid 的命名, 因为在 rocketmq client的实现过程中, msgid 存在很大的差异.
基本概念 offsetMsgId
offsetMsgId 本质上是 rocketmq commitLog 生成的, 生成格式如下:
 public static String createMessageId(final ByteBuffer input, final ByteBuffer addr, final long offset) { input.flip(); int msgIDLength = addr.limit() == 8 ? 16 : 28; input.limit(msgIDLength); input.put(addr); input.putLong(offset); return UtilAll.bytes2string(input.array()); } 可以发现, offsetMsgId 是基于commitLog 所在的地址 + 消息的offset 组成, 保证了 唯一性. 因此通过offsetMsgId 可以借助这个特性.
msgkey
在消息发送的时候, 发送的消息是可以指定消息的key的, 需要注意的是, msgKey可以设置多个.</description>
            <content type="html"><![CDATA[<h3 id="背景">背景</h3>
<p>最近基于golang做了 消息查询的功能, 这里做一些记录</p>
<h3 id="原理">原理</h3>
<p>rocketmq的消息查询, 支持两种模式: offsetMsgId 和 msgkey、uniqueKey, 我这里避免了 msgid 的命名, 因为在 rocketmq client的实现过程中, msgid 存在很大的差异.</p>
<h4 id="基本概念">基本概念</h4>
<p><strong>offsetMsgId</strong></p>
<p>offsetMsgId 本质上是 rocketmq commitLog 生成的, 生成格式如下:</p>
<pre><code>    public static String createMessageId(final ByteBuffer input, final ByteBuffer addr, final long offset) {
        input.flip();
        int msgIDLength = addr.limit() == 8 ? 16 : 28;
        input.limit(msgIDLength);

        input.put(addr);
        input.putLong(offset);

        return UtilAll.bytes2string(input.array());
    }
</code></pre><p>可以发现, offsetMsgId 是基于commitLog 所在的地址 + 消息的offset 组成, 保证了 唯一性. 因此通过offsetMsgId 可以借助这个特性.</p>
<p><strong>msgkey</strong></p>
<p>在消息发送的时候, 发送的消息是可以指定消息的key的, 需要注意的是, msgKey可以设置多个. 如下:</p>
<pre><code>public class Message implements Serializable {
	....
    public void setKeys(String keys) {
        this.putProperty(MessageConst.PROPERTY_KEYS, keys);
    }

    public void setKeys(Collection&lt;String&gt; keys) {
        StringBuffer sb = new StringBuffer();
        for (String k : keys) {
            sb.append(k);
            sb.append(MessageConst.KEY_SEPARATOR);
        }

        this.setKeys(sb.toString().trim());
    }
    .....
}
</code></pre><p><strong>uniqueKey</strong></p>
<p>producer发送消息的时候, 除了指定key之外, 还可以指定 uniqueKey, 和 key 不同的是, 需要用户保证 uniqueKey 的唯一性. 指定方式需要显示指定:</p>
<pre><code>Message msg = new Message(&quot;test_create_topic&quot;, &quot;body&quot;)
msg.putProperty(MixAll.UNIQUE_MSG_QUERY_FLAG, &quot;uniqueKey&quot;)
</code></pre><p>无论是 uniqueKey 还是 msgKey 都会被索引到 key的索引文件中. 而 offsetMsgId 并不会被索引, 因为offsetMsgId是自描述完备的, 因此不需要单独存储</p>
<h4 id="存储">存储</h4>
<p>rocketmq 提供的 key&amp;&amp;uniquekey查询, 是基于 rocketmq dispathcher 机制, 之前我们了解到, rocketmq 是将所有topic的消息 写入到同一个 commitLog, 然后启动了另一个线程从 commitlog 中不断读取消息并追加到 相应的 consumer queue 中. 这里的dispatcher是一个责任链的实现, 不仅仅有 CommitLogDispatcherBuildConsumeQueue (就是生成consumer queue的逻辑) 的实现, 还有 CommitLogDispatcherBuildIndex 的实现, CommitLogDispatcherBuildIndex 就是不断读取消息 并写入 indexFile.</p>
<p>indexFile 的文件格式: storePath/${year}${month}${day}${hour}${min}${sec}${mills} 的格式, 在indexFile 中, 每个key的索引记录(index record)是:</p>
<pre><code>keysHash (4)
phyoffset (4)
timediff (8)
slotValue (4)
</code></pre><p>这里的 keyHash 是 $topic#$key 的<code>String#hashCode</code>, 需要注意的, indexFile 会分别设置的 多个key 单独索引, 所以一个消息设置了 2个key, 就有两个索引记录. 但是对多个key进行索引的场景下, 必然存在 hash冲突, 大家对于hashmap对于冲突的解决是很了解的, 在 indexFile 的实现中, 也是使用传统的 <code>开址链表</code> 法, 下面讲解下 indexFile的内容.</p>
<p>在indexFile 中, 分成了三部分, indexHeader、 hashSlot、index records.</p>
<p>indexHeader 存储了: beginTimestamp endTimestamp beginPhyOffset endPhyOffset hashSlotCount indexCount, 其中, beginTimestamp 和 endTimestamp 是为了方便在消息搜索的时候对多个索引文件进行时间过滤, 以及存储消息时间的时候减少字节, index record 的 timediff 就是 消息的生成时间 - beginTimestamp 得到的.  beginPhyOffset endPhyOffset 的作用是 在删除的时候, 通过比较 endPhyOffset &lt; 指定的offset 进行过期文件删除. hashSlotCount 记录了 indexFile 中 hashSlot 的数量, 没什么意义, indexCount记录了 当前 indexFile中 index record 的数量, 只有当 indexCount 小于 配置的最大值, 才能新增 索引记录(index record), 默认  5_000_000 * 4.</p>
<p>hashSlots 也单独划分的一块文件空间, 用来存储 hashSlot, 相当于 <code>开址链表</code> 的数组空间. 和 传统的map的 <code>开址链表</code> 不同, hashSlot 并不存储 index key, 而是存储了 “最新” 的index record的物理偏移(因为 index record 是固定长度编码, 并且是 append模式递增存储, 这里只需要存储index  record的 index count), 当有重复的key映射到同一个 hashSlot 的情况, 会在新的 index record 的字段 slotValue 记录之前的 index record 的 count 地址, 这个时候的hashSlot则存储了 新添加的 index record 的count. 这样实现相对简单, 如果 hashSlot 总是存储第一个 index record 的count, 那么重复场景下 就需要操作两个 index record (除了新增的, 还需要操作老的index  record, 将老的index record的 slotValue更新到 新的 index record的值), 而当前实现只需要操作 一个 index record.</p>
<p>index records 的文件空间存储的是实际的 index record. 如上面的记录, 需要注意的是, 因为查询的时候是可以指定时间, 所以这里存储了 timediff, slotValue 起到了链式存储的效果, phyoffset 是为了从 commitlog 获取实际的消息.</p>
<h4 id="查询流程">查询流程</h4>
<p>mqadmin 支持了多种查询的方式:</p>
<pre><code>   queryMsgById         Query Message by Id
   queryMsgByKey        Query Message by Key
   queryMsgByUniqueKey  Query Message by Unique key
   queryMsgByOffset     Query Message by offset
</code></pre><p>首先, 对于key的查询, 都是基于 IndexService#queryOffset. 签名如下:</p>
<pre><code>queryOffset(String topic, String key, int maxNum, long begin, long end)
</code></pre><p>签名中, maxNum 是为了避免broker读取大量数据返回给客户端, 影响broker的稳定性. begin 和 end 是 希望查询的数据的时间范围, 查询遍历每个索引文件的时候, 会先对文件的时间进行过滤, 索引文件在 indexHeader 中存储了 beginTimestamp 和 endTimestamp, 分别表示了第一条index record 的时间 和 最后一个 index record 的时间. 搜索的时候, 根据key进行hash得到 hash slot, 然后对 hash slot 指定的 index record 进行读取和过滤. 读取过程中, 因为每一个index record 的 SLOTvalue 都记录了 相同hash 的 前一个 index record 地址, 从而实现了 冲突链表的过滤</p>
<p>下面分别列出几种查询的实现</p>
<ol>
<li>queryMsgByKey</li>
</ol>
<p>根据key搜索消息, 这种方式的查询场景下, 参数 begin 是0, end 是 <code>Long.MAX_VALUE</code>, 也就是说, <code>queryMsgByKey</code> 不会对消息的时间进行过滤. 最多获取 64条记录 (这些是mqadmin的硬编码), store 也有参数限制一次获取的最多的结果数量，默认也是 64.</p>
<p>需要注意的是, 因为 indexFile 存储的是key的hash结果, 所以 从indexFile搜索到的消息的key 可能并不是和搜索key 一致, 仅仅是hash结果一样, 因此本地需要过滤</p>
<ol start="2">
<li>queryMsgById</li>
</ol>
<p>这里的id指的是 offsetMsgId, 根据前面的理解, offsetMsgId 其实包含了 brokerAddr 和 commitLog offset 信息, 因此可以直接向 目标broker请求读取指定offset的内容</p>
<p>这里有一个让consumer重新消费的功能, 具体分析参照 queryMsgByUniqueKey</p>
<ol start="3">
<li>queryMsgByUniqueKey</li>
</ol>
<p>因为uniqueKey的特殊性, 可能是用户主动设置的(会创建索引), 也可能是 offsetMsgId(这种不会创建索引), 所以, 在请求的时候, 需要区分两种情况, 然后请求相应的api. 需要注意的是, 如果是用户设置的 uniqueKey, 那么和 queryMsgByKey 的逻辑有所不同, 因为业务保证了 uniqueKey 的唯一性, 因此本地不需要过滤.</p>
<p>另外, 还支持了通过 clientId 进行 <code>CONSUME_MESSAGE_DIRECTLY</code> 的指令, CONSUME_MESSAGE_DIRECTLY 是为了将让消费者能够直接消费消息. 整个流程如下:</p>
<ul>
<li>按照之前的流程查找 uniqueKey 的消息</li>
<li>本地消费消息, 取出消息的所在的broker信息</li>
<li>向消息所在的broker发送  CONSUME_MESSAGE_DIRECTLY, 指定 group 和 clientId</li>
<li>broker 将消息封装成 CONSUME_MESSAGE_DIRECTLY 的命令, 包含了指定的消息, 发送给消费者</li>
<li>消费者消费这条消息. 需要注意的是, 这里是 一次同步调用, 消费的结果直接返回给broker</li>
</ul>
<ol start="4">
<li>queryMsgByOffset</li>
</ol>
<p>这个api的逻辑比较清晰, 因为已经知道了 消息的broker queue 和 offset 信息, 因此直接启动consumer消费一条消息就可以了.</p>
<p>#文件删除#</p>
<p>index file的过期文件删除依赖 commitlog 和 consumerqueue 的删除逻辑, rocketmq 根据配置定期删除 过期文件. 默认执行周期: <code>cleanResourceInterval = 10_000;</code>, 就是 10s 为一个周期. 主要执行逻辑参照 CleanConsumeQueueService#deleteExpiredFiles 和 IndexService#deleteExpiredFile, 删除的主要逻辑是:</p>
<ol>
<li>获取commitlog 的 minoffset</li>
<li>检查indexFile 文件最早的文件的 endPhyOff 是否小于 minoffset, 如果是不小于, 那么return, 不做任何处理, 即 indexFile没过期</li>
<li>遍历indexFile 所有文件, 收集 满足endPhyOff &lt; minOffset 条件的 indexFile, 执行删除操作 (fileChannel#close + file#delete)</li>
</ol>
<h3 id="思考">思考</h3>
<ol>
<li>竟然提供了消息重新消费的功能</li>
<li>多种消息查询方式便于运维</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq_recover</title>
            <link>https://xujianhai.fun/posts/rocketmq_recover/</link>
            <pubDate>Tue, 24 Mar 2020 23:43:23 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_recover/</guid>
            <description>背景 最近要开发延迟消息, 这里记录下 recover相关的逻辑实现
原理 之前知道, rocketmq是所有的消息统一投递到 commitlog, 然后异步构建 consumer queue, 那么, 如果机器正常重启/异常宕机的情况下, 又是怎么恢复的呢?
前菜 rocketmq 使用了 checkpoint 文件记录了 physicMsgTimestamp logicsMsgTimestamp indexMsgTimestamp 三个字段, 分别表示 commitlog 的flush的时间点、comsumer queue的flush的时间点、index file 刷新的时间点. 也就是 已经落地磁盘的时间点. (通过fileChannel#force)
那么 这些时间点什么场景下会被更新, 什么时候checkpoint会flush呢?
 physicMsgTimestamp  首先, CommitLog 本身既有一个定时flush的任务, 根据flush方式的不同, 有两种实现: GroupCommitService 和 FlushRealTimeService(后面单独分析), 无论是同步还是异步, 每次flush之后都会设置 physicMsgTimestamp.
除此之外, 在 dledger模式中, slave构建 consumer queue的时候 也会设置 physicMsgTimestamp
logicsMsgTimestamp  在定时flush consumer queue 以及 追加consumer queue消息的时候, 都会更新. (因此, logicsMsgTimestamp 并不是 consumer queue flush的时间)</description>
            <content type="html"><![CDATA[<h2 id="背景">背景</h2>
<p>最近要开发延迟消息, 这里记录下 recover相关的逻辑实现</p>
<h2 id="原理">原理</h2>
<p>之前知道, rocketmq是所有的消息统一投递到 commitlog, 然后异步构建 consumer queue, 那么, 如果机器正常重启/异常宕机的情况下, 又是怎么恢复的呢?</p>
<h3 id="前菜">前菜</h3>
<p>rocketmq 使用了 checkpoint 文件记录了 physicMsgTimestamp logicsMsgTimestamp indexMsgTimestamp 三个字段, 分别表示 commitlog 的flush的时间点、comsumer queue的flush的时间点、index file 刷新的时间点. 也就是 已经落地磁盘的时间点. (通过fileChannel#force)</p>
<p>那么 这些时间点什么场景下会被更新, 什么时候checkpoint会flush呢?</p>
<ol>
<li>physicMsgTimestamp</li>
</ol>
<p>首先, CommitLog 本身既有一个定时flush的任务, 根据flush方式的不同, 有两种实现: GroupCommitService 和 FlushRealTimeService(后面单独分析), 无论是同步还是异步, 每次flush之后都会设置 physicMsgTimestamp.</p>
<p>除此之外, 在 dledger模式中, slave构建 consumer queue的时候 也会设置 physicMsgTimestamp</p>
<ol start="2">
<li>logicsMsgTimestamp</li>
</ol>
<p>在定时flush consumer queue 以及 追加consumer queue消息的时候, 都会更新. (因此, logicsMsgTimestamp 并不是 consumer queue flush的时间)</p>
<ol start="3">
<li>indexMsgTimestamp</li>
</ol>
<ul>
<li>在切换 indexFile 的时候, 会触发 把之前的 indexFile 刷新 以及 更新 indexMsgTimestamp</li>
</ul>
<p><strong>flush</strong></p>
<ul>
<li>rocketmq 正常关闭 (这里会触发两次刷新, DefaultMessageStore#shutdown 会分别调用 storeCheckpoint#flush storeCheckpoint#shutdown)</li>
<li>在indexFile切换的时候, 不仅仅会flush 之前的 index file, 还会触发 checkpoint file flush</li>
<li>conumer queue 定时刷新的时候, 除了更新 logicsMsgTimestamp, 也会触发 checkpoint file flush</li>
</ul>
<h3 id="正常重启">正常重启</h3>
<p>rocketmq 每次启动的时候, 会在存储根目录下面新建一个 abort 文件, 如果是正常关闭, 那么在shutdown的时候会删除 abort 文件, 如果是异常宕机 (断电、进程强杀等), abort 文件就会一直存放在那里. 因此在启动的时候, DefaultMessageStore 就会检查是否存在 abort 文件, 判断是正常启动 还是 异常启动.</p>
<p>恢复过程中, 需要区分两种情况, commitlog文件可能已经完全被删除了, 这个时候需要将 consumer queue文件也全部删除
常规恢复的场景中, 主要分为三步骤:</p>
<ol>
<li>恢复 consumer queue, 获取 consumer queue 中最大的 offset</li>
<li>恢复 取commitlog, 最大的 processOffset, 对 consumer queue 的多出的数据文件进行 截断</li>
<li>根据 commitlog 恢复 consumer queue 的消息</li>
</ol>
<p>1-3参考 DefaultMessageStore#recover、 ConsumeQueue#recover、CommitLog#recoverNormally, 4 的逻辑在 DefaultMessageStore#start</p>
<p><strong>流程1</strong>
从倒数3个文件开始恢复 (不足三个, 从第一个文件开始, 倒数第三个 是一个经验数值), 找到最大的 consumer queue的 processOffset, 设置元信息: flushedWhere 和 committedWhere; 然后遍历文件进行 truncate:</p>
<pre><code>文件起始offset 大于 processOffset 的直接删除, 
文件结束offset 小于 processOffset 不处理,
offset 位于 文件内部的情况, 设置文件的 元信息
</code></pre><p>需要注意的是, 因为 rocketmq 支持 tag ext 扩展文件, 因此在恢复的时候, 也会对 tag ext 恢复. 这里不赘述</p>
<p><strong>流程2</strong></p>
<p>从倒数3个文件开始恢复 (不足三个, 从第一个文件开始), 通过检查文件内容是否合法确定最后的写入位置: <code>processOffset</code>, 这里检查文件内容合法的方法比较特殊: 读取文件内容并构建一个 dispatchRequest. 根据 processOffset 设置元信息: flushedWhere 和 committedWhere, 遍历文件进行 truncate, 逻辑和 consumer queue的一样.</p>
<p>和 consumer queue 文件恢复不同的地方在于, commitlog 需要对consumer queue的文件内容进行 “纠偏”. 因为 consumer queue的数据都是从 commitlog 构建的, 因此 需要确保consumer queue的数据在 commitlog 全都要找到, 因此在恢复的时候, 需要根据 commitlog 的 processOffset 对 consumer queue 进行截断 和 元信息重置</p>
<ul>
<li>需要注意的是, 在读取文件如何区分 读取到文件末尾的情况?</li>
</ul>
<p>commitlog在写入数据的时候, 会进行判断 msgLen + 8() &gt;  剩余文件空间, 如果true, 那么, 就会放弃在这个文件写入, 会轮转到下一个文件写入, 同时在这个文件的末尾写入 totalSize(int=4 byte) + CommitLog.BLANK_MAGIC_CODE(int=4 byte)</p>
<p><strong>流程3</strong></p>
<p>完成上面两个流程, 基本上保证了 commitlog 和 consumer queue 文件的正确性. 但是这里存在一个问题, 可能consumer queue的数据 少于commitlog, 因为 构建consumer queue 速度慢于 commitlog 或者 consumer queue 文件被删除 或者新启的broker copy了别的机器的commitlog. 那么就需要一个机制 将commitlog 中的数据 重新构建到 consumer queue, 流程3 就是做了这件事情</p>
<p>通过获取 consumer queue最大的 processOffset, 然后从 commitlog 的 processOffset 点位进行构建工作.</p>
<h3 id="异常宕机">异常宕机</h3>
<p>异常宕机相比于正常关闭, 需要借助 checkpoint 文件进行恢复.</p>
<p>整体流程和正常恢复差异不大, 依旧是上面上个流程</p>
<ol>
<li>恢复 consumer queue, 获取 consumer queue 中最大的 offset</li>
<li>恢复 取commitlog, 最大的 processOffset, 对 consumer queue 的多出的数据文件进行 截断</li>
<li>根据 commitlog 恢复 consumer queue 的消息</li>
</ol>
<p>唯一的不同, 在于第二步骤, 因为是异常宕机, 所以不能从倒数第三个, 需要一个可以 check的时间点(minTime) 进行恢复, 这个时间点之前的文件是可以认为是正确的, 时间点之后的文件开始恢复. 根据配置的不同, 有两种选择:</p>
<ol>
<li>需要根据checkpoint记录的 physicMsgTimestamp 和 logicsMsgTimestamp 的最小值开始恢复.</li>
<li>
<ol>
<li>配置 MessageStoreConfig#messageIndexSafe = ture (默认false) 和 messageIndexEnable = true (默认true) 的时候, 会以 physicMsgTimestamp logicsMsgTimestamp indexMsgTimestamp 的最小开始恢复</li>
</ol>
</li>
</ol>
<p>确定了时间点之后, 倒序找到第一个文件存储时间小于 minTime的文件, 从这个文件开始, 不断读取消息 并执行 dispatcher 责任链逻辑: 构建consumer queue、构建index message. 剩下的逻辑: 重置元信息、截断 consumer queue 和正常恢复一样</p>
<p>需要注意的是, 这里提供了 index message 的minTime 机制, 对于一些依赖 index 逻辑的场景, 还是很有必要的</p>
<h2 id="总结">总结</h2>
<ol>
<li>异常恢复需要重建 consumer queue 和 index message</li>
<li>恢复过程需要确保 consumer queue 信息和 commitlog  对齐, 不能多(截断)、不能少(重新reinput)</li>
<li>依赖index message的场景, 需要开启 MessageStoreConfig#messageIndexSafe=true, 确保index 的完整性</li>
</ol>
<h2 id="思考">思考</h2>
<ol>
<li>代码冗余度很高, 需要优化下</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq_flow_control</title>
            <link>https://xujianhai.fun/posts/rocketmq_flow_control/</link>
            <pubDate>Mon, 23 Mar 2020 22:31:59 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_flow_control/</guid>
            <description>背景 rocketmq推广过程中, 偶尔会遇到 [TIMEOUT_CLEAN_QUEUE]broker busy, start flow control for a while, period in queue: 201ms, size of queue: 5389 类似的报错, 导致上游业务失败率报警以及错误日志飙升. 在相应的监控上, rocketmq 的发送qps也是非常高.
原因 其实这个行为是 rocektmq broker 的自我保护机制, 那么什么时候会触发呢? 这个主要是在 store 进行put 消息的时候会触发. 之前讲过, 在 rocketmq 的处理机制中, netty 将读取到的消息 会封装成 RequestTask 对象提交到 executorService 的队列中, 然后等待 executorService 调度执行. 那么, 这里存在两种情况:
  queue已经被写满了, 无法再提交新的任务, 那么会触发 RejectedExecutionException, 这个时候, rocketmq broker 会返回 RemotingSysResponseCode.SYSTEM_BUSY, 提示信息是: [OVERLOAD]. 参考: NettyRemotingAbstract#processRequestCommand
  调度延迟的问题. 我在 11:05 提交了一个写入请求, 但是因为 写入流程耗时 增加, 导致我的请求到 11:06 才被处理, 对于实时在线业务而言, 这条消息其实早就超时了, 这种情况, rocketmq 有两套机制:</description>
            <content type="html"><![CDATA[<h2 id="背景">背景</h2>
<p>rocketmq推广过程中, 偶尔会遇到 <code>[TIMEOUT_CLEAN_QUEUE]broker busy, start flow control for a while, period in queue: 201ms, size of queue: 5389</code> 类似的报错, 导致上游业务失败率报警以及错误日志飙升. 在相应的监控上, rocketmq 的发送qps也是非常高.</p>
<h2 id="原因">原因</h2>
<p>其实这个行为是 rocektmq broker 的自我保护机制, 那么什么时候会触发呢? 这个主要是在 store 进行put 消息的时候会触发. 之前讲过, 在 rocketmq 的处理机制中, netty 将读取到的消息 会封装成 RequestTask 对象提交到 executorService 的队列中, 然后等待 executorService 调度执行. 那么, 这里存在两种情况:</p>
<ol>
<li>
<p>queue已经被写满了, 无法再提交新的任务, 那么会触发 <code>RejectedExecutionException</code>, 这个时候, rocketmq broker 会返回 <code>RemotingSysResponseCode.SYSTEM_BUSY</code>, 提示信息是: <code>[OVERLOAD]</code>. 参考: <code>NettyRemotingAbstract#processRequestCommand</code></p>
</li>
<li>
<p>调度延迟的问题. 我在 11:05 提交了一个写入请求, 但是因为 写入流程耗时 增加, 导致我的请求到 11:06 才被处理, 对于实时在线业务而言, 这条消息其实早就超时了, 这种情况, rocketmq 有两套机制:</p>
<ul>
<li>
<p>2.1 定期检查queue, 会检查 RequestTask 的生成时间 和 当前时间的 差值, 如果超过了配置的超时时间, 就会返回 <code>RemotingSysResponseCode.SYSTEM_BUSY</code>, 提示 <code>TIMEOUT_CLEAN_QUEUE</code>, 参考 BrokerFastFailure#cleanExpiredRequestInQueue</p>
</li>
<li>
<p>2.2 对于 send 请求, 会检查当前系统是否 <code>isOSPageCacheBusy</code>, 如果 true, 就会拿取 queue 的第一个 RequestTask 返回 <code>RemotingSysResponseCode.SYSTEM_BUSY</code>, 提示 <code>PCBUSY_CLEAN_QUEUE</code>, 直到 判断是 false. 那么, isOSPageCacheBusy 的判断逻辑是什么呢? 如下:</p>
</li>
</ul>
</li>
</ol>
<pre><code>@Override
public boolean isOSPageCacheBusy() {
    long begin = this.getCommitLog().getBeginTimeInLock();
    long diff = this.systemClock.now() - begin;

    return diff &lt; 10000000
        &amp;&amp; diff &gt; this.messageStoreConfig.getOsPageCacheBusyTimeOutMills();
}
</code></pre><p>getOsPageCacheBusyTimeOutMills默认是1000ms, 可以发现, 这里diff 需要在 (1000, 10_000_000) 的区间, 也就是commitlog 最近的一条写入大于1s的情况  (beginTimeInLock 在写入的时候会被设置当前时间戳, 但是在写入成功后会被重置为0), diff 的 上界为了区别于 没有写入的情况(没有写入, beginTimeInLock就是0, diff=this.systemClock.now() ).</p>
<ol start="3">
<li>
<p>写入逻辑之前会进行一次 <code>isOSPageCacheBusy</code>的检查, true 则设置 PutMessageResult 状态 <code>OS_PAGECACHE_BUSY</code>, 返回 <code>ResponseCode.SYSTEM_ERROR</code>, 提示 <code>[PC_SYNCHRONIZED]</code></p>
</li>
<li>
<p>为了避免 pageCache busy 场景下请求的无效投递到queue, 在提交queue之前, 会检查条件 <code>rejectRequest</code>, 判断条件是 <code>this.brokerController.getMessageStore().isOSPageCacheBusy() ||this.brokerController.getMessageStore().isTransientStorePoolDeficient()</code>, 其中, <code>isTransientStorePoolDeficient</code> 已经被弃用, isOSPageCacheBusy 参见上文.</p>
</li>
</ol>
<p>综合上面的分析, 因为我们业务的qps很高, 导致了 <code>TIMEOUT_CLEAN_QUEUE</code>  的提示, 是因为 处理写入时间 过长</p>
<h2 id="解决">解决</h2>
<p>扩容, 降低broker的热点负载</p>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq_msgid</title>
            <link>https://xujianhai.fun/posts/rocketmq_msgid/</link>
            <pubDate>Tue, 17 Mar 2020 18:18:05 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_msgid/</guid>
            <description>最近利用 msgId 进行一些延迟实现, 结果发现, msgId 在 producer 和 consumer 两侧是不一致的.
复现 我用producer发送一条消息如下:
SendResult [sendStatus=SEND_OK, msgId=0AFE2AEF000018B4AAC2562A9AC70000, offsetMsgId=0AE1578800002A9F0000000C6C988CC2, messageQueue=MessageQueue [topic=test_create_topic, brokerName=sandbox_boe4, queueId=0], queueOffset=0] 需要注意的是, msgId 和 offsetMsgId 是不一样的. 在consumer侧, 我接受到的消息如下:
Receive New Messages: [MessageExt [queueId=0, storeSize=197, queueOffset=0, sysFlag=0, bornTimestamp=1584437632711, bornHost=/10.254.42.239:49872, storeTimestamp=1584437632868, storeHost=/10.225.87.136:10911, msgId=0AE1578800002A9F0000000C6C988CC2, commitLogOffset=53361544386, bodyCRC=198614610, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic=&#39;test_create_topic&#39;, flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=1, KEYS=OrderID188, CONSUME_START_TIME=1584437674686, UNIQ_KEY=0AFE2AEF000018B4AAC2562A9AC70000, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100], transactionId=&#39;null&#39;}]] consumer 收到的 msg的 msgId 和 producer的 msgId 是不一样的, 但是, producer侧的msgId 和 consumer侧的 UNIQUE_KEY 的值是一样的, producer 的 offsetMsgId 和 consumer侧的 msgId 是一致的.</description>
            <content type="html"><![CDATA[<p>最近利用 msgId 进行一些延迟实现, 结果发现, msgId 在 producer 和 consumer 两侧是不一致的.</p>
<h2 id="复现">复现</h2>
<p>我用producer发送一条消息如下:</p>
<pre><code>SendResult [sendStatus=SEND_OK, msgId=0AFE2AEF000018B4AAC2562A9AC70000, offsetMsgId=0AE1578800002A9F0000000C6C988CC2, messageQueue=MessageQueue [topic=test_create_topic, brokerName=sandbox_boe4, queueId=0], queueOffset=0]
</code></pre><p>需要注意的是, msgId 和 offsetMsgId 是不一样的. 在consumer侧, 我接受到的消息如下:</p>
<pre><code>Receive New Messages: [MessageExt [queueId=0, storeSize=197, queueOffset=0, sysFlag=0, bornTimestamp=1584437632711, bornHost=/10.254.42.239:49872, storeTimestamp=1584437632868, storeHost=/10.225.87.136:10911, msgId=0AE1578800002A9F0000000C6C988CC2, commitLogOffset=53361544386, bodyCRC=198614610, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic='test_create_topic', flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=1, KEYS=OrderID188, CONSUME_START_TIME=1584437674686, UNIQ_KEY=0AFE2AEF000018B4AAC2562A9AC70000, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100], transactionId='null'}]] 
</code></pre><p>consumer 收到的 msg的 msgId 和 producer的 msgId 是不一样的, 但是, producer侧的msgId 和 consumer侧的 UNIQUE_KEY 的值是一样的, producer 的 offsetMsgId 和 consumer侧的 msgId 是一致的.</p>
<p>通过控制台搜索, 发现 (msgId)producer 和 (offsetMsgId)producer 都可以搜到消息, 并且显示的msgId 都是 (msgId)producer.</p>
<h2 id="分析">分析</h2>
<p>翻看了源代码发现, (msgId)producer 其实是客户端设置的, 用户没有设置的时候, 会自动生成一个, 参看 MessageClientIDSetter#createUniqID. 这个值在处理发送的响应的时候会被用到, producer 将这个生成的 <code>PROPERTY_UNIQ_CLIENT_MESSAGE_ID_KEYIDX</code> 的值 放到了 <code>SendResult</code> 的 msgId 对象中, 而将 broker 的msgId 设置到了 <code>SendResult</code> 的 offsetMsgId 中</p>
<p>但是在broker侧, <code>CommitLog#createMessageId</code> 基于 ip+物理偏移 创建的 msgId 传递给了 producer, producer 存储成 offsetMsgId. 需要注意的是 broker 并不会存储这个 msgId. 在消费的时候, consumer 在获取到消息之后, 会基于获取的 storeHost+commotLogOffset 重新创建出 msgId 放到msg 中, 具体参看 MessageDecoder#decode.</p>
<p>所以, 基本上就是 producer 和 consumer 两边的概念不一致导致的</p>
<h2 id="补充">补充</h2>
<p>uniqueKey 自动生成的规则像这样: ip+pid+classloader hashcode + 当前时间戳 + 生存时间长度(最大一个月) + counter. 显然, 后面的一段时间并不能保证唯一,  而且在 k8s场景中, ip 也不是唯一的, 用户设置 msgid, 不是单一的key, 是可以多个的.</p>
]]></content>
        </item>
        
        <item>
            <title>Kip_broker</title>
            <link>https://xujianhai.fun/posts/kip_broker/</link>
            <pubDate>Mon, 09 Mar 2020 23:13:41 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kip_broker/</guid>
            <description>这里主要记录kafka broker 的相关proposal
 broker 增加配置: fetch.max.bytes, 避免部分consumer影响其他consumer.
proposal</description>
            <content type="html"><![CDATA[<blockquote>
<p>这里主要记录kafka broker 的相关proposal</p>
</blockquote>
<p>broker 增加配置: fetch.max.bytes, 避免部分consumer影响其他consumer.<br>
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-541%3A+Create+a+fetch.max.bytes+configuration+for+the+broker">proposal</a></p>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq_allocate</title>
            <link>https://xujianhai.fun/posts/rocketmq_allocate/</link>
            <pubDate>Sun, 08 Mar 2020 20:04:52 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_allocate/</guid>
            <description>最近一年中, 经常有用户不同的服务用一个group分别订阅不同的topic, 导致部分partition不消费
 场景 业务反馈的时候, 通常是 监控上部分partition lag 增长, 并且queue的消费qps是0.
通过使用 mqadmin consumerProgress 查看offset 提交的时候, 发现这个group提交了多个topic, 并且每次结果不一样
-&amp;gt; % mqadmin consumerProgress -g groupA -n $addr -s true RocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0). RocketMQLog:WARN Please initialize the logger system properly. #Topic #Broker Name #QID #Broker Offset #Consumer Offset #Client IP #Diff #LastTime %RETRY%groupA broker1 0 0 0 ip1 0 N/A topicA broker1 0 2180901 2180901 ip1 0 2020-03-08 20:10:04 topicA broker1 1 2000000 0 ip1 200000 2020-03-08 00:10:04 -&amp;gt; % mqadmin consumerProgress -g groupA -n $addr -s true RocketMQLog:WARN No appenders could be found for logger (io.</description>
            <content type="html"><![CDATA[<blockquote>
<p>最近一年中, 经常有用户不同的服务用一个group分别订阅不同的topic, 导致部分partition不消费</p>
</blockquote>
<h3 id="场景">场景</h3>
<p>业务反馈的时候, 通常是 监控上部分partition lag 增长, 并且queue的消费qps是0.</p>
<p>通过使用 <code>mqadmin consumerProgress</code> 查看offset 提交的时候, 发现这个group提交了多个topic, 并且每次结果不一样</p>
<pre><code>-&gt; % mqadmin consumerProgress -g  groupA  -n $addr  -s true
RocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0).
RocketMQLog:WARN Please initialize the logger system properly.
#Topic        #Broker Name                      #QID  #Broker Offset        #Consumer Offset      #Client IP           #Diff                 #LastTime
%RETRY%groupA  broker1                           0     0                     0                     ip1                  0                     N/A
topicA         broker1                           0     2180901               2180901               ip1                  0                     2020-03-08 20:10:04
topicA         broker1                           1     2000000               0                     ip1                  200000                2020-03-08 00:10:04
</code></pre><pre><code>-&gt; % mqadmin consumerProgress -g  groupA  -n $addr  -s true
RocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0).
RocketMQLog:WARN Please initialize the logger system properly.
#Topic        #Broker Name                      #QID  #Broker Offset        #Consumer Offset      #Client IP           #Diff                 #LastTime
%RETRY%groupA  broker1                           0     0                     0                     ip1                  0                     N/A
topicB         broker1                           0     2172997               2172997               ip1                  0                     2020-03-08 20:10:05
topicB         broker1                           1     1000000               0                     ip1                  10000                 2020-03-08 00:10:04
</code></pre><p>上面可以发现, groupA 消费了 topicA、topicB, 但是都只消费了一个queue, 导致了其他queue的lag. (为了说明问题, 这里很多使用了填充, 比如 ip、groupName、topic)</p>
<p>难道rocketmq不支持consumer client 同时订阅多个 topic?</p>
<p>其实正常使用rocketmq的情况下, 是允许一个consumer同时订阅多个topic的, 但是需要在consumer client启动前一次性订阅完多个topic, 比如下图, 一次性订阅TopicTestA 和 TopicTestB</p>
<p><img src="/rocketmq_sub_multi.png" alt="rocketmq_sub_multi" title="rocketmq_sub_multi"></p>
<p>但是如果业务分开来分别订阅的话, 就会存在问题, 如下使用</p>
<p><img src="/rocketmq_sub_a.png" alt="rocketmq_sub_a.png" title="rocketmq_sub_a.png"></p>
<p><img src="/rocketmq_sub_b.png" alt="rocketmq_sub_b.png" title="rocketmq_sub_b.png"></p>
<h3 id="原因">原因</h3>
<p>为什么出现这样的原因?</p>
<p>首先, 为什么每次查询的结果不一样. 这个和broker维护consumer信息的实现 以及 心跳实现有关, 关键结构如下:</p>
<pre><code>public class ConsumerManager {
    private final ConcurrentMap&lt;String/* Group */, ConsumerGroupInfo&gt; consumerTable =
        new ConcurrentHashMap&lt;String, ConsumerGroupInfo&gt;(1024);
    ......
}

public class ConsumerGroupInfo {
    private static final InternalLogger log = InternalLoggerFactory.getLogger(LoggerName.BROKER_LOGGER_NAME);
    private final String groupName;
    private final ConcurrentMap&lt;String/* Topic */, SubscriptionData&gt; subscriptionTable =
        new ConcurrentHashMap&lt;String, SubscriptionData&gt;()
    .......
}
</code></pre><p>重点是 SubscriptionData 的维护上, 可以发现, 本质上这是个映射的实现: group -&gt; topic -&gt; SubscriptionData. 之所以可以订阅多个topic, 是因为 ConsumerGroupInfo 内部维护了一个topic的map, 这样订阅多个topic的时候, 只需要将topic和订阅数据 存放在 ConcurrentMap 中, 这个行为是在心跳的机制中实现的</p>
<p>但是在问题场景中, topicA 和 topicB 是分两种心跳投递的, 第一种心跳是 groupA 订阅 topicA 的心跳, 假设是服务A启动的进程; 第二种心跳是 groupA 订阅 topicB 的心跳, 假设是 服务B启动的. 因为是两次心跳逻辑, 在rocketmq中心跳中订阅数据是基于覆盖方式实现的, 关键实现如 ConsumerGroupInfo#updateSubscription:</p>
<pre><code>public class ConsumerGroupInfo {
    public boolean updateSubscription(final Set&lt;SubscriptionData&gt; subList) {
        boolean updated = false;

        for (SubscriptionData sub : subList) {
            SubscriptionData old = this.subscriptionTable.get(sub.getTopic());
            if (old == null) {
            	SubscriptionData prev = this.subscriptionTable.putIfAbsent(sub.getTopic(), sub);
            	.....
            } else if (sub.getSubVersion() &gt; old.getSubVersion()) {
                if (this.consumeType == ConsumeType.CONSUME_PASSIVELY) {
                    log.info(&quot;subscription changed, group: {} OLD: {} NEW: {}&quot;,
                        this.groupName,
                        old.toString(),
                        sub.toString()
                    );
                }

                this.subscriptionTable.put(sub.getTopic(), sub);
            }
        }
    	......
	}
}
</code></pre><p>这里补充下, sub#subVersion 就是当前时间. 也就是说, 随着多次心跳, topic的 SubscriptionData 会经常变化. 在我们执行命令 <code>mqamdin consumerProgress</code>的逻辑, cosnumer 的topic消费信息会根据 订阅信息进行过滤, 因为心跳的原因, 导致每次过滤出的topic 不一样, 也就会导致我们看到的结果不一样</p>
<p>那么, 为什么 消费存在lag呢? 这个就和 消费的rebalance 有关了.</p>
<p>在cluster模式的消费的流程中, 需要走以下几个逻辑(不是串行的, 这里为了简单说明):</p>
<ol>
<li>遍历本地订阅的topic</li>
<li>获取这个topic下所有的mq</li>
<li>获取这个consumer group的所有consumerID</li>
<li>根据算法, 将topic下的所有mq分配给所有consumerID</li>
<li>当前consumer会尝试消费分配给自己的mq (如果是顺序消费, 会存在一次加锁行为, 这里不讨论)</li>
</ol>
<p>这里需要注意的是, 无论是 服务A 还是 服务B 的消费, 在第三步中, 获取的 consumerID 都是 两个服务总共的 consumer, 这样在执行分配的时候, topicA 和 topicB 都是基于所有的consumer进行分配的. 可是, 服务A的consumer client 并不会去消费 topicB, 这样 topicB分配给 服务A consumer client 的 mq 并不会消费, 导致了lag.</p>
<h3 id="如何快速定位">如何快速定位</h3>
<p>我们可以使用 <code>mqadmin consumerProgress</code> 查看这个consumer group 订阅topic的offset提交情况, 如果发现多个 topic 的提交情况, 并且并没有一次性订阅多个topic, 基本上是这种情况了</p>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq_subsconfig</title>
            <link>https://xujianhai.fun/posts/rocketmq_subs/</link>
            <pubDate>Sun, 08 Mar 2020 15:35:05 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_subs/</guid>
            <description>最近咨询订阅配置的人比较多, 这里进行分析下.
 配置信息 订阅配置信息是consumer向broker消费消息的凭证, 如果broker开启了 autoCreateSubscriptionGroup=false , 那么consumer client在消费之前, 必须通过命令行或者控制台上创建订阅配置, 然后consumer client使用配置订阅的名字. 通过命令行创建的订阅如下:
-&amp;gt; % mqadmin updateSubGroup usage: mqadmin updateSubGroup [-a &amp;lt;arg&amp;gt;] [-b &amp;lt;arg&amp;gt;] [-c &amp;lt;arg&amp;gt;] [-d &amp;lt;arg&amp;gt;] -g &amp;lt;arg&amp;gt; [-h] [-i &amp;lt;arg&amp;gt;] [-m &amp;lt;arg&amp;gt;] [-n &amp;lt;arg&amp;gt;] [-q &amp;lt;arg&amp;gt;] [-r &amp;lt;arg&amp;gt;] [-s &amp;lt;arg&amp;gt;] [-w &amp;lt;arg&amp;gt;] -a,--notifyConsumerIdsChanged &amp;lt;arg&amp;gt; notify consumerId changed -b,--brokerAddr &amp;lt;arg&amp;gt; create subscription group to which broker -c,--clusterName &amp;lt;arg&amp;gt; create subscription group to which cluster -d,--consumeBroadcastEnable &amp;lt;arg&amp;gt; broadcast -g,--groupName &amp;lt;arg&amp;gt; consumer group name -h,--help Print help -i,--brokerId &amp;lt;arg&amp;gt; consumer from which broker id -m,--consumeFromMinEnable &amp;lt;arg&amp;gt; from min offset -n,--namesrvAddr &amp;lt;arg&amp;gt; Name server address list, eg: 192.</description>
            <content type="html"><![CDATA[<blockquote>
<p>最近咨询订阅配置的人比较多, 这里进行分析下.</p>
</blockquote>
<h2 id="配置信息">配置信息</h2>
<p>订阅配置信息是consumer向broker消费消息的凭证, 如果broker开启了 <code>autoCreateSubscriptionGroup=false</code> , 那么consumer client在消费之前, 必须通过命令行或者控制台上创建订阅配置, 然后consumer client使用配置订阅的名字. 通过命令行创建的订阅如下:</p>
<pre><code>-&gt; % mqadmin  updateSubGroup
usage: mqadmin updateSubGroup [-a &lt;arg&gt;] [-b &lt;arg&gt;] [-c &lt;arg&gt;] [-d &lt;arg&gt;] -g &lt;arg&gt; [-h] [-i &lt;arg&gt;] [-m &lt;arg&gt;]
       [-n &lt;arg&gt;] [-q &lt;arg&gt;] [-r &lt;arg&gt;] [-s &lt;arg&gt;] [-w &lt;arg&gt;]
 -a,--notifyConsumerIdsChanged &lt;arg&gt;       notify consumerId changed
 -b,--brokerAddr &lt;arg&gt;                     create subscription group to which broker
 -c,--clusterName &lt;arg&gt;                    create subscription group to which cluster
 -d,--consumeBroadcastEnable &lt;arg&gt;         broadcast
 -g,--groupName &lt;arg&gt;                      consumer group name
 -h,--help                                 Print help
 -i,--brokerId &lt;arg&gt;                       consumer from which broker id
 -m,--consumeFromMinEnable &lt;arg&gt;           from min offset
 -n,--namesrvAddr &lt;arg&gt;                    Name server address list, eg: 192.168.0.1:9876;192.168.0.2:9876
 -q,--retryQueueNums &lt;arg&gt;                 retry queue nums
 -r,--retryMaxTimes &lt;arg&gt;                  retry max times
 -s,--consumeEnable &lt;arg&gt;                  consume enable
 -w,--whichBrokerWhenConsumeSlowly &lt;arg&gt;   which broker id when consume slowly
</code></pre><p>其中, <code>-b</code> 和 <code>-c</code> 两个参数是对立的, <code>-b</code> 只会请求对应的broker, 而<code>-c</code> 则会先获取指定集群下的所有broker地址, 然后遍历执行创建<code>SubscriptionGroupConfig</code>. <code>-m</code> 目前是多余的配置, 暂时不起任何作用. 其他的配置中, 比较重要的如下:</p>
<ul>
<li>
<p>notifyConsumerIdsChanged: 当有新的consumer连接到broker的时候, 是否允许broker遍历已经注册的consumer进行通知请求, cosumer接收到通知请求后, 会触发rebalance. 这个参数主要的作用是什么呢?  如果是有序消费, 并没有太大的影响, 只是添加的consumer需要在下一轮rebalance之后才能消费, 并且是 获取到broker的队列锁之后才能消费; 如果是并发消费, 关闭这个选项的话, 就会导致严重的重复消费. 因为和有序消费不同, 并发消费没有队列锁, 那么, 如果关闭选项的话, 每个consumer不能及时感知到其他consumer的存在, 每个consumer rebalance的实际不一样, 导致一段时间内, 有的consumer消费的是加入前分配的结果, 有的consumer消费的则是分配后的结果, consumer主动触发rebalance是 20s.</p>
</li>
<li>
<p>consumeBroadcastEnable: 这个只有在需要广播消息的时候才需要打开, 一般用不到</p>
</li>
<li>
<p>consumeEnable: 正常使用直接设置成 true 就可以了. 如果希望所有的consumer都不消费, 比如 敏感秘密级别的原因, 设置成 false</p>
</li>
<li>
<p>whichBrokerWhenConsumeSlowly: 这个只有在 consumer group lag 非常大的时候才会触发. 只有在 brokerConfig#isSlaveReadEnable 打开的情况下才会奏效. 当master lag非常大的时候, rocketmq 是有策略的: 重定向consumer 到 slave 消费, 众所周知, master-salve 同步配置中, 每个broker是有 brokerId 的, brokerId=0 是master, brokerId大于0 的是slave, 一般建议设置为1, 一些master-slave的配置中, 为了保证数据不丢, 配置了两个slave, 一般是 slaveId=1 和 slaveId=2, 所以, 因为 whichBrokerWhenConsumeSlowly 只能设置一个值, 因为 存在一个slave空闲的场景</p>
</li>
</ul>
<h2 id="存储">存储</h2>
<p>SubscriptionGroupConfig 是存储在 broker 上的, 并且以 json 格式存储在 storePath 路径下. 上面只讲述了创建的功能, 其实 mqadmin 还提供了删除了功能.</p>
<h2 id="使用">使用</h2>
<ol>
<li>心跳</li>
</ol>
<ul>
<li>
<p>consumer group定期心跳的时候, 心跳数据包含了group name 和 订阅的topic. 这里存在一个判断, 如果brokerConfig中 <code>autoCreateSubscriptionGroup=true</code> 的话, 即使 group 没有注册过订阅信息, 这里就会创建一个默认的 SubscriptionGroupConfig. 为后面拉取消息 提供凭证. 如果说 broker 关闭自动创建 SubscriptionGroupConfig: <code>autoCreateSubscriptionGroup=false</code>, 那么 没有注册过的group 无法消费消息</p>
</li>
<li>
<p>在处理心跳的时候, 还会根据 <code>SubscriptionGroupConfig#notifyConsumerIdsChanged</code> 决定是否向已经注册的consumer client 发送 <code>consumerIdChange</code> 事件, 来触发 consumer client 的 rebalance</p>
</li>
</ul>
<ol start="2">
<li>拉消息</li>
</ol>
<ul>
<li>
<p>拉取消息之前, 必须有相应的 SubscriptionGroupConfig 信息, 如果没有的话, 就不能消费. 所以, 如果group没有注册过订阅信息, 那么 group 必须心跳成功后, 才有可能正常消费.</p>
</li>
<li>
<p>拉取消息的时候, 如果lag很大, 在 <code>brokerConfig#isSlaveReadEnable=true</code> 的配置下, 则会根据 <code>SubscriptionGroupConfig#whichBrokerWhenConsumeSlowly</code> 的slaveId 来重定向consumer client 向指定的slave 拉取消息</p>
</li>
</ul>
<ol start="3">
<li>监控</li>
</ol>
<ul>
<li>比较特殊的情况, 线上经常使用group测试消费数据一段时间后, 就不在使用了. 但是 group 的lag监控却在一直增长, 并可能引起报警影响用户的生活. 如果用的是开源的rocketmq 监控, 即使使用 <code>mqadmin#deleteSubGroup</code> 也不能消除lag</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq_article</title>
            <link>https://xujianhai.fun/posts/rocketmq_article/</link>
            <pubDate>Sun, 08 Mar 2020 12:00:05 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq_article/</guid>
            <description> 这里主要是收集一些比较不错的rocketmq 相关的文章
 使用和优化 rocketmq官方文档的优化使用: irqbalance 关闭、中断聚合、numa: 链接
双机房 源码分析 </description>
            <content type="html"><![CDATA[<blockquote>
<p>这里主要是收集一些比较不错的rocketmq 相关的文章</p>
</blockquote>
<h2 id="使用和优化">使用和优化</h2>
<p>rocketmq官方文档的优化使用: irqbalance 关闭、中断聚合、numa: <a href="http://jm.taobao.org/2017/03/23/20170323/">链接</a></p>
<h2 id="双机房">双机房</h2>
<h2 id="源码分析">源码分析</h2>
]]></content>
        </item>
        
        <item>
            <title>GOMAXPROCS</title>
            <link>https://xujianhai.fun/posts/max_proc/</link>
            <pubDate>Sat, 07 Mar 2020 20:04:32 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/max_proc/</guid>
            <description>这个是年前的一个案例, 通过团队成员解决, 因为比较经典, 还是写下blog进行记录.
 背景 随着业务接入, 服务的集群cpu逐渐上涨到 40%, 有时候流量一段时间上涨, 就会触发cpu 80% 报警, 常规情况下一般是简单扩容就好了, 但是本着cpu优化的角度, 开始进行了profile.
因为我们的实现基于go的, 直接用 go pprof 进行cpu 分析 就可以了. 结果发现, runtime.findrunnable 的cpu占比比较高, 通过搜索发现, 可能是因为 cpu设置的问题. 于是进行了环境变量的打印, golang默认的 cpu 是获取物理机的cpu: 128, 但是我们的服务是部署在 k8s 上的, cpu 的数量应该是通过 MY_CPU_LIMIT 获取所在容器的cpu:16, 中间差了7倍的数量
但是为什么 cpu 数量设置的不正确会影响 cpu 利用率呢？
这个需要讲到 GOMAXPROCS 的参数了, 这个参数规定了 P 的最大数量, 默认取值是 cpu数量, 通过设置 最大并行度(GOMAXPROCS) 为 cpu 数量, 可以充分利用每个cpu, 避免线程切换间的代价. 如果说将 GOMAXPROCS 设置成了128, 首先并行执行go代码的线程数膨胀, 但是由于 k8s 容器对于cpu的约束，导致只有 16个cpu 运行 128个线程 (至少128个, 因为系统调用的线程是不受 GOMAXPROCS 约束的)</description>
            <content type="html"><![CDATA[<blockquote>
<p>这个是年前的一个案例, 通过团队成员解决, 因为比较经典, 还是写下blog进行记录.</p>
</blockquote>
<h2 id="背景">背景</h2>
<p>随着业务接入, 服务的集群cpu逐渐上涨到 40%, 有时候流量一段时间上涨, 就会触发cpu 80% 报警, 常规情况下一般是简单扩容就好了, 但是本着cpu优化的角度, 开始进行了profile.</p>
<p>因为我们的实现基于go的, 直接用 go pprof 进行cpu 分析 就可以了. 结果发现, runtime.findrunnable 的cpu占比比较高, 通过搜索发现, 可能是因为 cpu设置的问题. 于是进行了环境变量的打印, golang默认的 cpu 是获取物理机的cpu: 128, 但是我们的服务是部署在 k8s 上的, cpu 的数量应该是通过 <code>MY_CPU_LIMIT</code> 获取所在容器的cpu:16, 中间差了7倍的数量</p>
<p>但是为什么 cpu 数量设置的不正确会影响 cpu 利用率呢？</p>
<p>这个需要讲到 GOMAXPROCS 的参数了, 这个参数规定了 P 的最大数量, 默认取值是 cpu数量, 通过设置 最大并行度(GOMAXPROCS) 为 cpu 数量, 可以充分利用每个cpu, 避免线程切换间的代价. 如果说将 GOMAXPROCS 设置成了128, 首先并行执行go代码的线程数膨胀, 但是由于 k8s 容器对于cpu的约束，导致只有 16个cpu 运行 128个线程 (至少128个, 因为系统调用的线程是不受 GOMAXPROCS 约束的)</p>
<h2 id="解决方案">解决方案</h2>
<p>直接获取k8s内部的环境变量 <code>MY_CPU_LIMIT</code>, 然后设置到procs中: <code>runtime.GOMAXPROCS(n)</code>, 上线后 集群cpu利用率逐渐降低了10%+</p>
<h2 id="后记">后记</h2>
<p>根据我司的<a href="https://www.infoq.cn/article/mu-1bFHNmrdd0kybgPXx">分享</a>, 建议是 <code>export GOMAXPROCS=$MY_CPU_LIMIT-1</code>, 为什么减1, 是预留核给系统线程(维护进程和线程的上下文信息以及线程切换). 打算尝试下, 虽然是从pct99的角度出发</p>
<p>怎么更好的方式 cpu数量和 核数不对呢? 通过 http://ip:port/debug/pprof/ 可以看 threadcreate 的数量, 如果非常大, 一般是 maxprocs 不正常</p>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq Heartbeat Timeout</title>
            <link>https://xujianhai.fun/posts/rocketmq-heartbeat-timeout/</link>
            <pubDate>Sat, 07 Mar 2020 09:35:43 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-heartbeat-timeout/</guid>
            <description>背景 最近线上发现了一些报警: &amp;ldquo;send heart beat to broker add: xxx error: request timeout&amp;rdquo;, 同时伴随着服务重启, 会出现consumer 流量短时间降低, 同时 consumer的连接创建也很缓慢
排查 通过关键字匹配, 发现这个是 rocektmq-golang-sdk 的一处错误打印, 是心跳命令请求broker超时的场景下打印的
既然是请求rocketmq超时了, 直接登录到线上rocketmq broker查看负载, 但是通过top执行发现cpu和内存占比都比较正常, 同时 netstat -anp | grep pid 扫描的socket的数量也只有几千个,没有异常点.
没有线索的情况下, 我们继续排查日志内容, 通过 tailf broker.log 一段时间后, 发现有一些类似 &amp;ldquo;event queue size 10000 enough, so drop this event CLOSE&amp;rdquo; 和 &amp;ldquo;event queue size 10000 enough, so drop this event CONNECT&amp;rdquo; 的日志, 同样进行关键字匹配, 发现这段逻辑是 rocketmq 对event的抽象处理, event比如: CONNECT/CLOSE/IDLE/EXCEPTION, 以 CLOSE 为例, 当rocketmq netty server 监听到 主动关闭或者被动关闭 连接的时候, 会实例化一个 CLOSE 类型的 event信息 投递到了 eventQueue, 这个 eventQueue 大小是 1w, 当大小大于 1w 的时候, 就会投递失败 (这里有个坑), eventQueue 投递后的消息是由一个单线程异步处理的, 线程会回调根据注册的listener进行回调, 这一块逻辑参考 NettyRemotingServer#channelInactive、NettyRemotingAbstract#putNettyEvent 和 NettyRemotingAbstract#run, 注册的回调逻辑实现在 ClientHousekeepingService#onChannelClose, 回调都做了什么事情呢?</description>
            <content type="html"><![CDATA[<h2 id="背景">背景</h2>
<p>最近线上发现了一些报警: &ldquo;send heart beat to broker add: xxx error: request timeout&rdquo;, 同时伴随着服务重启, 会出现consumer 流量短时间降低, 同时 consumer的连接创建也很缓慢</p>
<h2 id="排查">排查</h2>
<p>通过关键字匹配, 发现这个是 rocektmq-golang-sdk 的一处错误打印, 是心跳命令请求broker超时的场景下打印的</p>
<p><img src="/rocketmq-sync-timeout.png" alt="rocketmq-sync-timeout" title="rocketmq-sync-timeout"></p>
<p>既然是请求rocketmq超时了, 直接登录到线上rocketmq broker查看负载, 但是通过top执行发现cpu和内存占比都比较正常, 同时 <code>netstat -anp | grep pid</code> 扫描的socket的数量也只有几千个,没有异常点.</p>
<p>没有线索的情况下, 我们继续排查日志内容, 通过 <code>tailf broker.log</code> 一段时间后, 发现有一些类似 &ldquo;event queue size 10000 enough, so drop this event CLOSE&rdquo; 和 &ldquo;event queue size 10000 enough, so drop this event CONNECT&rdquo; 的日志, 同样进行关键字匹配, 发现这段逻辑是 rocketmq 对event的抽象处理, event比如: CONNECT/CLOSE/IDLE/EXCEPTION, 以 CLOSE 为例, 当rocketmq  netty server 监听到 主动关闭或者被动关闭 连接的时候, 会实例化一个 CLOSE 类型的 event信息 投递到了 eventQueue, 这个 eventQueue 大小是 1w, 当大小大于 1w 的时候, 就会投递失败 (这里有个坑), eventQueue 投递后的消息是由一个单线程异步处理的, 线程会回调根据注册的listener进行回调, 这一块逻辑参考 NettyRemotingServer#channelInactive、NettyRemotingAbstract#putNettyEvent 和  NettyRemotingAbstract#run, 注册的回调逻辑实现在 ClientHousekeepingService#onChannelClose, 回调都做了什么事情呢?</p>
<ol>
<li>从producer注册信息中删除这个地址的channel对象</li>
<li>从consumer注册信息中删除这个地址的channel对象, 如果consumer group没有连接, 删除这个group, 并回调consumerIdsChangeListener</li>
<li>从filter注册信息中删除这个地址的注册</li>
</ol>
<p>如下图
<img src="/rocketmq-server-channel-close.png" alt="rocketmq-server-channel-close" title="rocketmq-server-channel-close.png"></p>
<p>因此, eventQueue 出现丢弃 event 的情况 是一个问题, 但是和我们的心跳超时并没有太紧密的关联, 并且 event的处理逻辑范围太广, 线索失去了踪迹.</p>
<p>回到rocketmq本身的心跳实现, rocketmq的心跳处理过程也很简单, 就是简单的内存注册, 也没有复杂的实现, 所以心跳处理应该非常快, 不应该存在处理耗时长的情况, 并且, 结合rocketmq processor的实现方式和配置参数, rocektmq heartbeat 的 线程池队列有 5w大小, 难道我们的心跳qps 已经这么高了? 通过内部注入的客户端打点发现, 我们心跳的qps成功 1.8k, 失败 2.8k, 加起来连 5k qps 也不到, 所以 远远低于线程池queue的配置, 那么, 只剩下一个假设了, 心跳处理的很慢, 但是为什么呢?</p>
<p>处理慢的情况, 要么是存在锁竞争, 要么是实现逻辑问题. 因为通过代码查看并没有明显的锁竞争行为, 因此使用火焰图进行了cpu profile: <code>./profiler.sh -d 60 -f profile.svg  pid</code>. 因为是线上场景, 不适合使用 lightweight-java-profiler，这里我们采用了 async-profiler. 注意: 需要打开如下开关</p>
<pre><code>echo 1 &gt; /proc/sys/kernel/perf_event_paranoid
echo 0 &gt; /proc/sys/kernel/kptr_restrict
</code></pre><p>下载路径: <a href="https://github.com/jvm-profiling-tools/async-profiler/blob/master/README.md">https://github.com/jvm-profiling-tools/async-profiler/blob/master/README.md</a></p>
<p>火焰图结果如下</p>
<p><img src="/rocketmq-timeout-profile.png" alt="rocketmq-timeout-profile" title="rocketmq-timeout-profile.png"></p>
<p>显然, ProducerManager 在 idle 和 closeEvent 分别触发的 doChannelCloseEvent 占用耗时过长，打开存在问题的函数如下</p>
<p><img src="/rocketmq-producer-close.png" alt="rocketmq-producer-close" title="rocketmq-producer-close.png"></p>
<p>根据代码的实现, 我们怀疑 for循环遍历 groupChannelTable 耗时长, 如果是这种场景, 那么, groupChannelTable 应该很大. 为了佐证我们的猜想, 进行了jmap内存分析: <code>jmap -dump:format=b,file=filename.hprof pid</code>, 通过小伙伴 jvisualvm 的分析, 发现 groupChannelTable 的key非常多, 如下图, 将近 140w</p>
<p><img src="/rocketmq-timeout-mapkey.png" alt="rocketmq-timeout-mapkey" title="rocketmq-timeout-mapkey"></p>
<p>根据目前debug的证据: groupChannelTable key太多, for 循环遍历耗时长, 导致处理 doChannelCloseEvent 处理耗时长. 但是这个跟心跳处理又有什么关系呢? 心跳逻辑的实现中, 存在doChannelCloseEvent#add的操作, 根据 ConcurrentHashMap 的内部实现注释, 迭代和add是存在锁竞争操作的, 注释如下:</p>
<pre><code>/**
 * TreeNodes used at the heads of bins. TreeBins do not hold user
 * keys or values, but instead point to list of TreeNodes and
 * their root. They also maintain a parasitic read-write lock
 * forcing writers (who hold bin lock) to wait for readers (who do
 * not) to complete before tree restructuring operations.
 */
static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; {
</code></pre><p>那么基本上可以破案了. groupChannelTable key 太多, 迭代时间长, TreeBin 在读写操作上存在锁竞争, 导致心跳超时. 至此, 心跳超时、eventqueue丢弃、cpu profile、heap dump 的内容串联起来了</p>
<h2 id="解决方案">解决方案</h2>
<p>根据分析的内容, 问题出现在了 groupChannelTable 的遍历问题上, 那么, 是否可以 减少 groupChannelTable 的key数量呢? 根据 groupChannelTable的定义, key 是 producer client传递的group, 这个group是 transactional 场景中使用的, 参考: <a href="https://rocketmq.apache.org/docs/core-concept/">https://rocketmq.apache.org/docs/core-concept/</a> . 那么, 如果我们让 业务方的客户端 尽量复用 group, 比如说让业务的一个服务中的producer 都是用服务的名字作为group, 那么, 就能大大减少 groupChannelTable 的大小, 并且避免 groupChannelTable 此前的恶性循环 (每次遍历耗时很长, 耗时长又导致关闭缓慢, 进而导致groupChannelTable变大, 因为关闭过程中可能又有新的连接). 确定这个解决方案后, 迅速coding、测试环境部署、上线. 上线后不久 错误日志开始跌0, 同时 超时的qps 也降低到了0, 重新cpu profile得到火焰图也正常了, 如下</p>
<p><img src="/rocketmq-timeout-normal.png" alt="rocketmq-timeout-normal" title="rocketmq-timeout-normal.png"></p>
<h2 id="后记">后记</h2>
<ol>
<li>
<p>groupChannelTable 的key太多, 还是一个很疑惑的点, 为什么会触发这种问题, 仅仅是因为 我们 producer 很多?</p>
</li>
<li>
<p>之前讲到consumer也存在连接耗时长、流量短时间降低的现象</p>
</li>
</ol>
<p>如果队列中有一个心跳请求的处理出现的耗时长的情况，那么后面的请求的耗时也会相应增加, 并且consumer创建的时候, 需要向所有的broker发起一次心跳操作进行订阅信息的注册。在用admin命令获取连接的时候，是根据 retry topic下注册的consumer获取信息，因此，如果订阅信息没有注册上去，admin也就无法获取相关的连接. Producer 的发送行为就不依赖于心跳的成功，因此producer发送没有受到影响.</p>
]]></content>
        </item>
        
        <item>
            <title>Protobuf</title>
            <link>https://xujianhai.fun/posts/protobuf/</link>
            <pubDate>Sat, 11 Jan 2020 15:16:40 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/protobuf/</guid>
            <description>gogo-protobuf 扩展了 protobuf 的使用姿势, 不仅添加了丰富的插件: string/euqla/marshal/unmarshal, 性能上还进行了优化. gogo-protobuf 的插件体系相对于原生的protobuf-go的插件实现(虽然只有一个grpc), 不仅丰富, 而且支持开关. 开关是借助于 描述符中extension 实现的.
gogo-protobuf 因为支持的插件体系比较多, 为此, 将插件分成了几种启用级别, 对外是不同的使用入口. 比如 protoc-gen-gogofast、protoc-gen-gogofaster、protoc-gen-gogoslick. 除了通过不同的入口, 还可以通过不同proto文件的参数定制, 比如 option (gogoproto.gostring_all) = true; 实现给每个message添加string的方法.
补充:  extension  extension 是 proto2 中支持的语法, 在新的pb文件中, 使用了 Any 进行了替代. 更多关于extension可以参考: https://developers.google.com/protocol-buffers/docs/proto#extensions, Any可以参照 https://developers.google.com/protocol-buffers/docs/proto3#any . 但是在 gogo的使用实现中, 还是用 extend 机制, proto2 用extend, proto3 使用本地登记的方式
2.validator
validator插件 https://github.com/mwitkow/go-proto-validators 提供了字段检查的功能, 会根据proto文件生成goalng validator代码文件.
protoc插件  protoc是支持插件的, 比如gogo-out其实就是去找gogo的插件, govalidators_out就是找 govalidators插件
其他深入的点 union group 类型.</description>
            <content type="html"><![CDATA[<p>gogo-protobuf 扩展了 protobuf 的使用姿势, 不仅添加了丰富的插件: string/euqla/marshal/unmarshal, 性能上还进行了优化.
gogo-protobuf 的插件体系相对于原生的protobuf-go的插件实现(虽然只有一个grpc), 不仅丰富, 而且支持开关. 开关是借助于 描述符中extension 实现的.</p>
<p>gogo-protobuf 因为支持的插件体系比较多, 为此, 将插件分成了几种启用级别, 对外是不同的使用入口. 比如 protoc-gen-gogofast、protoc-gen-gogofaster、protoc-gen-gogoslick.
除了通过不同的入口, 还可以通过不同proto文件的参数定制, 比如
<code>option (gogoproto.gostring_all) = true;</code> 实现给每个message添加string的方法.</p>
<h3 id="补充">补充:</h3>
<ol>
<li>extension</li>
</ol>
<p>extension 是 proto2 中支持的语法, 在新的pb文件中, 使用了 Any 进行了替代. 更多关于extension可以参考: <a href="https://developers.google.com/protocol-buffers/docs/proto#extensions,">https://developers.google.com/protocol-buffers/docs/proto#extensions,</a> Any可以参照 <a href="https://developers.google.com/protocol-buffers/docs/proto3#any">https://developers.google.com/protocol-buffers/docs/proto3#any</a> . 但是在 gogo的使用实现中, 还是用 extend 机制, proto2 用extend, proto3 使用本地登记的方式</p>
<p>2.validator</p>
<p>validator插件 <a href="https://github.com/mwitkow/go-proto-validators">https://github.com/mwitkow/go-proto-validators</a> 提供了字段检查的功能, 会根据proto文件生成goalng validator代码文件.</p>
<ol start="3">
<li>protoc插件</li>
</ol>
<p>protoc是支持插件的, 比如gogo-out其实就是去找gogo的插件, govalidators_out就是找 govalidators插件</p>
<ol start="4">
<li>其他深入的点
union group 类型. group类型已经废弃, 不赘述</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Kip 2018 12</title>
            <link>https://xujianhai.fun/posts/kip-2018-12/</link>
            <pubDate>Wed, 11 Dec 2019 19:49:58 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kip-2018-12/</guid>
            <description> reassignment 限流   摘要: 对 reassignment 的 replication 进行限流, 避免全局限流的导致isr落后的partition 的无法追上。这里的限流是动态的. 以前的通用的限流不进行废弃, 因为存在无isr而且 follower 用光带宽的时候, 这个限制还是需要的。新增加了两个配置: leader.reassignment.throttled.rate 和 follower.reassignment.throttled.rate, 前者是 leader broker 统一限流, 但是因为kafka reassignment 的 partition follower 可以有很多个, leader限流的话需要计算每个followerd的比例, 所以添加了 follower限流 kip: https://cwiki.apache.org/confluence/display/KAFKA/KIP-542%3A+Partition+Reassignment+Throttling  replication quota   摘要: client 的限流是通过延迟实现的, replica原来也有类似的限流 replica.fetch.max.bytes , 不过是 partition 级别的。新的方案, 添加了 LeaderQuotaRate 和 FollowerQuotaRate. kip: https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas  </description>
            <content type="html"><![CDATA[<ol>
<li>reassignment 限流</li>
</ol>
<ul>
<li>摘要: 对 reassignment 的 replication 进行限流, 避免全局限流的导致isr落后的partition 的无法追上。这里的限流是动态的. 以前的通用的限流不进行废弃, 因为存在无isr而且 follower 用光带宽的时候, 这个限制还是需要的。新增加了两个配置: <code>leader.reassignment.throttled.rate</code> 和 <code>follower.reassignment.throttled.rate</code>, 前者是 leader broker 统一限流, 但是因为kafka reassignment 的 partition follower 可以有很多个, leader限流的话需要计算每个followerd的比例, 所以添加了 follower限流</li>
<li>kip: <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-542%3A+Partition+Reassignment+Throttling">https://cwiki.apache.org/confluence/display/KAFKA/KIP-542%3A+Partition+Reassignment+Throttling</a></li>
</ul>
<ol start="2">
<li>replication quota</li>
</ol>
<ul>
<li>摘要: client 的限流是通过延迟实现的, replica原来也有类似的限流 <code>replica.fetch.max.bytes</code> , 不过是 partition 级别的。新的方案, 添加了 <code>LeaderQuotaRate</code> 和 <code>FollowerQuotaRate</code>.</li>
<li>kip: <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas">https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq Admin</title>
            <link>https://xujianhai.fun/posts/rocketmq-admin/</link>
            <pubDate>Sun, 17 Nov 2019 21:50:30 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-admin/</guid>
            <description>查看消费进度:
sh mqadmin consumerProgress -g $groupName -n ${ip:port} 查看客户端的连接信息
sh mqadmin consumerConnection -g $group -n ${ip:port} 查看topic状态
sh mqadmin topicStatus -t $topic -n ${ip:port} 按照时间重置offset
sh mqadmin resetOffsetByTime -t $topic -n ${ip:port} -g $group -s $ms 创建topic
sh mqadmin updateTopic -t $topic -n ${ip:port} -c $cluster -r 1 -w 1 -o true </description>
            <content type="html"><![CDATA[<p>查看消费进度:</p>
<pre><code>sh mqadmin consumerProgress -g $groupName -n ${ip:port}
</code></pre><p>查看客户端的连接信息</p>
<pre><code>sh mqadmin consumerConnection -g $group -n ${ip:port}
</code></pre><p>查看topic状态</p>
<pre><code>sh mqadmin topicStatus -t $topic  -n ${ip:port}
</code></pre><p>按照时间重置offset</p>
<pre><code>sh mqadmin resetOffsetByTime  -t $topic -n  ${ip:port} -g $group -s $ms
</code></pre><p>创建topic</p>
<pre><code>sh mqadmin updateTopic -t $topic  -n ${ip:port} -c $cluster -r 1 -w 1 -o true
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Kafka Controller Redesign</title>
            <link>https://xujianhai.fun/posts/kafka-controller-redesign/</link>
            <pubDate>Sun, 10 Nov 2019 11:10:24 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-controller-redesign/</guid>
            <description>最近学习 kafka 相关的kip, 发现了一个 kafka controller redesign 的设计的文章, 这里叙述一下:
  kip: https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#
  kip: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Redesign
  里面主要就 kafka controller 当前遇到的问题 进行了总结并提出了 部分解决方案:
 zk异步写入 控制请求和 数据请求使用优先级队列分离 使用 generation 区分 controller -&amp;gt; broker 的请求信息 清晰的代码组织: 逻辑简化收敛 使用单线程的事件处理 简化 controller 的并发实现 (1.1.0)  </description>
            <content type="html"><![CDATA[<p>最近学习 kafka 相关的kip, 发现了一个 kafka controller redesign 的设计的文章, 这里叙述一下:</p>
<ul>
<li>
<p>kip: <a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#">https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#</a></p>
</li>
<li>
<p>kip: <a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Redesign">https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Redesign</a></p>
</li>
</ul>
<p>里面主要就 kafka controller 当前遇到的问题 进行了总结并提出了 部分解决方案:</p>
<ol>
<li>zk异步写入</li>
<li>控制请求和 数据请求使用优先级队列分离</li>
<li>使用 generation 区分 controller -&gt; broker 的请求信息</li>
<li>清晰的代码组织: 逻辑简化收敛</li>
<li>使用单线程的事件处理 简化 controller 的并发实现 (1.1.0)</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Kafka Group Codereview</title>
            <link>https://xujianhai.fun/posts/kafka-group-codereview/</link>
            <pubDate>Sat, 09 Nov 2019 21:52:25 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-group-codereview/</guid>
            <description>最近学习 kafka-connect 的设计和实现. 其中设计到 group member protocol 的内容. 这里展开学习.
kafka group member 协议 主要参考 AbstractCoordinator 的实现流程 以及 ConsumerCoordinator 的实现.
整体生命流程: 找到一个Node -&amp;gt; find coordinator protocol -&amp;gt; onJoinPrepare(子类) -&amp;gt; join group protocol -&amp;gt; sync group protocol(onJoinLeader 包含了任务分配的结果/follower 空的assignment) -&amp;gt; enable heartbeat -&amp;gt;onJoinComplete(子类处理分配结果). 心跳线程处理 coordinator的网络连接. leader 是 coordinator 选举的. ConsumerCoordinator子类 从上面的流程中可以知道, 继承 AbstractCoordinator 的子类, 需要实现 onJoinPrepare、metadata、onLeavePrepare、performAssignment、onJoinComplete
 onJoinPrepare: 在 eager 模式下, 上次分配的内容全部 revoked; 在 COOPERATIVE 模式下, 只撤回不在定于的 topic 的 partition. metadata: sendJoinGroupRequest使用的数据信息, 用于后面的分配.</description>
            <content type="html"><![CDATA[<p>最近学习 kafka-connect 的设计和实现. 其中设计到 group member protocol 的内容. 这里展开学习.</p>
<h2 id="kafka-group-member-协议">kafka group member 协议</h2>
<p>主要参考 AbstractCoordinator 的实现流程 以及 ConsumerCoordinator 的实现.</p>
<h3 id="整体生命流程">整体生命流程:</h3>
<pre><code>找到一个Node -&gt; find coordinator protocol -&gt; onJoinPrepare(子类) -&gt; join group protocol -&gt; sync group protocol(onJoinLeader 包含了任务分配的结果/follower 空的assignment) -&gt; enable heartbeat -&gt;onJoinComplete(子类处理分配结果). 心跳线程处理 coordinator的网络连接. leader 是 coordinator 选举的. 
</code></pre><h3 id="consumercoordinator子类">ConsumerCoordinator子类</h3>
<p>从上面的流程中可以知道, 继承 AbstractCoordinator 的子类, 需要实现 onJoinPrepare、metadata、onLeavePrepare、performAssignment、onJoinComplete</p>
<ul>
<li>onJoinPrepare: 在 eager 模式下, 上次分配的内容全部 revoked; 在 COOPERATIVE 模式下, 只撤回不在定于的 topic 的 partition.</li>
<li>metadata: sendJoinGroupRequest使用的数据信息, 用于后面的分配. 会提交上次分配的 assignemnt.</li>
<li>performAssignment: 执行分配</li>
<li>onJoinComplete: 回调assigner处理分配结果</li>
</ul>
<h3 id="学习">学习</h3>
<ol>
<li>
<p>注意, 在配置的时候, 每个source集群需要配置一个不一样的 groupId.
kafkaConsumer 支持 正则表达式订阅, 并且动态更新元数据, 保证及时订阅到 新增的 topic.</p>
</li>
<li>
<p>poll 调用的时候 会执行 join group 流程. 通过不断调用 cleints.poll 就可以保证自己在 group 里面. 通过配置 <code>max.poll.interval.ms</code> 避免假死过程中持续的心跳导致 partition 持有问题的活锁.</p>
</li>
</ol>
<p>partition &lt; consumers 会发生什么呢?</p>
]]></content>
        </item>
        
        <item>
            <title>Kafka Group Kip</title>
            <link>https://xujianhai.fun/posts/kafka-group-kip/</link>
            <pubDate>Thu, 07 Nov 2019 09:54:35 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-group-kip/</guid>
            <description>这里主要讨论 kafka group 相关的协议: rebalance, partition 等
https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner
https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol
https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request
https://cwiki.apache.org/confluence/display/KAFKA/KIP-379%3A+Multiple+Consumer+Group+Management
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=89070828
https://cwiki.apache.org/confluence/display/KAFKA/KIP-341%3A+Update+Sticky+Assignor%27s+User+Data+Protocol
https://cwiki.apache.org/confluence/display/KAFKA/KIP-389%3A+Introduce+a+configurable+consumer+group+size+limit
在 group非常大的时候, rebalance 次数就会增加; rebalance 时间取决于最慢的consumer, group 越大, 慢consumer出现的概率就越大. 除此之外, group coordinator 可能多个 group 共享的, 所以彼此会影响. 这个提案中, 提出了 `consumer.group.max.size` 的概念, 对 server端进行了保护. 当有超过数量的member加入, 将会收到 异常. https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request
之前都是 broker 在 收到 joinGroup request 的时候, 返回 uuid 给 client 作为 member.id, 在边缘case中(client不断重启加入), 可能导致内存膨胀. 这个 proposal 中, 就是需要用户手动提交 memebr.id https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances
为了避免rebalance导致 有状态的应用程序的数据迁移. 目前的状态 broker group status: Running -&amp;gt; member JoinGroupRequest -&amp;gt; broker group status: PREPARE_REBALANCE -&amp;gt; broker group status: COMPLETING_REBALANCE -&amp;gt; sync group request (group members) -&amp;gt; SyncGroupResponse (broker send to memebrs) 其中, 第一个加入的 member 就是 group leader.</description>
            <content type="html"><![CDATA[<p>这里主要讨论 kafka group 相关的协议: rebalance, partition 等</p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner">https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol">https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request">https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-379%3A+Multiple+Consumer+Group+Management">https://cwiki.apache.org/confluence/display/KAFKA/KIP-379%3A+Multiple+Consumer+Group+Management</a></p>
<p><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=89070828">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=89070828</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-341%3A+Update+Sticky+Assignor%27s+User+Data+Protocol">https://cwiki.apache.org/confluence/display/KAFKA/KIP-341%3A+Update+Sticky+Assignor%27s+User+Data+Protocol</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-389%3A+Introduce+a+configurable+consumer+group+size+limit">https://cwiki.apache.org/confluence/display/KAFKA/KIP-389%3A+Introduce+a+configurable+consumer+group+size+limit</a></p>
<pre><code>在 group非常大的时候, rebalance 次数就会增加; rebalance 时间取决于最慢的consumer, group 越大, 慢consumer出现的概率就越大. 除此之外, group coordinator 可能多个 group 共享的, 所以彼此会影响. 
这个提案中, 提出了 `consumer.group.max.size` 的概念, 对 server端进行了保护. 当有超过数量的member加入, 将会收到 异常. 
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request">https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request</a></p>
<pre><code>之前都是 broker 在 收到 joinGroup request 的时候, 返回 uuid 给 client 作为 member.id, 在边缘case中(client不断重启加入), 可能导致内存膨胀. 这个 proposal 中, 就是需要用户手动提交 memebr.id
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances">https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances</a></p>
<pre><code>为了避免rebalance导致 有状态的应用程序的数据迁移.
目前的状态 broker group status: Running -&gt; member JoinGroupRequest -&gt; broker group status: PREPARE_REBALANCE -&gt; broker group status: COMPLETING_REBALANCE -&gt; sync group request (group members) -&gt; SyncGroupResponse (broker send to memebrs)
其中, 第一个加入的 member 就是 group leader.
这个方案的实现, 是持久化 member.id(需要用户主动配置`group.instance.id`), 这样, 每次client 重启就不会重新生成 member.id. 在执行 assignmemt的时候, 可以根据 member.id 执行相同的分配. 这个就是 'static membership'. 没有配置 `group.instance.id` 的情况下, 则是 动态的, member.id 由 coordinator 分配. 
membership rebalance protocol 被触发的情况: 
1. 新的 member join 
2. leader rejoin 
3. existing member offline 
4. member leave group(leave group request) 
比较有意思的是, 是添加了 `group.instance.id`, 和 member.id 一起用在了  Join/Sync/Heartbeat/OffsetCommit request/responses. member.id 是 broker 生层的递增的数字.
在proposal中, 列举了 滚动升级中 range assigner 的列子, 如果是 dynamic membership, 就会引起迁移; 如果是 static membership, 就不会.
使用了 static membership, 只有 session timeout 才会在 broker 中移除 client. 之前在 COMPLETING_REBALANCE 阶段就会移除。
为了维护 `group.instance.id` 的分配关系, 会将映射存储在 “_consumer_offsets” 的 topic 里面
</code></pre><ul>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-134%3A+Delay+initial+consumer+group+rebalance">https://cwiki.apache.org/confluence/display/KAFKA/KIP-134%3A+Delay+initial+consumer+group+rebalance</a></li>
</ul>
<pre><code>GroupCoordinator 在收到 加入 新的或者empty group, group 状态会进入 InitialRebalance, 并延迟 min(rebalanceTimeout, group.initial.rebalance.delay.ms) 等待其他的 member加入, 如果有member加入, 那么 延迟就会重置. 过期后就会进入 PreparingRebalance. 
0.11.0 中发布的功能
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Kafka Mirror Review</title>
            <link>https://xujianhai.fun/posts/kafka-mirror-review/</link>
            <pubDate>Sat, 02 Nov 2019 23:33:49 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-mirror-review/</guid>
            <description></description>
            <content type="html"><![CDATA[]]></content>
        </item>
        
        <item>
            <title>Kafka Connect Codereview</title>
            <link>https://xujianhai.fun/posts/kafka-connect-codereview/</link>
            <pubDate>Sat, 02 Nov 2019 23:32:49 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-connect-codereview/</guid>
            <description>kafka 支持两种模式: standalone 和 distributed. 分开来讲.
组件 kafka-connect 主要的组件如下
 Herder: 管理 worker、statusBackingStore、configBackingStore、offsetBackingStore. Connect: kafka-connect 组件的生命周期的管理 statusBackingStore: task/worker 状态变化, 都需要调用这个 offsetBackingStore: 管理offset configBackingStore: 管理配置 ConfigProvider: 管理配置信息  standalone standalone 的实现比较简单, 这里简单说下 生命周期.
生命周期:
流程: ConnectStandalone#main -&amp;gt; StandaloneHerder#putConnectorConfig -&amp;gt; StandaloneHerder#startConnector -&amp;gt; 启动 connector + 启动 task 启动 connector: Worker#startConnector -&amp;gt; WorkerConnector#start (管理connector的生命周期) -&amp;gt; Connector#start 启动task: Worer#startTask -&amp;gt; WorkerTask#run...execute (executorService执行) StatusBackingStore: MemoryStatusBackingStore MemoryConfigBackingStore: MemoryConfigBackingStore standalone的依赖 XXXBackingStore 都是 memory 的实现, 不赘述.
学习 kafka-connect 很喜欢用 callback 参数, 将结果callback 出去, 一定程度上能够增强 并发度</description>
            <content type="html"><![CDATA[<p>kafka 支持两种模式: standalone 和 distributed. 分开来讲.</p>
<h2 id="组件">组件</h2>
<p>kafka-connect 主要的组件如下</p>
<ul>
<li>Herder: 管理 worker、statusBackingStore、configBackingStore、offsetBackingStore.</li>
<li>Connect: kafka-connect 组件的生命周期的管理</li>
<li>statusBackingStore: task/worker 状态变化, 都需要调用这个</li>
<li>offsetBackingStore: 管理offset</li>
<li>configBackingStore: 管理配置</li>
<li>ConfigProvider: 管理配置信息</li>
</ul>
<h2 id="standalone">standalone</h2>
<p>standalone 的实现比较简单, 这里简单说下 生命周期.</p>
<p>生命周期:</p>
<pre><code>流程: 
ConnectStandalone#main -&gt; StandaloneHerder#putConnectorConfig -&gt; StandaloneHerder#startConnector -&gt; 启动 connector + 启动 task 

启动 connector:
     Worker#startConnector -&gt; WorkerConnector#start (管理connector的生命周期) -&gt; Connector#start
启动task:
    Worer#startTask -&gt; WorkerTask#run...execute (executorService执行)

StatusBackingStore: MemoryStatusBackingStore 
MemoryConfigBackingStore: MemoryConfigBackingStore
</code></pre><p>standalone的依赖 XXXBackingStore 都是 memory 的实现, 不赘述.</p>
<h3 id="学习">学习</h3>
<p>kafka-connect 很喜欢用 callback 参数, 将结果callback 出去, 一定程度上能够增强 并发度</p>
<h2 id="分布式">分布式</h2>
<p>kafka 的分布式 和 kafka standalone, 除了在 storage 上的使用不同(KafkaConfigBackingStore、KafkaStatusBackingStore、KafkaOffsetBackingStore), 还使用了 group member protocol 管理分布式的worker, connector/task 的任务分配 通过 group member protocol 管理. 为了避免单个worker挂掉导致任务的重新分配, 使用 IncrementalCooperativeAssignor 管理任务的分配, 在这个实现中, 如果 单个worker 挂了, 这个worker的任务就会移动到其他worker中, 而不是无脑的分配所有的worker.</p>
<h3 id="关于任务分配的主要类">关于任务分配的主要类:</h3>
<ul>
<li>DistributedHerder: 在 joinPrepare, 暂停所有 connector/task; joinComplete, 记录结果</li>
<li>WorkerCoordinator: group member 协议处理 worker member 的加入 退出. metadata 中存放了 分配情况: ExtendedAssignment. poll 行为不了解&hellip;&hellip; client.poll 是为了触发 rebalance?</li>
<li>ExtendedAssignment: 除了包含 分配的 connector 和 tasks, 还有 revoked 的 connector 和 tasks.</li>
<li>IncrementalCooperativeAssignor的学习, 需要理解下 kip: <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect">https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect</a>. 先分配 connector, 再分配 tasks, 而不是 connector分配一个, 在分配 connector 的task, 避免 单task的connector情况 导致 奇偶worker 分配不均匀</li>
<li>KafkaConfigBackingStore: 这个存储需要处理 compacted 的复杂情况.配置都是先 producer 发送消息, consumer 拉取配置后才放到本地的, 整体配置信息进行了异步化. 除此之外, 通过 offset 信息, 表示当前的 kafka-connect worker 的状态. 所以在 ConnectProtocol#WorkerState 中, 使用了  offset 字段作为状态信息. 在 group member protocol 管理的任务分配环节, 需要验证 leader 的 KafkaConfigBackingStore 中的 offset 和 成员中最大的 offset 的关系, 需要确保 leader offset 是最大的, 否则不能进行分配.</li>
<li>KafkaOffsetBackingStore: 同上</li>
<li>KafkaStatusBackingStore: 同上</li>
</ul>
<h3 id="问题">问题</h3>
<ul>
<li>新加入的 connector是怎么分配的呢?</li>
</ul>
<p>DistributedHerder 会定时调用tick, tick中有一个步骤: handleRebalanceCompleted; 每次rebalance结束, 就会启动新增的 assignment(task/connector).
在调用 handleRebalanceCompleted 需要确保 自己在 group member protocol 活着, 通过 #poll 确定.</p>
<ul>
<li>ExtendedAssignment 中 revokedConnectorIds revokedTaskIds 的设计眼里不了解</li>
<li>WorkerState 中 url 和 offset; offset 是 configBackingStore 中的offset, 在分布式中, 就是 KafkaConfigBackingStore 的 offset (consumer消费到的offset), 表示当前 worker 最新的状态; url 是 Header的, 每个集群一个 Header, 对应的 url 是 source的.</li>
<li>AbstractCoordinator: group memeber protocol 的实现骨架. 找到一个Node -&gt; find coordinator protocol -&gt; onJoinPrepare(子类) -&gt; join group protocol -&gt; sync group protocol(onJoinLeader 包含了任务分配的结果/follower 空的assignment) -&gt; enable heartbeat -&gt;onJoinComplete(子类处理分配结果). 心跳线程处理 coordinator的网络连接. leader 是 coordinator 选举的. 注意, 在配置的时候, 每个source集群需要配置一个不一样的 groupId.</li>
</ul>
<h3 id="疑惑">疑惑</h3>
<p>group member protocol 的kip 没找到, 很疑惑</p>
<h2 id="isolation">Isolation:</h2>
<p>类隔离的本质就是 通过不同的classloader 来隔离不同的实现. 在 kafka-connect 中, Source/Sink/Connector/Converter/Transformation/ConfigProvider/RestConnection/ConnectorClientOverridePolicy 都是插件, 允许用户自定义实现, kafka-connect通过加载用户指定的路径信息加载这些插件. 需要注意的是, 用户的指定路径可以是多个目录; 其次, 这些插件不希望彼此的依赖相互影响.</p>
<p>在看实现的时候, 有两个 classLoader:</p>
<ol>
<li>PluginClassLoader: 每一个配置的 jar文件/.class文件 都是一个 类加载器, 实现隔离机制. 在需要隔离的类(不在黑名单或者在白名单), 用 UrlClassLoader 进行加载, 否则用 父类 ClassLoader 进行加载 (在实现的时候, 有些调用来自connect&quot;主路径&rdquo;, 有些来自connector/task, 后者肯定已经在类隔离机制中, 所以, 在调用实例化 converter 等对象的时候, 使用当前 classloader 就可以了)</li>
<li>DelegatingClassLoader: 维护扫描到的用户添加的类信息</li>
</ol>
<p>在扫描用户类路径的时候, 有两种扫描的方式:</p>
<ol>
<li>reflection 获取具体的类: Connector/Converter/HeaderConverter/Transformation: 用户多种自定义实现.  一个 kafka-connect 有多个实现.</li>
<li>利用 serviceLoader 获取 具体的实现类: ConfigProvider/ConnectRestExtension/ConnectorClientConfigOverridePolicy: 一个 kafka-connect 就一个.</li>
</ol>
<h3 id="学习-1">学习</h3>
<ol>
<li>
<p>org.reflection 包不错. 可以从 url 路径列表下 获取 类信息</p>
</li>
<li>
<p>在实现类隔离机制的时候, 通过 ClassLoader#ParallelLoaders 方法扩大并行度. 在类加载过程中, 如果是 允许并行的话, 锁住的是 每个类名字; 否则的话, 锁住的是 ClassLoader 当前实现类对象.</p>
</li>
</ol>
<h2 id="rest">rest</h2>
<p>servlet 设计. 暂时不赘述.</p>
<h3 id="疑惑-1">疑惑</h3>
]]></content>
        </item>
        
        <item>
            <title>Kafka Connect Design Kip</title>
            <link>https://xujianhai.fun/posts/kafka-connect-design-kip/</link>
            <pubDate>Sat, 02 Nov 2019 23:31:22 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-connect-design-kip/</guid>
            <description>早期的设计 最早的设计可以看: KIP-26 - Add Kafka Connect framework for data import/export,
通过kip, 可以发现, connect设计的目标是 导入和导出数据. 设计目标:
 只聚焦数据处理 并行度的支持. 能够支持大量数据的拷贝 尽可能提供 准确一次的分发. 管理元数据 设计良好的connector api. 易于扩展 流式和批处理的支持 Strandalone 和 集群的支持  为什么是基于 kafka 构建一套connect, 而不是其他框架呢?
 kafka 本身就有并行度的概念 kafka 本身良好的 容错能力, 使得编码简单 kafka 提供了 准确一次、最多一次、最少一次  除此之外, 其他的框架 本身也是从一个具体的case进行 泛化, 不能很好的利用kafka本身的优势。 学习成本和复杂度很高. 部分依赖于 yarn, 对于大集群是好处, 但是应该是 利用而不是依赖. 最后, 其他框架的技术栈 和 kafka 不匹配.
那么, 为什么 kafka-connect 和 kafka 放在一起呢?
 文档入口友好度 生态化.</description>
            <content type="html"><![CDATA[<h3 id="早期的设计">早期的设计</h3>
<p>最早的设计可以看: <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=58851767">KIP-26 - Add Kafka Connect framework for data import/export</a>,</p>
<p>通过kip, 可以发现, connect设计的目标是 导入和导出数据. 设计目标:</p>
<ul>
<li>只聚焦数据处理</li>
<li>并行度的支持. 能够支持大量数据的拷贝</li>
<li>尽可能提供 准确一次的分发.</li>
<li>管理元数据</li>
<li>设计良好的connector api. 易于扩展</li>
<li>流式和批处理的支持</li>
<li>Strandalone 和 集群的支持</li>
</ul>
<p>为什么是基于 kafka 构建一套connect, 而不是其他框架呢?</p>
<ul>
<li>kafka 本身就有并行度的概念</li>
<li>kafka 本身良好的 容错能力, 使得编码简单</li>
<li>kafka 提供了 准确一次、最多一次、最少一次</li>
</ul>
<p>除此之外, 其他的框架 本身也是从一个具体的case进行 泛化, 不能很好的利用kafka本身的优势。 学习成本和复杂度很高. 部分依赖于 yarn, 对于大集群是好处, 但是应该是 利用而不是依赖. 最后, 其他框架的技术栈 和 kafka 不匹配.</p>
<p>那么, 为什么 kafka-connect 和 kafka 放在一起呢?</p>
<ul>
<li>文档入口友好度</li>
<li>生态化. 在使用kafka的时候, 不需要到处找其他框架的connector</li>
<li>更好的集成kafka相关的接口</li>
</ul>
<h3 id="模块设计">模块设计</h3>
<p>设计上, kafka connect 主要有一下三个模块:</p>
<ol>
<li>数据模型的定义. 和 kafka 使用byteArray 不一样. 数据模型将 序列化/反序列化作为可插拔的组件. 模型定义是 schema.</li>
<li>与外部系统交互的 connector 定义. connector负责切分数据(监听外部系统变化更新数据切分), task 负责生产和消费 records</li>
<li>worker模型. connector执行模式、coordination、配置存储、offset 存储、offset commit 管理. 负责任务的负载均衡, 以及 rest 接口支持. Strandalone 和 分布式模式</li>
</ol>
<h3 id="关于数据存储">关于数据存储:</h3>
<p>主要存储的对象: 用户提供的connector配置、connector生产的task配置、offset数据. 目前主要是 本地文件 和 kafka/zk 两种实现方式, 对应 standalone 和 分布式模式.</p>
<p>kafka sink 直接复用 offset. kafka source 在 offset 数据上需要添加一些元信息.</p>
<h3 id="分发保证">分发保证:</h3>
<p>kafka connect 支持 三种不同的分发模式. 至少一次、最多一次、准确一次.</p>
<ul>
<li>至少一次: source connector, producer 支持 flush; sink connector, consumer 本身支持 offset 提交</li>
<li>最多一次: 机制和上面类似, 但是可以通过缓存一批数据 进行提交</li>
<li>准确一次: source connector, 通过 <a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer">幂等producer</a> 实现; sink connector, 可以借助于 topic + partition + offset 实现</li>
</ul>
<h3 id="进程管理和集群资源管理">进程管理和集群资源管理</h3>
<p>connect 框架不负责 进程的 开始/暂停/重启. 可以利用yarn/mesos/k8s, 或者不需要其他框架. 使用方式:</p>
<ul>
<li>kafka connector 作为一个服务. 进程管理可以通过 Chef/Puppet/Ansible/Salt 实现, connecotor 通过 rest api 提交.</li>
<li>资源管理 connector. 通过 mesos/yarn/k8s/Slider/Marathon 管理</li>
<li>standalone 模式</li>
<li>嵌入式模式. 提供了嵌入式 api</li>
</ul>
<h3 id="例子">例子</h3>
<p>jdbc、hdfs、log 和 mirror make</p>
<h3 id="接口">接口</h3>
<p>cli: standalone 和 cluster 模式分两种
rest api:
embedded api:</p>
<h3 id="反对方案">反对方案</h3>
<ol>
<li>kafka connect 是和 kafka 放在一起的. 接口更加一致, 管理更加方便</li>
<li>不使用第三方的流处理框架: kafka-connect 如果使用 流处理框架, 那么 流处理实现会很复杂, 涉及 source/sink. 而且, 流处理框架不会太关心 kafka-connect</li>
<li>不支持 push-based source connectors. 针对任务调度变得复杂,集成其他第三方数据源 也复杂.</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Kafka Connect Kip</title>
            <link>https://xujianhai.fun/posts/kafka-connect-kip/</link>
            <pubDate>Sat, 02 Nov 2019 23:30:41 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-connect-kip/</guid>
            <description>kip 最近研究kafka-connect, 相关的kip 整理:
KIP-521: Enable redirection of Connect&amp;rsquo;s log4j messages to a file by default
KIP-507: Securing Internal Connect REST Endpoints
KIP-495: Dynamically Adjust Log Levels in Connect
可以通过 网络请求 动态调整 loglevel. 重启后修改是丢失的 KIP-481: SerDe Improvements for Connect Decimal type in JSON
KIP-475: New Metrics to Measure Number of Tasks on a Connector
添加了 connector-total-task-count 、connector-running-task-count、connector-paused-task-count、connector-failed-task-count、connector-unassigned-task-count、connector-destroyed-task-count 的监控指标 KIP-465: Add Consolidated Connector Endpoint to Connect REST API
KIP-458: Connector Client Config Override Policy</description>
            <content type="html"><![CDATA[<h2 id="kip">kip</h2>
<p>最近研究kafka-connect, 相关的kip 整理:</p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-521%3A+Enable+redirection+of+Connect%27s+log4j+messages+to+a+file+by+default">KIP-521: Enable redirection of Connect&rsquo;s log4j messages to a file by default</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-507%3A+Securing+Internal+Connect+REST+Endpoints">KIP-507: Securing Internal Connect REST Endpoints</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-495%3A+Dynamically+Adjust+Log+Levels+in+Connect">KIP-495: Dynamically Adjust Log Levels in Connect</a></p>
<pre><code>可以通过 网络请求 动态调整 loglevel. 重启后修改是丢失的
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-481%3A+SerDe+Improvements+for+Connect+Decimal+type+in+JSON">KIP-481: SerDe Improvements for Connect Decimal type in JSON</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-475%3A+New+Metrics+to+Measure+Number+of+Tasks+on+a+Connector">KIP-475: New Metrics to Measure Number of Tasks on a Connector</a></p>
<pre><code>添加了 connector-total-task-count 、connector-running-task-count、connector-paused-task-count、connector-failed-task-count、connector-unassigned-task-count、connector-destroyed-task-count 的监控指标
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-465%3A+Add+Consolidated+Connector+Endpoint+to+Connect+REST+API">KIP-465: Add Consolidated Connector Endpoint to Connect REST API</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-458%3A+Connector+Client+Config+Override+Policy">KIP-458: Connector Client Config Override Policy</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-454%3A+Expansion+of+the+ConnectClusterState+interface">KIP-454: Expansion of the ConnectClusterState interface</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-449%3A+Add+connector+contexts+to+Connect+worker+logs">KIP-449: Add connector contexts to Connect worker logs</a></p>
<pre><code>log4j.properties 添加 %X{connector.context} 信息.
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-440%3A+Extend+Connect+Converter+to+support+headers">KIP-440: Extend Connect Converter to support headers</a></p>
<pre><code>在 Converter 的两个 序列化/反序列化方法中 添加了 header 参数
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect">KIP-415: Incremental Cooperative Rebalancing in Kafka Connect</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-411%3A+Make+default+Kafka+Connect+worker+task+client+IDs+distinct">KIP-411: Make default Kafka Connect worker task client IDs distinct</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-305%3A+Add+Connect+primitive+number+converters">KIP-305: Add Connect primitive number converters</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-298%3A+Error+Handling+in+Connect">KIP-298: Error Handling in Connect</a></p>
<pre><code>错误重试次数、最大重试次数的支持
deadletterqueue 的支持
消息header中添加了部分元数据
metrics 支持
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations">KIP-297: Externalizing Secrets for Connect Configurations</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-285%3A+Connect+Rest+Extension+Plugin">KIP-285: Connect Rest Extension Plugin</a></p>
<pre><code>support custom authentication filter 
support custom authorization filter  
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-238%3A+Expose+Kafka+cluster+ID+in+Connect+REST+API">KIP-238: Expose Kafka cluster ID in Connect REST API</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-215%3A+Add+topic+regex+support+for+Connect+sinks">KIP-215: Add topic regex support for Connect sinks</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-212%3A+Enforce+set+of+legal+characters+for+connector+names">KIP-212: Enforce set of legal characters for connector names</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-208%3A+Add+SSL+support+to+Kafka+Connect+REST+interface">KIP-208: Add SSL support to Kafka Connect REST interface</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-197+Connect+REST+API+should+include+the+connector+type+when+describing+a+connector">KIP-197 Connect REST API should include the connector type when describing a connector</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-196%3A+Add+metrics+to+Kafka+Connect+framework">KIP-196: Add metrics to Kafka Connect framework</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-154+Add+Kafka+Connect+configuration+properties+for+creating+internal+topics">KIP-154 Add Kafka Connect configuration properties for creating internal topics</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-151+Expose+Connector+type+in+REST+API">KIP-151 Expose Connector type in REST API</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-146+-+Classloading+Isolation+in+Connect">KIP-146 - Classloading Isolation in Connect</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect">KIP-145 - Expose Record Headers in Kafka Connect</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-128%3A+Add+ByteArrayConverter+for+Kafka+Connect">KIP-128: Add ByteArrayConverter for Kafka Connect</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-89%3A+Allow+sink+connectors+to+decouple+flush+and+offset+commit">KIP-89: Allow sink connectors to decouple flush and offset commit</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-75+-+Add+per-connector+Converters">KIP-75 - Add per-connector Converters</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect">KIP-66: Single Message Transforms for Kafka Connect</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-65%3A+Expose+timestamps+to+Connect">KIP-65: Expose timestamps to Connect</a></p>
<pre><code>
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-52%3A+Connector+Control+APIs">KIP-52: Connector Control APIs</a></p>
<pre><code>提供 pause、resume、restart(connector/tasks)
</code></pre><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-51+-+List+Connectors+REST+API">KIP-51 - List Connectors REST API</a></p>
<pre><code>提供了 connector list 接口
</code></pre><p><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=58851767">KIP-26 - Add Kafka Connect framework for data import/export</a></p>
<pre><code>最初的设计. 提出了 kafka-connect 框架.
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Rocketmq Mirror Review</title>
            <link>https://xujianhai.fun/posts/rocketmq-mirror-review/</link>
            <pubDate>Sat, 02 Nov 2019 11:19:22 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-mirror-review/</guid>
            <description></description>
            <content type="html"><![CDATA[]]></content>
        </item>
        
        <item>
            <title>Rocketmq Connect Review</title>
            <link>https://xujianhai.fun/posts/rocketmq-connect-review/</link>
            <pubDate>Sat, 02 Nov 2019 10:27:03 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-connect-review/</guid>
            <description></description>
            <content type="html"><![CDATA[]]></content>
        </item>
        
        <item>
            <title>Rocketmq Connect</title>
            <link>https://xujianhai.fun/posts/rocketmq-connect/</link>
            <pubDate>Fri, 01 Nov 2019 19:44:59 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-connect/</guid>
            <description>最近在弄 rocketmq replicator相关的开源工作, 这里记录下 connect 的调试步骤:
  启动 connect-runtime 直接 参照 rocketmq-connect README.md, 足够用了
  启动 connect-sample
   执行 mvn clean install -Dmaven.test.skip=true 打包 将target目录下的jar复制到 runtime 配置的 pluginPath 下面 执行下面的 curl 命令启动 sample connect  curl http://localhost:8081/connectors/test0\?config\=%7b%22connector-class%22%3a%22org.apache.rocketmq.connect.file.FileSourceConnector%22%2c%22topic%22%3a%22fileTopic%22%2c%22filename%22%3a%22%2fhome%2fconnect%2frocketmq-externals%2frocketmq-connect%2frocketmq-connect-runtime%2fsource-file.txt%22%2c%22source-record-converter%22%3a%22org.apache.rocketmq.connect.runtime.converter.JsonConverter%22%7d 因为 rest http 编码的缘故, 这里进行了 urlencode 编码. 没有进行 urlencode 编码之前, 张这样
curl http://localhost:8081/connectors/test0\?config={&amp;quot;connector-class&amp;quot;:&amp;quot;org.apache.rocketmq.connect.file.FileSourceConnector&amp;quot;,&amp;quot;topic&amp;quot;:&amp;quot;fileTopic&amp;quot;,&amp;quot;filename&amp;quot;:&amp;quot;home/connect/rocketmq-externals/rocketmq-connect/rocketmq-connect-runtime/source-file.txt&amp;quot;,&amp;quot;source-record-converter&amp;quot;:&amp;quot;org.apache.rocketmq.connect.runtime.converter.JsonConverter&amp;quot;} </description>
            <content type="html"><![CDATA[<p>最近在弄 rocketmq replicator相关的开源工作, 这里记录下 connect 的调试步骤:</p>
<ol>
<li>
<p>启动 connect-runtime
直接 参照 rocketmq-connect README.md, 足够用了</p>
</li>
<li>
<p>启动 connect-sample</p>
</li>
</ol>
<ul>
<li>执行 <code>mvn clean install -Dmaven.test.skip=true</code> 打包</li>
<li>将target目录下的jar复制到 runtime 配置的 pluginPath 下面</li>
<li>执行下面的 curl 命令启动 sample connect</li>
</ul>
<pre><code>curl  http://localhost:8081/connectors/test0\?config\=%7b%22connector-class%22%3a%22org.apache.rocketmq.connect.file.FileSourceConnector%22%2c%22topic%22%3a%22fileTopic%22%2c%22filename%22%3a%22%2fhome%2fconnect%2frocketmq-externals%2frocketmq-connect%2frocketmq-connect-runtime%2fsource-file.txt%22%2c%22source-record-converter%22%3a%22org.apache.rocketmq.connect.runtime.converter.JsonConverter%22%7d 
</code></pre><p>因为 rest http 编码的缘故, 这里进行了 urlencode 编码. 没有进行 urlencode 编码之前, 张这样</p>
<pre><code>curl  http://localhost:8081/connectors/test0\?config={&quot;connector-class&quot;:&quot;org.apache.rocketmq.connect.file.FileSourceConnector&quot;,&quot;topic&quot;:&quot;fileTopic&quot;,&quot;filename&quot;:&quot;home/connect/rocketmq-externals/rocketmq-connect/rocketmq-connect-runtime/source-file.txt&quot;,&quot;source-record-converter&quot;:&quot;org.apache.rocketmq.connect.runtime.converter.JsonConverter&quot;}
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Kip Simple</title>
            <link>https://xujianhai.fun/posts/kip-simple/</link>
            <pubDate>Fri, 01 Nov 2019 13:06:56 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kip-simple/</guid>
            <description>KIP-352: Distinguish URPs caused by reassignment kip: https://cwiki.apache.org/confluence/display/KAFKA/KIP-352%3A+Distinguish+URPs+caused+by+reassignment  背景: 在 kafka中 有一个指标 kafka.UnderReplicatedPartitions, 这个是用来显示 集群中处于同步失败或者失效状态的分区数.
这个kip讲的是在 reassignment 过程中 URP (under-replicated partition) metrics不准确, 不利于报警的情况。 在 reassignment 的时候, 部分新增的 replica 因为正在同步数据而不再 ISR, 但是broker确认为这些 replica在 URP 中, 导致 hasUnderReplicatedPartitions 不准确. 当前计算方式是: info.isr.size &amp;lt; info.replicas.size, 所以, 在reassignment 情况下, 应该是 info.isr.size &amp;lt; replica - adding, 非 reassignment情况保持不变. 具体的pr: https://github.com/apache/kafka/pull/7361/files.
note: 更多监控参考 https://docs.confluent.io/current/kafka/monitoring.html</description>
            <content type="html"><![CDATA[<ul>
<li>KIP-352: Distinguish URPs caused by reassignment
kip: <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-352%3A+Distinguish+URPs+caused+by+reassignment">https://cwiki.apache.org/confluence/display/KAFKA/KIP-352%3A+Distinguish+URPs+caused+by+reassignment</a></li>
</ul>
<p>背景: 在 kafka中 有一个指标 <code>kafka.UnderReplicatedPartitions</code>, 这个是用来显示 集群中处于同步失败或者失效状态的分区数.</p>
<p>这个kip讲的是在 reassignment 过程中 URP (under-replicated partition) metrics不准确, 不利于报警的情况。 在 reassignment 的时候, 部分新增的 replica 因为正在同步数据而不再 ISR,  但是broker确认为这些 replica在 URP 中, 导致 <code>hasUnderReplicatedPartitions</code> 不准确. 当前计算方式是: <code>info.isr.size &lt; info.replicas.size</code>, 所以, 在reassignment 情况下, 应该是 <code>info.isr.size &lt; replica - adding</code>, 非 reassignment情况保持不变. 具体的pr: <a href="https://github.com/apache/kafka/pull/7361/files">https://github.com/apache/kafka/pull/7361/files</a>.</p>
<p>note: 更多监控参考 <a href="https://docs.confluent.io/current/kafka/monitoring.html">https://docs.confluent.io/current/kafka/monitoring.html</a></p>
]]></content>
        </item>
        
        <item>
            <title>Kafka Jira</title>
            <link>https://xujianhai.fun/posts/kafka-jira/</link>
            <pubDate>Fri, 01 Nov 2019 11:49:29 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-jira/</guid>
            <description>用来记录自己在 kafka 的探险历程.
 未来规划: https://cwiki.apache.org/confluence/display/KAFKA/Future+release+plan 目前最新的: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=125307901 所有的kip: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals 模块划分: https://issues.apache.org/jira/projects/KAFKA?selectedItem=com.atlassian.jira.jira-projects-plugin:components-page 所有的issue: https://issues.apache.org/jira/browse/KAFKA-9127?jql=project%20%3D%20KAFKA%20AND%20status%20%3D%20Open  </description>
            <content type="html"><![CDATA[<p>用来记录自己在 kafka 的探险历程.</p>
<ul>
<li>未来规划: <a href="https://cwiki.apache.org/confluence/display/KAFKA/Future+release+plan">https://cwiki.apache.org/confluence/display/KAFKA/Future+release+plan</a></li>
<li>目前最新的: <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=125307901">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=125307901</a></li>
<li>所有的kip: <a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals">https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals</a></li>
<li>模块划分: <a href="https://issues.apache.org/jira/projects/KAFKA?selectedItem=com.atlassian.jira.jira-projects-plugin:components-page">https://issues.apache.org/jira/projects/KAFKA?selectedItem=com.atlassian.jira.jira-projects-plugin:components-page</a></li>
<li>所有的issue: <a href="https://issues.apache.org/jira/browse/KAFKA-9127?jql=project%20%3D%20KAFKA%20AND%20status%20%3D%20Open">https://issues.apache.org/jira/browse/KAFKA-9127?jql=project%20%3D%20KAFKA%20AND%20status%20%3D%20Open</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Kafka Connect Start</title>
            <link>https://xujianhai.fun/posts/kafka-connect-start/</link>
            <pubDate>Sun, 27 Oct 2019 11:08:14 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-connect-start/</guid>
            <description>最近在弄rocketmq-connect, 顺便参考 kafka-connect 的原理和实践. 这里做一下记录.
前置工作  打包file模块  gradle connect:file:build -x test   指定 plugin 路径  修改 config/connect-standalone.properties 中 plugin.path 为你的路径 将 file 模块打包的 jar 移动到 plugin.path 中   创建topic  ./kafka-topics.sh &amp;ndash;create &amp;ndash;topic connect-test &amp;ndash;replication-factor 1 &amp;ndash;partitions 1 &amp;ndash;zookeeper localhost:2181 ./kafka-topics.sh &amp;ndash;list &amp;ndash;zookeeper localhost:2181    启动 file source  启动kafka, 参考 kafka-start 创建connect的source文件: filesource.txt, 文件内容如下:  Hello World One Step of Kafka  修改 config/connect-file-source.</description>
            <content type="html"><![CDATA[<p>最近在弄rocketmq-connect, 顺便参考 kafka-connect 的原理和实践. 这里做一下记录.</p>
<h2 id="前置工作">前置工作</h2>
<ul>
<li>打包file模块
<ul>
<li>gradle connect:file:build -x test</li>
</ul>
</li>
<li>指定 plugin 路径
<ul>
<li>修改 config/connect-standalone.properties 中 plugin.path 为你的路径</li>
<li>将 file 模块打包的 jar 移动到 plugin.path 中</li>
</ul>
</li>
<li>创建topic
<ul>
<li>./kafka-topics.sh &ndash;create &ndash;topic connect-test &ndash;replication-factor 1 &ndash;partitions 1 &ndash;zookeeper localhost:2181</li>
<li>./kafka-topics.sh &ndash;list &ndash;zookeeper localhost:2181</li>
</ul>
</li>
</ul>
<h2 id="启动-file-source">启动 file source</h2>
<ul>
<li>启动kafka, 参考 <a href="/post/kafka-start">kafka-start</a></li>
<li>创建connect的source文件: filesource.txt, 文件内容如下:</li>
</ul>
<pre><code>Hello
World
One
Step
of
Kafka
</code></pre><ul>
<li>修改 config/connect-file-source.properties 文件, 替换file变量</li>
</ul>
<pre><code>file=/Users/snow_young/deploy/data/filesource.txt  
</code></pre><ul>
<li>intellij 启动 ConnectStandalone (这次会失败, 需要配置)</li>
<li>ConnectStandalone 添加程序参数: config/connect-standalone.properties config/connect-file-source.properties</li>
<li>指定log4j配置文件
<ul>
<li>第一种: ConnectStandalone 添加VM Options: -Dlog4j.configuration=file:你的kafka路径/kafka/config/connect-log4j.properties</li>
<li>第二种: 将 config/connect-log4j.properties 复制到 runtime项目的 resources 文件夹下面，重命名为 log4j.properties</li>
</ul>
</li>
<li>设置日志路径: 在 log4j.properties 中添加 kafka.logs.dir=你自己的路径. 我这里是 /Users/snow_young/deploy/log</li>
<li>将 compile libs.slf4jlog4j 复制到 build.gradle文件的 connect-runtime 项目的 dependencies 中</li>
</ul>
<h2 id="启动-file-sink">启动 file sink</h2>
<ul>
<li>修改 config/connect-file-source.properties 修改 sink 文件: file=/Users/snow_young/deploy/data/filesink.txt, 替换成自己的文件路径就可以了</li>
<li>项目参数添加 config/connect-standalone.properties config/connect-file-source.properties</li>
<li>如果是同时启动 file-source 和 file-sink 的话, 需要处理rest端口重复的问题. 在 config/connect-standalone.properties 添加配置 rest.port=8090, 端口替换成其他的不冲突的端口就可以</li>
<li>查看 file-sink 文件, 可以发现内容已经输出了.</li>
</ul>
<pre><code>-&gt; % cat filesink.txt
Hello
World
One
Step
of
Kafka
</code></pre><h2 id="同时启动source-和-sink">同时启动source 和 sink</h2>
<ul>
<li>项目参数添加 config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Kafka Start</title>
            <link>https://xujianhai.fun/posts/kafka-start/</link>
            <pubDate>Thu, 24 Oct 2019 11:39:18 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-start/</guid>
            <description>最近开始对kafka contribute, 升级到最新的kafka, 突然发现 kafka 不能启动了, 特此记录.
 启动类配置: core目录下 kafka.Kafka 类是启动入口, 可以先进行run创建启动文件 启动参数配置: Program Arguments: config/server.properties 环境变量配置: Environment Variables: kafka.logs.dir=${自己的路径}/log, 比如我配置的是: kafka.logs.dir=/Users/snow_young/deploy/log 日志配置: 将 config/log4j.properties 复制到 core/src/main/resources 目录下 将 compile libs.slf4jlog4j 复制到 build.gradle文件的 core 项目的 dependencies 中 启动zk: zkServer start. 我是 brew 安装的 zk, 启动比较方便 重新运行, 大功告成  注意  kafka 加载到 intellij, 可能会存在索引缺失, intellij 的 import 路径标红的情况发生. 为了避免这样的情况发生, 我们需要进行索引构建 命令行使用 gradle idea 构建索引 Intellij 更新缓存. 通过选项卡 File -&amp;gt; invalidate caches and restart, 进行一次重启  </description>
            <content type="html"><![CDATA[<p>最近开始对kafka contribute, 升级到最新的kafka, 突然发现 kafka 不能启动了, 特此记录.</p>
<ol>
<li>启动类配置: core目录下 kafka.Kafka 类是启动入口, 可以先进行run创建启动文件</li>
<li>启动参数配置: Program Arguments: config/server.properties</li>
<li>环境变量配置: Environment Variables: kafka.logs.dir=${自己的路径}/log, 比如我配置的是: kafka.logs.dir=/Users/snow_young/deploy/log</li>
<li>日志配置: 将 config/log4j.properties 复制到 core/src/main/resources 目录下</li>
<li>将 compile libs.slf4jlog4j 复制到 build.gradle文件的 core 项目的 dependencies 中</li>
<li>启动zk: zkServer start. 我是 brew 安装的 zk, 启动比较方便</li>
<li>重新运行, 大功告成</li>
</ol>
<h2 id="注意">注意</h2>
<ol>
<li>kafka 加载到 intellij, 可能会存在索引缺失, intellij 的 import 路径标红的情况发生. 为了避免这样的情况发生, 我们需要进行索引构建</li>
<li>命令行使用 gradle idea 构建索引</li>
<li>Intellij 更新缓存. 通过选项卡 File -&gt; invalidate caches and restart, 进行一次重启</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Schema</title>
            <link>https://xujianhai.fun/posts/pulsar-schema/</link>
            <pubDate>Fri, 20 Sep 2019 11:51:39 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-schema/</guid>
            <description>pulsar相比于其他的mq产品, 支持了schema管理, 相比于其他MQ产品, 有很强的竞争力.
pulsar schema 主要支持 json、avro 和 protobuf, 还支持keyvalue 和 常见的基础类型. 定义了 Schema 接口统一了 decode encode行为. 通过SchemaInfo将schema信息传递服务端. (参考Commands#newSend Commands#newSubscribe Commands#newGetSchemaResponse)
admin平台: SchemasImpl: getSchemaInfo:
producer:
consumer: 启动的时候, 会发起 subcribe 交互协议. client 在 subscribe 协议中添加了 schema 参数, broker 接收到 subscribe 请求后, 会从 schemaStorage 获取 topic(如果是partition topic: my-topic-partition-1, 返回的是 my-topic) 最新版本的数据 检查schema 的兼容性,不兼容的情况下, 会返回错误. 不存在, 就会将 schema 存储到 schemaStorage 中。 检查兼容性需要满足一下任何一个条件:
 原来有schema 有producer或者consuer在使用, 有发送过消息 (PersistentTopic检查ledger, NonPersistentTopic是检查本地记录) 否则, 都是添加schema  兼容性检查原理: 有两种schema检查方式, 满足任意一个就可以:</description>
            <content type="html"><![CDATA[<p>pulsar相比于其他的mq产品, 支持了schema管理, 相比于其他MQ产品, 有很强的竞争力.</p>
<p>pulsar schema 主要支持 json、avro 和 protobuf, 还支持keyvalue 和 常见的基础类型. 定义了 Schema 接口统一了 decode encode行为.
通过SchemaInfo将schema信息传递服务端. (参考Commands#newSend Commands#newSubscribe Commands#newGetSchemaResponse)</p>
<p>admin平台:
SchemasImpl:
getSchemaInfo:</p>
<p>producer:</p>
<p>consumer:
启动的时候, 会发起 subcribe 交互协议. client 在 subscribe 协议中添加了 schema 参数, broker 接收到 subscribe 请求后, 会从 schemaStorage 获取 topic(如果是partition topic: my-topic-partition-1, 返回的是 my-topic) 最新版本的数据 检查schema 的兼容性,不兼容的情况下, 会返回错误. 不存在, 就会将 schema 存储到 schemaStorage 中。
检查兼容性需要满足一下任何一个条件:</p>
<ol>
<li>原来有schema</li>
<li>有producer或者consuer在使用,</li>
<li>有发送过消息 (PersistentTopic检查ledger, NonPersistentTopic是检查本地记录)
否则, 都是添加schema</li>
</ol>
<p>兼容性检查原理:
有两种schema检查方式, 满足任意一个就可以:</p>
<ol>
<li>通过比较 schema data 的hash值</li>
<li>针对性比较: avro的兼容性通过 avro.SchemaValidator 进行. json区分两种版本, 都是老版本, 通过比较 schema 的id相等性. 如果一方使用新版本(avro), 那么, 兼容.</li>
<li>如果没有设置, 都是兼容的</li>
</ol>
<p>如何做到版本区分:
存储的时候, 如果有历史版本, 会检查 schema 的版本兼容性,</p>
<pre><code>        SchemaStorageFormat.IndexEntry emptyIndex = SchemaStorageFormat.IndexEntry.newBuilder()
                        .setVersion(0)
                        .setHash(copyFrom(hash))
                        .setPosition(SchemaStorageFormat.PositionInfo.newBuilder()
                                .setEntryId(-1L)
                                .setLedgerId(-1L)
                        ).build();
</code></pre><pre><code>newSubscribe 
ClientCnx#handleGetSchemaResponse: 
</code></pre>
<p>客户端:</p>
<p>服务器端:</p>
<p>schenma官方文档:
<a href="https://pulsar.apache.org/docs/zh-CN/next/concepts-schema-registry/">https://pulsar.apache.org/docs/zh-CN/next/concepts-schema-registry/</a></p>
<p>schema 需要兼容, 不兼容的就不接受了
schema存储在 bookkeeper 上
client 是感知到 schema 的，producer java通过泛型支持
基本类型 avro pb 类型支持</p>
<p>优先支持
pb
avro
thrift</p>
<p>pulsar
客户端行为
broker存储
我们公司的实现</p>
<p>reference:</p>
<ol>
<li><a href="https://pulsar.apache.org/docs/zh-CN/next/concepts-schema-registry/">pulsar schema</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Python Deadlock</title>
            <link>https://xujianhai.fun/posts/python-deadlock/</link>
            <pubDate>Sun, 08 Sep 2019 11:20:15 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/python-deadlock/</guid>
            <description>一次线上针对python 死锁问题进行了追踪, 这里进行记录.
 更新镜像源: 我使用了清华大学的开源镜像站 https://mirror.tuna.tsinghua.edu.cn/help/ubuntu/ vim /etc/apt/sources.list  # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse sudo apt-get update</description>
            <content type="html"><![CDATA[<p>一次线上针对python 死锁问题进行了追踪, 这里进行记录.</p>
<ol>
<li>更新镜像源:
我使用了清华大学的开源镜像站 <a href="https://mirror.tuna.tsinghua.edu.cn/help/ubuntu/">https://mirror.tuna.tsinghua.edu.cn/help/ubuntu/</a>
vim  /etc/apt/sources.list</li>
</ol>
<pre><code># 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse
</code></pre><p>sudo apt-get update</p>
<ol start="2">
<li>
<p>下载gdb套件
sudo apt-get install gdb python2.7-dbg</p>
</li>
<li>
<p>确定进程</p>
</li>
</ol>
<pre><code>xxxxx:xxxx# ps -ef | grep python
root          58       1  0 14:23 ?        00:00:01 python launch.py
root         130      58  0 14:23 ?        00:00:06 python launch.py
root         131      58  0 14:23 ?        00:00:22 python launch.py
root         132      58  0 14:23 ?        00:00:17 python launch.py
root         133      58  0 14:23 ?        00:00:17 python launch.py
root         134      58  0 14:23 ?        00:00:17 python launch.py
root        6461     383  0 16:01 pts/0    00:00:00 grep python
</code></pre><ol start="4">
<li>看下进程的状态:</li>
</ol>
<pre><code>strace -T -tt -e trace=all -p 131
xxx(xxxx):log# strace -T -tt -e trace=all -p  130 
Process 130 attached
15:34:41.294497 futex(0x1b19c20, FUTEX_WAIT_PRIVATE, 0, NULL
</code></pre><ol start="5">
<li>gdb调试</li>
</ol>
<pre><code>sudo gdb python -p 131
info threads 
thread 1 
py-list 
info frame
py-up
</code></pre><p>相关gdb技巧不叙述. 参考地址: <a href="https://www.cnblogs.com/chengliangsheng/p/3597010.html">blog</a></p>
]]></content>
        </item>
        
        <item>
            <title>Site Blog</title>
            <link>https://xujianhai.fun/posts/site-blog/</link>
            <pubDate>Wed, 03 Jul 2019 17:45:53 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/site-blog/</guid>
            <description>如何将github个人blog迁移到自己的域名? 比如阿里云上买的自己的域名: xujianhai.fun 步骤如下:
 阿里云 CName. github 仓库设置 custom domain 在其他教程中, 往往会缺失第二步, 很蛋疼.  </description>
            <content type="html"><![CDATA[<p>如何将github个人blog迁移到自己的域名? 比如阿里云上买的自己的域名: xujianhai.fun
步骤如下:</p>
<ul>
<li>阿里云 CName.</li>
<li>github 仓库设置 custom domain
在其他教程中, 往往会缺失第二步, 很蛋疼.</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Golang Byvalue</title>
            <link>https://xujianhai.fun/posts/golang-byvalue/</link>
            <pubDate>Wed, 03 Jul 2019 13:39:19 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-byvalue/</guid>
            <description>最近同事在设计 interceptor的时候, 发现 resp 通信层修改后, 并没有上传到 业务逻辑层, 后来发现是 调入了 golang 传值的陷阱中去了. 这里写一个例子一起review下golang传值的特点.
 package main import ( &amp;quot;fmt&amp;quot; ) type Message struct{ Topic string Body []byte switcher bool } func doInt(resp interface{}) { resp = &amp;amp;Message{ Topic: &amp;quot;hahha&amp;quot;, Body: []byte(&amp;quot;heiya heiya&amp;quot;), } } func doStruct(resp *Message) { resp = &amp;amp;Message{ Topic: &amp;quot;hahha&amp;quot;, Body: []byte(&amp;quot;heiya heiya&amp;quot;), } } func doField(resp *Message) { resp.Topic = &amp;quot;haha&amp;quot; resp.Body = []byte(&amp;quot;lalal&amp;quot;) resp.switcher = true } func main() { r := new(Message) doInt(r) fmt.</description>
            <content type="html"><![CDATA[<p>最近同事在设计 interceptor的时候, 发现 resp 通信层修改后, 并没有上传到 业务逻辑层, 后来发现是 调入了 golang 传值的陷阱中去了.
这里写一个例子一起review下golang传值的特点.</p>
<pre><code>
package main

import (
    &quot;fmt&quot;
)

type Message struct{
    Topic string
    Body []byte
    switcher bool
}

func doInt(resp interface{}) {
    resp = &amp;Message{
        Topic: &quot;hahha&quot;,
        Body: []byte(&quot;heiya heiya&quot;),
    }
}

func doStruct(resp *Message) {
    resp = &amp;Message{
        Topic: &quot;hahha&quot;,
        Body: []byte(&quot;heiya heiya&quot;),
    }
}

func doField(resp *Message) {
    resp.Topic = &quot;haha&quot;
    resp.Body = []byte(&quot;lalal&quot;)
    resp.switcher = true
}

func main() {
    r := new(Message)
    doInt(r)
    fmt.Printf(&quot;after interface, msg: %v\n&quot;, r)

    r1 := new(Message)
    doStruct(r1)
    fmt.Printf(&quot;after struct, msg: %v\n&quot;, r1)

    r2 := new(Message)
    doField(r2)
    fmt.Printf(&quot;after field. msg: %v\n&quot;, r2)
}
</code></pre><p>在运行后, 会惊讶的发现:</p>
<pre><code>after interface, msg: &amp;{ [] false}
after struct, msg: &amp;{ [] false}
after field. msg: &amp;{haha [108 97 108 97 108] true}
</code></pre><p>因为在两种场景中, 传参是直接指向了另一块内存, 根据传值引用的特性, 虽然修改了 传参, 但是因为是传值的行为, 所以并不会透传到 使用者。第三种情况, 虽然也是传值类型, 但是参数和入参都是指向同一块内存的, 所以, 这里的修改, 实际上修改的是内存地址上的值, 因此使用者是可以感知的.</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Options</title>
            <link>https://xujianhai.fun/posts/golang-options/</link>
            <pubDate>Mon, 24 Jun 2019 22:49:36 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-options/</guid>
            <description>golang里面, 经常要使用到 option的配置, 有时候配置项太多, 以至于不能在函数参数列表中进行解决, 如果处理到一个函数列表参数很长的函数, 估计得抽风了.
常见的解决方案:
 Config对象, 将参数放到 Config对象中, 但是这样会很臃肿, 尤其是 是否设置和0值的区分度 会变得很模糊. 如果使用指针避免了 0值的问题，那么, 指针的对象 一般意味着 修改的传递性, 那么, 使用指针也会变的疑惑. 实例化传递的指针 在使用过程中变化了, 会产生什么影响? 调用者会很惶恐, 充满着不确定性.  参考的文章1、2中指出了使用Option 的方式进行简化, 通过变长参数的方式 提升了 可配置性、可维护性.
在grpc-go的实现中, option使用了新的方式, 提供了Option对象的配置形式. 比如 DialOptions 这里列出grpc serverOptions的使用方式:
type serverOptions struct { creds credentials.TransportCredentials codec baseCodec cp Compressor dc Decompressor unaryInt UnaryServerInterceptor streamInt StreamServerInterceptor inTapHandle tap.ServerInHandle statsHandler stats.Handler maxConcurrentStreams uint32 maxReceiveMessageSize int maxSendMessageSize int unknownStreamDesc *StreamDesc keepaliveParams keepalive.</description>
            <content type="html"><![CDATA[<p>golang里面, 经常要使用到 option的配置, 有时候配置项太多, 以至于不能在函数参数列表中进行解决, 如果处理到一个函数列表参数很长的函数, 估计得抽风了.</p>
<p>常见的解决方案:</p>
<ol>
<li>Config对象, 将参数放到 Config对象中, 但是这样会很臃肿, 尤其是 是否设置和0值的区分度 会变得很模糊. 如果使用指针避免了 0值的问题，那么, 指针的对象 一般意味着 修改的传递性, 那么, 使用指针也会变的疑惑. 实例化传递的指针 在使用过程中变化了, 会产生什么影响? 调用者会很惶恐, 充满着不确定性.</li>
</ol>
<p>参考的文章1、2中指出了使用Option 的方式进行简化, 通过变长参数的方式 提升了 可配置性、可维护性.</p>
<p>在grpc-go的实现中, option使用了新的方式, 提供了Option对象的配置形式. 比如 <a href="https://github.com/grpc/grpc-go/blob/263405c7fe47948606651e13ea1c47b30622de90/dialoptions.go">DialOptions</a>
这里列出grpc serverOptions的使用方式:</p>
<pre><code>type serverOptions struct {
    creds                 credentials.TransportCredentials
    codec                 baseCodec
    cp                    Compressor
    dc                    Decompressor
    unaryInt              UnaryServerInterceptor
    streamInt             StreamServerInterceptor
    inTapHandle           tap.ServerInHandle
    statsHandler          stats.Handler
    maxConcurrentStreams  uint32
    maxReceiveMessageSize int
    maxSendMessageSize    int
    unknownStreamDesc     *StreamDesc
    keepaliveParams       keepalive.ServerParameters
    keepalivePolicy       keepalive.EnforcementPolicy
    initialWindowSize     int32
    initialConnWindowSize int32
    writeBufferSize       int
    readBufferSize        int
    connectionTimeout     time.Duration
    maxHeaderListSize     *uint32
}

var defaultServerOptions = serverOptions{
    maxReceiveMessageSize: defaultServerMaxReceiveMessageSize,
    maxSendMessageSize:    defaultServerMaxSendMessageSize,
    connectionTimeout:     120 * time.Second,
    writeBufferSize:       defaultWriteBufSize,
    readBufferSize:        defaultReadBufSize,
}

type ServerOption interface {
    apply(*serverOptions)
}

func NewServer(opt ...ServerOption) *Server {
    opts := defaultServerOptions
    for _, o := range opt {
        o.apply(&amp;opts)
    }
    s := &amp;Server{
        lis:    make(map[net.Listener]bool),
        opts:   opts,
        conns:  make(map[io.Closer]bool),
        m:      make(map[string]*service),
        quit:   make(chan struct{}),
        done:   make(chan struct{}),
        czData: new(channelzData),
    }
    s.cv = sync.NewCond(&amp;s.mu)
    if EnableTracing {
        _, file, line, _ := runtime.Caller(1)
        s.events = trace.NewEventLog(&quot;grpc.Server&quot;, fmt.Sprintf(&quot;%s:%d&quot;, file, line))
    }

    if channelz.IsOn() {
        s.channelzID = channelz.RegisterServer(&amp;channelzServer{s}, &quot;&quot;)
    }
    return s
}
</code></pre><h2 id="参考">参考</h2>
<ol>
<li><a href="https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html">Self-referential functions and the design of options,
commandcenter</a></li>
<li><a href="https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis">Functional options for friendly API,， Dave Cheney</a></li>
<li>[](<a href="https://halls-of-valhalla.org/beta/articles/functional-options-pattern-in-go,54/">https://halls-of-valhalla.org/beta/articles/functional-options-pattern-in-go,54/</a>)</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Ants</title>
            <link>https://xujianhai.fun/posts/goroutine-pool/</link>
            <pubDate>Sun, 23 Jun 2019 21:53:44 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/goroutine-pool/</guid>
            <description>总述 在实践中, 我们经常使用 goroutine pool 来减少 go func的内存分配和gc压力, 这次主要参考了 fasthttp、ants、tidb、jager tarsgo.
细节  fasthttp: 参考前面的文章. ants: 主要参考了fasthttp的实现. 也是 worker数组的队列 + sync.Pool 的组合的方式, worker队列 也是 后进先出的处理方式. 不多叙述 jager: 实现参考: 地址, 是通过一个 有界队列实现的, 开启多个任务goroutine, 和 队列 channel 交互, 获取队列 channel的任务进行执行. targo: 实现地址, Pool内部 维护了 worker队列 和 job队列, 每个 worker 内部维护了一个 job队列. 这样, 请求会先进入 pool的job队列, pool通过worker队列分发job, 将job分发到相应的 worker的 job队列, 每个worker在初始化的时候, 会启动一个goroutine不断从 job队列取出来执行.  对比 对比各种实现: - jager的实现, 比较粗糙, 限定死了 worker数量, 不能很好的处理并发量上来的情况, 但是也符合本身的定位, 一个 有界的队列 - fasthttp、ants 定位是 workerpool, 能够很好的处理流量尖峰, 有回收空闲goroutine的能力.</description>
            <content type="html"><![CDATA[<h2 id="总述">总述</h2>
<p>在实践中, 我们经常使用 goroutine pool 来减少 go func的内存分配和gc压力, 这次主要参考了 fasthttp、ants、tidb、jager tarsgo.</p>
<h2 id="细节">细节</h2>
<ol>
<li><a href="https://github.com/valyala/fasthttp">fasthttp</a>: 参考前面的<a href="/post/fasthttp">文章</a>.</li>
<li><a href="https://github.com/panjf2000/ants">ants</a>: 主要参考了fasthttp的实现. 也是 worker数组的队列 + sync.Pool 的组合的方式, worker队列 也是 后进先出的处理方式.  不多叙述</li>
<li><a href="https://github.com/jaegertracing/jaeger">jager</a>:
实现参考: <a href="https://github.com/jaegertracing/jaeger/blob/master/pkg/queue/bounded_queue.go">地址</a>, 是通过一个 有界队列实现的, 开启多个任务goroutine, 和 队列 channel 交互, 获取队列 channel的任务进行执行.</li>
<li><a href="https://github.com/TarsCloud/TarsGo">targo</a>: 实现<a href="https://github.com/TarsCloud/TarsGo/tree/master/tars/util/gpool">地址</a>, Pool内部 维护了 worker队列 和 job队列, 每个 worker 内部维护了一个 job队列. 这样, 请求会先进入 pool的job队列, pool通过worker队列分发job, 将job分发到相应的 worker的 job队列, 每个worker在初始化的时候, 会启动一个goroutine不断从 job队列取出来执行.</li>
</ol>
<h2 id="对比">对比</h2>
<p>对比各种实现:
- jager的实现, 比较粗糙, 限定死了 worker数量, 不能很好的处理并发量上来的情况, 但是也符合本身的定位, 一个 有界的队列
- fasthttp、ants 定位是 workerpool, 能够很好的处理流量尖峰, 有回收空闲goroutine的能力.
- targo 的实现, 是一个简单的任务处理框架, 缺点是不能够scale out, 相比于 fasthttp 还是比较弱的.</p>
<h2 id="补充">补充</h2>
<p>tidb本身也有pool的实现, 但是已经在后来的开发中移除了.</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Expvar</title>
            <link>https://xujianhai.fun/posts/golang-expvar/</link>
            <pubDate>Sun, 23 Jun 2019 21:03:29 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-expvar/</guid>
            <description>在review fasthttp的刷实现中, 发现一个有趣的依赖: expvar, 是注册全局变量的, 可以通过 遍历的方式 将全局变量导出. 常规使用方式就是 通过 注册http url 来暴露注册的变量. 可以参考实现: gin框架 + http 整一个demo</description>
            <content type="html"><![CDATA[<p>在review fasthttp的刷实现中, 发现一个有趣的依赖: <a href="https://golang.org/pkg/expvar">expvar</a>, 是注册全局变量的, 可以通过 遍历的方式 将全局变量导出. 常规使用方式就是 通过 注册http url 来暴露注册的变量. 可以参考实现:
<a href="https://blog.csdn.net/jeffrey11223/article/details/78886923">gin框架 + http 整一个demo</a></p>
]]></content>
        </item>
        
        <item>
            <title>Http</title>
            <link>https://xujianhai.fun/posts/http/</link>
            <pubDate>Sun, 23 Jun 2019 20:57:26 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/http/</guid>
            <description>参考鸟窝的几个转载翻译:
 老的性能比较 iris的思考 新的web framework benchmark  通过新的benchmark可以知道, 由吞吐量、平均处理延时、内存占用 三个方面衡量, fasthttp 相关框架的性能非常高, iris 也不错, 但是 fasthttp 对其他框架的兼容比较差, 以后难以迁移. 压测中的一个亮点, 就是 http pipeling 开启的情况下, fasthttp 系列 和 iris 的性能彪的很高
我们使用了gin框架, 很无奈, 无论是 内存、cpu 还是吞吐量, 都不是最佳的.</description>
            <content type="html"><![CDATA[<p>参考鸟窝的几个转载翻译:</p>
<ol>
<li><a href="https://colobu.com/2016/03/23/Go-HTTP-request-router-and-web-framework-benchmark/">老的性能比较</a></li>
<li><a href="https://colobu.com/2016/04/01/Is-iris-the-fastest-golang-router-library/">iris的思考</a></li>
<li><a href="https://colobu.com/2016/04/06/the-fastest-golang-web-framework/">新的web framework benchmark</a></li>
</ol>
<p>通过新的<a href="https://colobu.com/2016/04/06/the-fastest-golang-web-framework/">benchmark</a>可以知道, 由吞吐量、平均处理延时、内存占用 三个方面衡量, fasthttp 相关框架的性能非常高,
iris 也不错, 但是 fasthttp 对其他框架的兼容比较差, 以后难以迁移. 压测中的一个亮点, 就是 http pipeling 开启的情况下, fasthttp 系列 和 iris 的性能彪的很高</p>
<p>我们使用了gin框架, 很无奈, 无论是 内存、cpu 还是吞吐量, 都不是最佳的.</p>
]]></content>
        </item>
        
        <item>
            <title>Pprof</title>
            <link>https://xujianhai.fun/posts/pprof/</link>
            <pubDate>Sun, 23 Jun 2019 19:22:23 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pprof/</guid>
            <description>总述 在性能分析方面, golang提供了 pprof 工具, 帮助定位 内存、cpu、goroutine 的问题. pprof提供的功能在 runtime/pprof 的包中, 也提供了 http的接口, 参考 net/http/pprof
经常使用:  总入口 http://$host:$port/debug/pprof 查看goroutine信息: http://$host:$port/debug/pprof/goroutine?debug=1 查看内存使用 go tool pprof -inuse_space http://$host:$port/debug/pprof/heap go tool pprof web http://10.110.160.41:9314/debug/pprof/heap 查看cpu使用 go tool pprof http://$host:$port/debug/pprof/profile  除此之外, 还可以用 ?seconds=60 放在url后面, 表示采样的时间间隔
其中, 2 和 3 都是在 交互式命令行中, 可以使用一下命令:
 top N: 查看排名前N个的函数 web: 进入web页面进行控制  pprof 还可以使用ui： go tool pprof -http :9090 http://$host:$port/debug/pprof/heap 指定代码路径, 可以查看相关代码: go tool pprof -call_tree -source_path ~/go/pkg/mod http://localhost:9301/debug/pprof/profile 指定采样间隔 go tool pprof &amp;ndash;seconds 25 http://localhost:9090/debug/pprof/profile cpu采样中, 常用的命令: top -cum top 20 -cum list funcname: 显示函数信息 web: 浏览器观察</description>
            <content type="html"><![CDATA[<h2 id="总述">总述</h2>
<p>在性能分析方面, golang提供了 pprof 工具, 帮助定位 内存、cpu、goroutine 的问题.
pprof提供的功能在 runtime/pprof 的包中, 也提供了 http的接口, 参考 net/http/pprof</p>
<h2 id="经常使用">经常使用:</h2>
<ol>
<li>总入口
http://$host:$port/debug/pprof</li>
<li>查看goroutine信息:
http://$host:$port/debug/pprof/goroutine?debug=1</li>
<li>查看内存使用
go tool pprof -inuse_space http://$host:$port/debug/pprof/heap
go tool pprof web http://10.110.160.41:9314/debug/pprof/heap</li>
<li>查看cpu使用
go tool pprof http://$host:$port/debug/pprof/profile</li>
</ol>
<p>除此之外, 还可以用 ?seconds=60 放在url后面, 表示采样的时间间隔</p>
<p>其中, 2 和 3 都是在 交互式命令行中, 可以使用一下命令:</p>
<ul>
<li>top N: 查看排名前N个的函数</li>
<li>web: 进入web页面进行控制</li>
</ul>
<p>pprof 还可以使用ui：
go tool pprof -http :9090 http://$host:$port/debug/pprof/heap
指定代码路径, 可以查看相关代码:
go tool pprof -call_tree -source_path ~/go/pkg/mod  http://localhost:9301/debug/pprof/profile
指定采样间隔
go tool pprof &ndash;seconds 25 http://localhost:9090/debug/pprof/profile
cpu采样中, 常用的命令:
top -cum
top 20 -cum
list funcname: 显示函数信息
web: 浏览器观察</p>
<p>pprof很好用, 但是也存在缺点: 不是很直观, 调用关系一般都比较深, 可以使用火焰图.</p>
<pre><code>go-torch -u http://$host:$port -t 30
</code></pre><p>运行命令后会生成一个 svg 图片, 然后点击打开就可以了</p>
<h2 id="常见的detection">常见的detection:</h2>
<ul>
<li>goroutine 跑飞了</li>
<li>heap跑飞了</li>
<li>cpu利用率过高</li>
</ul>
<h2 id="参考">参考</h2>
<ol>
<li><a href="https://golang.org/pkg/runtime/pprof/">runtime/pprof</a></li>
<li><a href="https://github.com/google/pprof/blob/master/doc/README.md">google官方文档</a></li>
<li><a href="https://github.com/google/pprof">google官方工具</a></li>
<li><a href="https://golang.org/doc/diagnostics.html#profiling">profiling官方文档</a></li>
<li><a href="https://github.com/gperftools/gperftools">gperftool工具</a></li>
<li><a href="https://blog.golang.org/profiling-go-programs">官方pprof示例使用blog</a></li>
<li><a href="http://www.brendangregg.com/flamegraphs.html">frame graph</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Fasthttp</title>
            <link>https://xujianhai.fun/posts/fasthttp/</link>
            <pubDate>Sun, 23 Jun 2019 18:12:26 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/fasthttp/</guid>
            <description>介绍 fasthttp 是一个极致复用的http框架, 根据官网, 相比于 net/http, 有 10x性能的提升. fasthttp 也提供了 adapter, 适配到 net/http.
实现 fasthttp 的核心理念就是复用, 在review代码的时候, 发现有大量的 sync.Pool 的使用, 比如 Server的池化: ServerPool, func worker 以及通过第三方组件 &amp;ldquo;github.com/valyala/bytebufferpool&amp;rdquo; 实现 byteBuffer 的 syncPool, 实现对 []byte 的复用.
在fasthttp中, 有一个核心的设计: workerPool. workPool 中, 有一个关键组件: workerChan, workerChan 既是资源的基本单位, 也是任务处理的基本单位. 每个到来的请求, 都会分配到workerChan中, 由workerChan的goroutine进行处理.
数据结构 workerChan的数据结构 和 分配如下:
type workerChan struct { lastUseTime time.Time ch chan net.Conn } func (wp *workerPool) Serve(c net.Conn) bool { ch := wp.</description>
            <content type="html"><![CDATA[<h2 id="介绍">介绍</h2>
<p>fasthttp 是一个极致复用的http框架, 根据<a href="https://github.com/valyala/fasthttp">官网</a>, 相比于 net/http, 有 10x性能的提升. fasthttp 也提供了 adapter, 适配到 net/http.</p>
<h2 id="实现">实现</h2>
<p>fasthttp 的核心理念就是复用, 在review代码的时候, 发现有大量的 sync.Pool 的使用, 比如 Server的池化: ServerPool, func worker 以及通过第三方组件 &ldquo;github.com/valyala/bytebufferpool&rdquo; 实现 byteBuffer 的 syncPool, 实现对 []byte 的复用.</p>
<p>在fasthttp中, 有一个核心的设计: workerPool. workPool 中, 有一个关键组件: workerChan, workerChan 既是资源的基本单位, 也是任务处理的基本单位. 每个到来的请求, 都会分配到workerChan中, 由workerChan的goroutine进行处理.</p>
<h3 id="数据结构">数据结构</h3>
<p>workerChan的数据结构 和 分配如下:</p>
<pre><code>type workerChan struct {
    lastUseTime time.Time
    ch          chan net.Conn
}

func (wp *workerPool) Serve(c net.Conn) bool {
    ch := wp.getCh()
    if ch == nil {
        return false
    }
    ch.ch &lt;- c
    return true
}

func (wp *workerPool) getCh() *workerChan {
    var ch *workerChan
    createWorker := false

    wp.lock.Lock()
    ready := wp.ready
    n := len(ready) - 1
    if n &lt; 0 {
        if wp.workersCount &lt; wp.MaxWorkersCount {
            createWorker = true
            wp.workersCount++
        }
    } else {
        ch = ready[n]
        ready[n] = nil
        wp.ready = ready[:n]
    }
    wp.lock.Unlock()

    if ch == nil {
        if !createWorker {
            return nil
        }
        vch := wp.workerChanPool.Get()
        if vch == nil {
            vch = &amp;workerChan{
                ch: make(chan net.Conn, workerChanCap),
            }
        }
        ch = vch.(*workerChan)
        go func() {
            wp.workerFunc(ch)
            wp.workerChanPool.Put(vch)
        }()
    }
    return ch
}
</code></pre><p>代码中, Serve 是 workerPool 的服务入口, getCh 是 workerChan 的分配逻辑, 实现中, 有两级缓存: ready的chan队列 和 workerChanPool 缓存池, ready队列的设计是 后进先出的, 是为了实现 cpu cache hot. ready队列的 workerChan 都是有 相应的 goroutine 运行的, 实现了 goroutine 池的概念. workerChanPool 只是 sync.Pool 的池化, 没有相应的goroutine, 分配之后, 需要 go 启动一个函数.</p>
<h3 id="clean">clean</h3>
<p>那么, workerChan 会不会一直增长呢? 比如, 流量突然增加, 然后随着流量逐渐下滑, goroutine是否也会下降呢?</p>
<p>workerPool在启动的时候, 是会定期执行 clean操作的, 将超过过期时间的空闲的goroutine进行关闭(通过发送nil实现). 如下</p>
<pre><code>func (wp *workerPool) Start() {
    if wp.stopCh != nil {
        panic(&quot;BUG: workerPool already started&quot;)
    }
    wp.stopCh = make(chan struct{})
    stopCh := wp.stopCh
    go func() {
        var scratch []*workerChan
        for {
            wp.clean(&amp;scratch)
            select {
            case &lt;-stopCh:
                return
            default:
                time.Sleep(wp.getMaxIdleWorkerDuration())
            }
        }
    }()
}

func (wp *workerPool) clean(scratch *[]*workerChan) {
    maxIdleWorkerDuration := wp.getMaxIdleWorkerDuration()

    // Clean least recently used workers if they didn't serve connections
    // for more than maxIdleWorkerDuration.
    currentTime := time.Now()

    wp.lock.Lock()
    ready := wp.ready
    n := len(ready)
    i := 0
    for i &lt; n &amp;&amp; currentTime.Sub(ready[i].lastUseTime) &gt; maxIdleWorkerDuration {
        i++
    }
    *scratch = append((*scratch)[:0], ready[:i]...)
    if i &gt; 0 {
        m := copy(ready, ready[i:])
        for i = m; i &lt; n; i++ {
            ready[i] = nil
        }
        wp.ready = ready[:m]
    }
    wp.lock.Unlock()

    // Notify obsolete workers to stop.
    // This notification must be outside the wp.lock, since ch.ch
    // may be blocking and may consume a lot of time if many workers
    // are located on non-local CPUs.
    tmp := *scratch
    for i, ch := range tmp {
        ch.ch &lt;- nil
        tmp[i] = nil
    }
}
</code></pre><p>需要注意的是, 因为ready 是 后进先出的实现, 所以,</p>
<pre><code>*scratch = append((*scratch)[:0], ready[:i]...)
</code></pre><p>就是 需要被删除的workerChan.</p>
<h3 id="执行">执行</h3>
<p>workerPool 的 workerChan 的执行函数是 workerFunc, 为了保证处理完单个请求不退出 goroutine, workerChan 使用了 chan net.Conn + for &hellip; range 的实现, workerPool 的 chan是无缓冲chan, 保证了 在启动的时候, 异步 goroutine 在启动之后, sender 才能发送数据. 每次处理完一个连接, 就会返回到 ready队列, 等待下一个 连接的处理.</p>
<p>如果被关闭了, 那么就会被归还到 workerChanPool 中, 最终在下一次gc的时候被释放掉.</p>
]]></content>
        </item>
        
        <item>
            <title>K8s Article</title>
            <link>https://xujianhai.fun/posts/k8s-article/</link>
            <pubDate>Fri, 21 Jun 2019 22:18:15 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/k8s-article/</guid>
            <description>kubectl 创建pod背后发生了什么 https://mp.weixin.qq.com/s/ctdvbasKE-vpLRxDJjwVMw 生成器的概念 需要理解下 initializer的概念 需要了解下 deployment controller/ replica controller 通过 etcd watcher 实现的吗 ? informers ? scheduler 中调度策略有哪些? CNI 有哪些知名的? 跨主机通信: overlay flannel pause 容器
发布策略、金丝雀发布、蓝绿发布、分批发布 ???</description>
            <content type="html"><![CDATA[<p>kubectl 创建pod背后发生了什么
<a href="https://mp.weixin.qq.com/s/ctdvbasKE-vpLRxDJjwVMw">https://mp.weixin.qq.com/s/ctdvbasKE-vpLRxDJjwVMw</a>
生成器的概念 需要理解下
initializer的概念 需要了解下
deployment controller/ replica controller 通过 etcd watcher 实现的吗 ?
informers ?
scheduler 中调度策略有哪些?
CNI 有哪些知名的? 跨主机通信: overlay flannel
pause 容器</p>
<p>发布策略、金丝雀发布、蓝绿发布、分批发布 ???</p>
]]></content>
        </item>
        
        <item>
            <title>Lru</title>
            <link>https://xujianhai.fun/posts/lru/</link>
            <pubDate>Wed, 19 Jun 2019 10:23:57 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/lru/</guid>
            <description>linux lru: 避免冷数据的干扰 https://www.ibm.com/developerworks/cn/linux/l-cn-pagerecycle/ mysql lru: 涉及到冷热数据 的概念 https://www.cnblogs.com/geaozhang/p/7276802.html: 3/8的点 区分冷热 http://mysql.taobao.org/monthly/2017/11/05/ mysql 还有一个 高低水位 的概念, 用于 flush list. dgraph 的lru:
golang的库: github.com/coocood/freecache hashicorp/golang-lru: 通过两个lru解决 冷热数据的问题. groupcache/lru: 代替memcached. go-cache: 带超时器的存储:</description>
            <content type="html"><![CDATA[<p>linux lru: 避免冷数据的干扰
<a href="https://www.ibm.com/developerworks/cn/linux/l-cn-pagerecycle/">https://www.ibm.com/developerworks/cn/linux/l-cn-pagerecycle/</a>
mysql lru: 涉及到冷热数据 的概念
<a href="https://www.cnblogs.com/geaozhang/p/7276802.html:">https://www.cnblogs.com/geaozhang/p/7276802.html:</a> 3/8的点 区分冷热
<a href="http://mysql.taobao.org/monthly/2017/11/05/">http://mysql.taobao.org/monthly/2017/11/05/</a>
mysql 还有一个 高低水位 的概念, 用于 flush list.
dgraph 的lru:</p>
<p>golang的库:
github.com/coocood/freecache
hashicorp/golang-lru: 通过两个lru解决 冷热数据的问题.
groupcache/lru: 代替memcached.
go-cache:   带超时器的存储:</p>
]]></content>
        </item>
        
        <item>
            <title>Gin</title>
            <link>https://xujianhai.fun/posts/gin/</link>
            <pubDate>Sun, 16 Jun 2019 10:12:55 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/gin/</guid>
            <description>gin是一个广泛使用的、golang风格的http框架, github地址: gin
Engine 在gin的实现中. Engine是整个核心, 定义如下
type Engine struct { RouterGroup FuncMap template.FuncMap allNoRoute HandlersChain allNoMethod HandlersChain noRoute HandlersChain noMethod HandlersChain pool sync.Pool trees methodTrees Engin实现了golang的http接口, 接口和启动方法的查看如下
// ServeHTTP conforms to the http.Handler interface. func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) engine.pool.Put(c) } func (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() address := resolveAddress(addr) debugPrint(&amp;#34;Listening and serving HTTP on %s\n&amp;#34;, address) err = http.</description>
            <content type="html"><![CDATA[<p>gin是一个广泛使用的、golang风格的http框架, github地址: <a href="https://github.com/gin-gonic/gin">gin</a></p>
<h3 id="engine">Engine</h3>
<p>在gin的实现中. Engine是整个核心, 定义如下</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-gin.go" data-lang="gin.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Engine</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">RouterGroup</span>

    <span style="color:#a6e22e">FuncMap</span>          <span style="color:#a6e22e">template</span>.<span style="color:#a6e22e">FuncMap</span>
    <span style="color:#a6e22e">allNoRoute</span>       <span style="color:#a6e22e">HandlersChain</span>
    <span style="color:#a6e22e">allNoMethod</span>      <span style="color:#a6e22e">HandlersChain</span>
    <span style="color:#a6e22e">noRoute</span>          <span style="color:#a6e22e">HandlersChain</span>
    <span style="color:#a6e22e">noMethod</span>         <span style="color:#a6e22e">HandlersChain</span>
    <span style="color:#a6e22e">pool</span>             <span style="color:#a6e22e">sync</span>.<span style="color:#a6e22e">Pool</span>
    <span style="color:#a6e22e">trees</span>            <span style="color:#a6e22e">methodTrees</span>
</code></pre></div><p>Engin实现了golang的http接口, 接口和启动方法的查看如下</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-gin.go" data-lang="gin.go"><span style="color:#75715e">// ServeHTTP conforms to the http.Handler interface.
</span><span style="color:#75715e"></span><span style="color:#66d9ef">func</span> (<span style="color:#a6e22e">engine</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">Engine</span>) <span style="color:#a6e22e">ServeHTTP</span>(<span style="color:#a6e22e">w</span> <span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">ResponseWriter</span>, <span style="color:#a6e22e">req</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">Request</span>) {
    <span style="color:#a6e22e">c</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">engine</span>.<span style="color:#a6e22e">pool</span>.<span style="color:#a6e22e">Get</span>().(<span style="color:#f92672">*</span><span style="color:#a6e22e">Context</span>)
    <span style="color:#a6e22e">c</span>.<span style="color:#a6e22e">writermem</span>.<span style="color:#a6e22e">reset</span>(<span style="color:#a6e22e">w</span>)
    <span style="color:#a6e22e">c</span>.<span style="color:#a6e22e">Request</span> = <span style="color:#a6e22e">req</span>
    <span style="color:#a6e22e">c</span>.<span style="color:#a6e22e">reset</span>()

    <span style="color:#a6e22e">engine</span>.<span style="color:#a6e22e">handleHTTPRequest</span>(<span style="color:#a6e22e">c</span>)

    <span style="color:#a6e22e">engine</span>.<span style="color:#a6e22e">pool</span>.<span style="color:#a6e22e">Put</span>(<span style="color:#a6e22e">c</span>)
}

<span style="color:#66d9ef">func</span> (<span style="color:#a6e22e">engine</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">Engine</span>) <span style="color:#a6e22e">Run</span>(<span style="color:#a6e22e">addr</span> <span style="color:#f92672">...</span><span style="color:#66d9ef">string</span>) (<span style="color:#a6e22e">err</span> <span style="color:#66d9ef">error</span>) {
    <span style="color:#66d9ef">defer</span> <span style="color:#66d9ef">func</span>() { <span style="color:#a6e22e">debugPrintError</span>(<span style="color:#a6e22e">err</span>) }()

    <span style="color:#a6e22e">address</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">resolveAddress</span>(<span style="color:#a6e22e">addr</span>)
    <span style="color:#a6e22e">debugPrint</span>(<span style="color:#e6db74">&#34;Listening and serving HTTP on %s\n&#34;</span>, <span style="color:#a6e22e">address</span>)
    <span style="color:#a6e22e">err</span> = <span style="color:#a6e22e">http</span>.<span style="color:#a6e22e">ListenAndServe</span>(<span style="color:#a6e22e">address</span>, <span style="color:#a6e22e">engine</span>)
    <span style="color:#66d9ef">return</span>
}
</code></pre></div><p>Engine隐式继承了RouterGroup, RouterGroup 提供了方法和middleware的注册, 每次注册方法的时候, 会进行merge middleware生成新的 HandlersChain, 并挂载到 响应方法的树上(本质上是一个前缀树).</p>
<p>gin前缀树的实现原理参考: <a href="https://segmentfault.com/a/1190000016655709">https://segmentfault.com/a/1190000016655709</a></p>
]]></content>
        </item>
        
        <item>
            <title>Golang Thrift</title>
            <link>https://xujianhai.fun/posts/golang-thrift/</link>
            <pubDate>Fri, 14 Jun 2019 23:14:25 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-thrift/</guid>
            <description>官网: https://github.com/apache/thrift
突然被问到 thrift的知识点, 问service mesh为什么使用 transport: thrift.NewTBufferedTransportFactory + NewHeaderTransportFactory, 协议上使用 NewHeaderProtocolFactory. 而非service mesh场景下, 却使用 transport: NewTBufferedTransportFactory, 协议上: NewTBinaryProtocolFactoryDefault
  协议层: thrift 提供的协议层的实现有下面几种:
binary: 二进制 compact: 压缩 json: simple json: debug 我们自定义了一个 HeaderProtocol, 配合的必须是 HeaderTransport. 通过对原来的transport封装实现的
  transport: TransportFactory: Stream: Buffered: HttpClient: MemoryBuffer: Framed: Header: 我们自定义了一个 HeaderTransport, 能够携带header信息, 比如 mesh信息. HeaderTransport只是在 原来的transport 上添加了 header信息, 原来的transport 只支持 Binary 和 Compact(内部定义).
  Frame&amp;amp;UnFramed: 第一代是没有长度编码响应体, 是 UnFramed.</description>
            <content type="html"><![CDATA[<p>官网: <a href="https://github.com/apache/thrift">https://github.com/apache/thrift</a></p>
<p>突然被问到 thrift的知识点, 问service mesh为什么使用 transport:  thrift.NewTBufferedTransportFactory + NewHeaderTransportFactory, 协议上使用 NewHeaderProtocolFactory. 而非service mesh场景下, 却使用 transport: NewTBufferedTransportFactory, 协议上: NewTBinaryProtocolFactoryDefault</p>
<ol>
<li>
<p>协议层:
thrift 提供的协议层的实现有下面几种:<br>
binary: 二进制
compact: 压缩
json:
simple json:
debug
我们自定义了一个 HeaderProtocol, 配合的必须是 HeaderTransport. 通过对原来的transport封装实现的</p>
</li>
<li>
<p>transport:
TransportFactory:
Stream:
Buffered:
HttpClient:
MemoryBuffer:
Framed:
Header:
我们自定义了一个 HeaderTransport, 能够携带header信息, 比如 mesh信息. HeaderTransport只是在 原来的transport 上添加了 header信息, 原来的transport 只支持 Binary 和 Compact(内部定义).</p>
</li>
</ol>
<p>Frame&amp;UnFramed: 第一代是没有长度编码响应体, 是 UnFramed.</p>
]]></content>
        </item>
        
        <item>
            <title>Bitmap</title>
            <link>https://xujianhai.fun/posts/bitmap/</link>
            <pubDate>Thu, 13 Jun 2019 10:08:59 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/bitmap/</guid>
            <description>https://badootech.badoo.com/bitmap-indexes-in-go-unbelievable-search-speed-bb4a6b00851
讲述了golang对bitmap的设计和优化. es的bitmap学习下</description>
            <content type="html"><![CDATA[<p><a href="https://badootech.badoo.com/bitmap-indexes-in-go-unbelievable-search-speed-bb4a6b00851">https://badootech.badoo.com/bitmap-indexes-in-go-unbelievable-search-speed-bb4a6b00851</a></p>
<p>讲述了golang对bitmap的设计和优化.
es的bitmap学习下</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Journalmq</title>
            <link>https://xujianhai.fun/posts/golang-journalmq/</link>
            <pubDate>Thu, 13 Jun 2019 09:50:30 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-journalmq/</guid>
            <description>参考: https://mp.weixin.qq.com/s/3NU_BptIp5UrDUIKQzjVxw</description>
            <content type="html"><![CDATA[<p>参考: <a href="https://mp.weixin.qq.com/s/3NU_BptIp5UrDUIKQzjVxw">https://mp.weixin.qq.com/s/3NU_BptIp5UrDUIKQzjVxw</a></p>
]]></content>
        </item>
        
        <item>
            <title>Golang Generate</title>
            <link>https://xujianhai.fun/posts/golang-generate/</link>
            <pubDate>Tue, 04 Jun 2019 09:55:54 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-generate/</guid>
            <description>go generate 本质上是一个是一个组合器: parser + ast + template 的组合, 最终生成文件.
const 初级demo: https://yushuangqi.com/blog/2017/go-command-generate.html function demo: interface demo:
写入到文件内部</description>
            <content type="html"><![CDATA[<p>go generate 本质上是一个是一个组合器: parser + ast + template 的组合, 最终生成文件.</p>
<p>const 初级demo: <a href="https://yushuangqi.com/blog/2017/go-command-generate.html">https://yushuangqi.com/blog/2017/go-command-generate.html</a>
function demo:
interface demo:</p>
<p>写入到文件内部</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Project</title>
            <link>https://xujianhai.fun/posts/golang-project/</link>
            <pubDate>Sun, 02 Jun 2019 22:59:19 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-project/</guid>
            <description>这篇是关于项目结构的思考.
1. 项目结构 常见的项目约定是: https://github.com/golang-standards/project-layout 因为是操作 rpc, 所以我更倾向于下面:
. ├── LICENSE.md ├── Makefile ├── README.md ├── build │ ├── README.md │ └── ci ├── cmd │ ├── README.md │ └── _your_app_ ├── deployments │ └── README.md ├── docs │ └── README.md ├── examples │ └── README.md ├── githooks │ └── README.md ├── internal │ ├── README.md │ ├── app │ │ └── _your_app_ │ └── pkg │ └── _your_private_lib_ ├── pkg │ ├── README.</description>
            <content type="html"><![CDATA[<p>这篇是关于项目结构的思考.</p>
<h2 id="1-项目结构">1. 项目结构</h2>
<p>常见的项目约定是: <a href="https://github.com/golang-standards/project-layout">https://github.com/golang-standards/project-layout</a>
因为是操作 rpc, 所以我更倾向于下面:</p>
<pre><code>.
├── LICENSE.md
├── Makefile
├── README.md
├── build
│   ├── README.md
│   └── ci
├── cmd
│   ├── README.md
│   └── _your_app_
├── deployments
│   └── README.md
├── docs
│   └── README.md
├── examples
│   └── README.md
├── githooks
│   └── README.md
├── internal
│   ├── README.md
│   ├── app
│   │   └── _your_app_
│   └── pkg
│       └── _your_private_lib_
├── pkg
│   ├── README.md
│   └── _your_public_lib_
├── scripts
│   └── README.md
├── test
│   └── README.md
├── tools
│   └── README.md
└── vendor
    └── README.md
</code></pre><p>虽然 go mod支持 proxy模式, 并且vendor只是个过渡兼容方案(意味着以后可能就没有vendor了), 个人还是建议vendor还是得保留下, 万一以后 proxy服务不可用, 那么, vendor打包方式就可以救命了(尤其是紧急fix的时候, proxy还不可用)</p>
<h2 id="2-注释">2. 注释</h2>
<p>官方的参考: <a href="https://blog.golang.org/godoc-documenting-go-code">https://blog.golang.org/godoc-documenting-go-code</a>
总结下来, 就是下面几个模板.</p>
<ol>
<li>project doc</li>
</ol>
<pre><code>// Package sort provides primitives for sorting slices and user-defined
// collections.
package sort
</code></pre><ol start="2">
<li>func/struct doc</li>
</ol>
<pre><code>// Fprint formats using the default formats for its operands and writes to w.
// Spaces are added between operands when neither is a string.
// It returns the number of bytes written and any write error encountered.
func Fprint(w io.Writer, a ...interface{}) (n int, err error) {
</code></pre><ol start="3">
<li>bug doc</li>
</ol>
<pre><code>// BUG(r): The rule Title uses for word boundaries does not handle Unicode punctuation properly.
</code></pre><h2 id="竞态检查">竞态检查</h2>
<p>线上代码尽量先本地build, build的同时, 进行一次竞态检查
参考: <a href="https://blog.golang.org/race-detector">https://blog.golang.org/race-detector</a>
常用命令: go build -race xxx</p>
<h2 id="代码整洁">代码整洁</h2>
<h2 id="channel--pipeline">channel &amp; pipeline</h2>
<p>pipeline: 多个channel串联实现任务处理, <a href="https://blog.golang.org/pipelines">官方blog</a></p>
<h2 id="tools">tools</h2>
<p>go generate: 生成代码, 释放手工.</p>
<ol>
<li><a href="https://blog.golang.org/pipelines">官方blog</a>, <a href="https://docs.google.com/document/d/1V03LUfjSADDooDMhe-_K59EgpTEm3V8uvQRuNMAEnjg/edit">proposal</a></li>
<li><a href="https://github.com/gojuno/generator">生成小助手</a></li>
</ol>
<h2 id="序列化方式">序列化方式</h2>
<p>官方: go gob
鸟窝的测试: <a href="https://colobu.com/2015/09/28/Golang-Serializer-Benchmark-Comparison/">https://colobu.com/2015/09/28/Golang-Serializer-Benchmark-Comparison/</a>
msp、gogoproto 是常用的两个, 并且性能很好.
在序列化到 redis中的时候, 经常使用 msgp.</p>
<h2 id="gitlab-hook">gitlab hook</h2>
<p><a href="https://blog.golang.org/go-fmt-your-code">https://blog.golang.org/go-fmt-your-code</a></p>
<h2 id="诊断">诊断</h2>
<p>pprof: <a href="https://blog.golang.org/profiling-go-programs">https://blog.golang.org/profiling-go-programs</a></p>
<h2 id="code-review">code review</h2>
<p><a href="https://github.com/golang/go/wiki/CodeReviewComments">https://github.com/golang/go/wiki/CodeReviewComments</a></p>
<h2 id="cicd">ci/cd</h2>
<ol>
<li>gitlab ci</li>
<li>golang-ci
相对而言, golang-ci 用的比较多</li>
</ol>
<h2 id="开源">开源</h2>
<p><a href="https://github.com/goreleaser/goreleaser">https://github.com/goreleaser/goreleaser</a>
将项目推送到 brew 官方仓库</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Sync</title>
            <link>https://xujianhai.fun/posts/golang-sync/</link>
            <pubDate>Sun, 02 Jun 2019 08:54:15 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-sync/</guid>
            <description>这个是对之前分析的 mutex、rwmutex、condition、semaphore的回顾和总结.
论文参考  https://swtch.com/semaphore.pdf 谈到 plan9 使用 user-space semaphore 代替 多进程协调中的flag状态的 spin lock. user-space semaphore 使用一个字段表示用户状态下的sema值, 0-&amp;gt;1 就是非竞态. 竞态情况下, 使用 内核的 QLock 排队锁 实现.
其中, 谈到 多进程的内核中, 自旋锁是可以的, 因为锁会被很快释放. 但是在用户态, 自旋锁状态下, 需要不断调度自己 + sleep, 其实什么也没有做.  golang中提供给用户的 Mutex 就是参照这个实现的, 先用 state 表示状态, 初始状态下直接获取锁, 并发争抢的时候, 进入semaRoot的逻辑, 形成sudog队列; 非抢占情况下, 只是简单的一次cas.
futex: redhat的文章 连接不上不去, 参照man
概念解析   sudog: goroutine 的 wait的表示, channel和 sudog是多对多的, 一个goroutine可能阻塞在多个对象上, 一个对象可能有多个 goroutine阻塞着, 使用 sudog解耦.
  内部mutex: golang自定义的mutex实现, 通过对指针地址操作实现.</description>
            <content type="html"><![CDATA[<p>这个是对之前分析的 mutex、rwmutex、condition、semaphore的回顾和总结.</p>
<h2 id="论文参考">论文参考</h2>
<ol>
<li><a href="https://swtch.com/semaphore.pdf">https://swtch.com/semaphore.pdf</a>
谈到 plan9 使用 user-space semaphore 代替 多进程协调中的flag状态的 spin lock.
user-space semaphore 使用一个字段表示用户状态下的sema值, 0-&gt;1 就是非竞态. 竞态情况下, 使用 内核的 QLock 排队锁 实现.<br>
其中, 谈到 多进程的内核中, 自旋锁是可以的, 因为锁会被很快释放. 但是在用户态, 自旋锁状态下, 需要不断调度自己 + sleep, 其实什么也没有做.</li>
</ol>
<p>golang中提供给用户的 Mutex 就是参照这个实现的, 先用 state 表示状态, 初始状态下直接获取锁, 并发争抢的时候, 进入semaRoot的逻辑, 形成sudog队列; 非抢占情况下, 只是简单的一次cas.</p>
<p>futex: <a href="http://people.redhat.com/drepper/futex.pdf">redhat的文章</a> 连接不上不去, 参照<a href="http://man7.org/linux/man-pages/man2/futex.2.html">man</a></p>
<h2 id="概念解析">概念解析</h2>
<ul>
<li>
<p>sudog: goroutine 的 wait的表示, channel和 sudog是多对多的, 一个goroutine可能阻塞在多个对象上, 一个对象可能有多个 goroutine阻塞着, 使用 sudog解耦.</p>
</li>
<li>
<p>内部mutex: golang自定义的mutex实现, 通过对指针地址操作实现. 最差情况下, 是系统的semaphore. 采用了优化策略, cas -&gt; 空循环 -&gt; os yield -&gt; semaphore. 进入semaphore状态的时候, 是多线程争抢, 存在线程排队的现象, 使用m的字段nextwaitm实现链表排队, 支持抢占.</p>
</li>
<li>
<p>semaRoot: 依赖内部的mutex实现的锁, 使用平衡树维护被锁的地址, 阻塞在同一个地址的sudog被维护在一个叶子上, sudog通过链表维护. 但是只维护了 251 个 semaRoot, 也就是说最多同时锁住 251 个地址.</p>
</li>
<li>
<p>Mutex实现: 直接依赖 semaRoot 实现.</p>
</li>
<li>
<p>cond实现: 依赖内部mutex提供锁的语义, 使用notifyList 维护等待的goroutine. 避免Wait + Signal + Broadcast竞争. 还依赖外部Mutex实现ticket生成的锁</p>
</li>
<li>
<p>syncmap:</p>
</li>
<li>
<p>rwmutex:</p>
</li>
<li>
<p>pool: 每个p都有一个private成员 以及一个 shared 链表, 每次获取, 先获取p本地的private成员, 没有, 就遍历p, 尝试获取shared链表, 还是没有, 就生成一个。但是，每次gc都会清除每个p的private和shared成员</p>
</li>
</ul>
<p>问题:
对比 cond.go 和 Mutex, 因为Mutex更多的是 外部优化, 这里直接对比 cond.go 和 semaRoot.<br>
cond.go 使用了 notifyList, 但是 semaRoot使用 平衡树 + goroutine链表 实现. cond.go 使用head、tail维护sudog链表, semaRoot使用sudog本身的next字段维护, 为什么有这个差异呢？
1. condition提供了 Wait + Signal + Broadcast 的语义, 实现上, 就是支持添加一个等待者、释放一个等待者、释放所有等待者, 并且Wait是放在列表尾部,  Signal是在头部.
3. condition的notifyList用自己的地址 作为 内部mutex锁的地址
4. condition的notifyList 使用 head、tail 两个sudog 维护列表. O(1)复杂度
3. Mutex 提供了 Lock + UnLock  的语义, 借助semaRoot, 默认插入链表的最后面, 也支持插入到前面, 删除的时候, 肯定是删除的头部.<br>
4. Semaroot 使用内部的 mutex 锁住 root 的lock,[ 这样, root 只有251个, Mutex很多的话, 并且lock/ublock比较频繁的话, 抢锁只会在 指定的数量上 的lock竞争. (condition做不到) &hellip; ]
4. SemaRoot 只用了一个 sudog 实现了 头部和尾部, 通过sudog的 waitlink 和 waittail.O(1) 复杂度
5. 所以, 从操作的特征上, 两者是类似的.  Mutex借助 semaroot, 插入可以定制化(前面或者后面), 都是O(1), 删除的都是头部. cond.go 中 notifyList 是 插入尾部, 删除头部, 也是O(1).
6. 因为 sudog 给 semaRoot 开了后门, 所以只需要一个就可以了.<br>
6. semaRoot 进行了锁的优化.</p>
<p>看下 commit 信息</p>
<p>关于优化点:</p>
<ul>
<li>cond的实现, 本身就需要 Mutex提供互斥语义, 但是只是提供了 生成ticket 的锁的语义. 在 Signal + Broadcast, 并没有 Mutex 加锁的操作, 是依靠 notifyList + 内部 mutex  实现的. 那么, 就一个优化点而言, cond.go 直接依赖 内部的mutex是否可以? 这样, 就避免多依赖一个 Mutex了, 实现能够更干净.</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Golang Error</title>
            <link>https://xujianhai.fun/posts/golang-error/</link>
            <pubDate>Sun, 02 Jun 2019 08:52:48 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-error/</guid>
            <description>golang的错误处理, 可以参看 官方 grpc的实践:</description>
            <content type="html"><![CDATA[<p>golang的错误处理, 可以参看 <a href="https://blog.golang.org/error-handling-and-go">官方</a>
grpc的实践:</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Epoll</title>
            <link>https://xujianhai.fun/posts/golang-epoll/</link>
            <pubDate>Wed, 29 May 2019 23:10:21 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-epoll/</guid>
            <description>套接字知识点: https://www.cnblogs.com/wmx-learn/p/5312259.html
 有 监听套接字 和 连接套接字 两个概念. listen 返回的是 监听套接字, accept 返回的是 每个client相关的 连接套接字. 大量客户端连接, 会导致 连接套接字增多 socket函数 只是创建了 socket文件描述符, bind 操作是为了给 socket 分配一个端口 socket 有 发送缓冲区、接受缓存区、等待列表  epoll https://mp.weixin.qq.com/s/MzrhaWMwrFxKT7YZvd68jw
 硬件层: 网线接收到数据写入网卡, 网卡将数据写入内存，同时发起中断, cpu收到中断后进行处理 socket阻塞与唤醒: 进行recv, 没有数据的情况下, 操作系统 会将进程 注册到 socket的等待列表, 同时将 进程从 活动列表 添加到 阻塞列表。然后, 有数据的到来, socket会唤醒等待列表的进程, 将进程放到工作队列, 这个时候进程recv的时候 就可以拿到数据了 所以, 阻塞和唤醒的本质就是: 注册到等待列表, 和 从等待列表中删除. 注意: 唤醒操作，就是讲 进程从 等待队列 放到了 工作队列 当大量客户端连接的时候, 服务端会有大量的 连接套接字, 这时候 recv的操作，会导致 进程 大量注册 socket的等待列表, 以及 唤醒的时候, 大量的从等待列表中删除、多次唤醒.</description>
            <content type="html"><![CDATA[<h2 id="套接字知识点">套接字知识点:</h2>
<p><a href="https://www.cnblogs.com/wmx-learn/p/5312259.html">https://www.cnblogs.com/wmx-learn/p/5312259.html</a></p>
<ol>
<li>有 监听套接字 和 连接套接字 两个概念. listen 返回的是 监听套接字, accept 返回的是 每个client相关的 连接套接字. 大量客户端连接, 会导致 连接套接字增多</li>
<li>socket函数 只是创建了 socket文件描述符, bind 操作是为了给 socket 分配一个端口</li>
<li>socket 有 发送缓冲区、接受缓存区、等待列表</li>
</ol>
<h2 id="epoll">epoll</h2>
<p><a href="https://mp.weixin.qq.com/s/MzrhaWMwrFxKT7YZvd68jw">https://mp.weixin.qq.com/s/MzrhaWMwrFxKT7YZvd68jw</a></p>
<ol>
<li>硬件层: 网线接收到数据写入网卡, 网卡将数据写入内存，同时发起中断, cpu收到中断后进行处理</li>
<li>socket阻塞与唤醒: 进行recv, 没有数据的情况下, 操作系统 会将进程 注册到 socket的等待列表, 同时将 进程从 活动列表 添加到 阻塞列表。然后, 有数据的到来, socket会唤醒等待列表的进程, 将进程放到工作队列, 这个时候进程recv的时候 就可以拿到数据了</li>
<li>所以, 阻塞和唤醒的本质就是: 注册到等待列表, 和 从等待列表中删除.</li>
<li>注意: 唤醒操作，就是讲 进程从 等待队列 放到了 工作队列</li>
<li>当大量客户端连接的时候, 服务端会有大量的 连接套接字, 这时候 recv的操作，会导致 进程 大量注册 socket的等待列表, 以及 唤醒的时候, 大量的从等待列表中删除、多次唤醒.</li>
<li>问题来了, 因为每次唤醒的时候, 进程是不知道是 哪个 socket唤醒的, 因此多了一次 遍历socket的过程.</li>
<li>select: 逻辑和上面类似, 传递一个[]fd作为要监视的socket, 在唤醒的时候 进行一个遍历, 判断哪个socket有数据</li>
<li>epoll: 简化了逻辑, 只需要传递一次 等待列表 (监听的socket很少改变?), 同时维护一个唤醒列表 来避免唤醒后的遍历.</li>
<li>epoll 使用 双链列表维护 就绪列表, 使用红黑树 维护 监视的socket.</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang String</title>
            <link>https://xujianhai.fun/posts/golang-string/</link>
            <pubDate>Sun, 26 May 2019 12:29:11 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-string/</guid>
            <description>总述 数据结构
type stringStruct struct { str unsafe.Pointer len int } string 本质上通过 slice 实现.
实现  concate 实现: 2/3/4/5 不懂呀 slicebytetostring: []byte 转换为 string对象 stringDataOnStack: 通过判断 string的指针是否在 stach的指针范围内 其他的太琐碎, 不提  特殊设计  使用固定大小的 tmpbuf 优化调用  问题  不是并发安全的, 需要atomic保证安全  </description>
            <content type="html"><![CDATA[<h2 id="总述">总述</h2>
<p>数据结构</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-string.go" data-lang="string.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">stringStruct</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">str</span> <span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Pointer</span>
    <span style="color:#a6e22e">len</span> <span style="color:#66d9ef">int</span>
}
</code></pre></div><p>string 本质上通过 slice 实现.</p>
<h2 id="实现">实现</h2>
<ul>
<li>concate 实现: 2/3/4/5 不懂呀</li>
<li>slicebytetostring: []byte 转换为 string对象</li>
<li>stringDataOnStack: 通过判断 string的指针是否在 stach的指针范围内</li>
<li>其他的太琐碎, 不提</li>
</ul>
<h2 id="特殊设计">特殊设计</h2>
<ol>
<li>使用固定大小的 tmpbuf 优化调用</li>
</ol>
<h2 id="问题">问题</h2>
<ol>
<li>不是并发安全的, 需要atomic保证安全</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Slice</title>
            <link>https://xujianhai.fun/posts/golang-slice/</link>
            <pubDate>Fri, 24 May 2019 17:43:40 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-slice/</guid>
            <description>总述 slice本质上 就是一个 内存地址指针, 通过 len、cap 实现内存块 的管理
实现 数据结构:
type slice struct { array unsafe.Pointer len int cap int } // An notInHeapSlice is a slice backed by go:notinheap memory. type notInHeapSlice struct { array *notInHeap len int cap int } 常用方法  扩容: growslice 扩容, len还是以前的大小, cap是新的cap. 拷贝: 简单的内存复制 append: ?没找到实现  特殊设计  使用数组管理了最大cap, 避免溢出  </description>
            <content type="html"><![CDATA[<h2 id="总述">总述</h2>
<p>slice本质上 就是一个 内存地址指针, 通过 len、cap 实现内存块 的管理</p>
<h2 id="实现">实现</h2>
<p>数据结构:</p>
<pre><code>type slice struct {
    array unsafe.Pointer
    len   int
    cap   int
}

// An notInHeapSlice is a slice backed by go:notinheap memory.
type notInHeapSlice struct {
    array *notInHeap
    len   int
    cap   int
}
</code></pre><h2 id="常用方法">常用方法</h2>
<ul>
<li>扩容: growslice
扩容, len还是以前的大小, cap是新的cap.</li>
<li>拷贝: 简单的内存复制</li>
<li>append: ?没找到实现</li>
</ul>
<h2 id="特殊设计">特殊设计</h2>
<ol>
<li>使用数组管理了最大cap, 避免溢出</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Writeb</title>
            <link>https://xujianhai.fun/posts/golang-writeb/</link>
            <pubDate>Fri, 24 May 2019 09:45:30 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-writeb/</guid>
            <description>总述 本文会分析gc中 writeBarrier、bitmap 、writeBuffer技术.
背景 写屏障这个技术是从 go1.5引入的, 是三色标记的gc需要的一个策略, 后来在go1.8中, 消除rescan的设计中进行了升级, 使用了混合屏障, 避免了rescan的STW的消耗. gc过程中, 除了写屏障技术, 在mark阶段也进行了 优化, 使用 bitmap 替换了sweep流程, 避免sweep带来的性能问题: 遍历堆、 缓存亲和力差.
分析 源码路径: mwbuf.go mbarrier.go
mbarrier mbarrier.go的注释很全了. (和proposal一致) 通过注释知道, golang使用了 Yuasa-style deletetion barrier 和 Dijkstra insertion barrier. shade(染色)和condition work 一起工作避免了 修改器对gc隐藏了对象. 方法如下
 当 从堆上unlink object 的时候, 进行染色, 将唯一的指针从堆上移动到栈上 当将指针 安装到 黑色对象, 进行染色. 将唯一指针从栈移动到 黑色对象. 一旦goroutine栈是黑色的, 就没必要 染色了  主要用来:
 内存顺序的解决 stack write global writes publication ordering singal handler pointer writes  在实现上, 是在类似memmove的操作前面</description>
            <content type="html"><![CDATA[<h2 id="总述">总述</h2>
<p>本文会分析gc中 writeBarrier、bitmap 、writeBuffer技术.</p>
<h2 id="背景">背景</h2>
<p>写屏障这个技术是从 <a href="https://docs.google.com/document/d/1wmjrocXIWTr1JxU-3EQBI6BK6KgtiFArkG47XK73xIQ/edit#">go1.5</a>引入的, 是三色标记的gc需要的一个策略, 后来在go1.8中, <a href="https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md">消除rescan</a>的设计中进行了升级, 使用了混合屏障, 避免了rescan的STW的消耗.
gc过程中, 除了写屏障技术, 在mark阶段也进行了 优化, 使用 bitmap 替换了sweep流程, 避免sweep带来的性能问题: 遍历堆、 缓存亲和力差.</p>
<h2 id="分析">分析</h2>
<p>源码路径: mwbuf.go mbarrier.go</p>
<h3 id="mbarrier">mbarrier</h3>
<p>mbarrier.go的注释很全了. (和proposal一致)
通过注释知道, golang使用了 Yuasa-style deletetion barrier 和 Dijkstra insertion barrier.
shade(染色)和condition work 一起工作避免了 修改器对gc隐藏了对象. 方法如下</p>
<ol>
<li>当 从堆上unlink object 的时候, 进行染色, 将唯一的指针从堆上移动到栈上</li>
<li>当将指针 安装到 黑色对象, 进行染色. 将唯一指针从栈移动到 黑色对象.</li>
<li>一旦goroutine栈是黑色的, 就没必要 染色了</li>
</ol>
<p>主要用来:</p>
<ol>
<li>内存顺序的解决</li>
<li>stack write</li>
<li>global writes</li>
<li>publication ordering</li>
<li>singal handler pointer writes</li>
</ol>
<p>在实现上, 是在类似memmove的操作前面</p>
<h3 id="使用场景">使用场景</h3>
<h2 id="mgcmarkgo">mgcmark.go</h2>
<h2 id="mbitmapgo">mbitmap.go</h2>
<h2 id="writebuffer">writebuffer</h2>
<p>每个p都有一个wirte buffer queue 作为 pool. 填满之后就会刷新到 gc workbuf.</p>
<p>数据结构</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-mwbbuf.go" data-lang="mwbbuf.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">wbBuf</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#75715e">// next points to the next slot in buf. It must not be a
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// pointer type because it can point past the end of buf and
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// must be updated without write barriers.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">//
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// This is a pointer rather than an index to optimize the
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// write barrier assembly.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">next</span> <span style="color:#66d9ef">uintptr</span>

    <span style="color:#75715e">// end points to just past the end of buf. It must not be a
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// pointer type because it points past the end of buf and must
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// be updated without write barriers.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">end</span> <span style="color:#66d9ef">uintptr</span>

    <span style="color:#75715e">// buf stores a series of pointers to execute write barriers
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// on. This must be a multiple of wbBufEntryPointers because
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// the write barrier only checks for overflow once per entry.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">buf</span> [<span style="color:#a6e22e">wbBufEntryPointers</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">wbBufEntries</span>]<span style="color:#66d9ef">uintptr</span>
}
</code></pre></div><p>其实, next、end 都是指向 buf的 下一个和最后一个. 其中, 每次的pointer都是放两个指针: old、new. 在 wbBufFlush的实现中, 就是进行 染成灰色、标记.</p>
<h3 id="使用场景-1">使用场景</h3>
<p>在<a href="/post/golang-other">atomic.Value的实现和分析</a>中, StorePointer会先调用 atomicwb: 在原子指针写之前 执行 写屏障操作.</p>
<h2 id="其他">其他</h2>
<h3 id="消除stw-stack-rescan的讨论">消除STW stack rescan的讨论</h3>
<p>golang1.7 使用 Dijkstra write barrier 处理 并发指针修改. 因为 读取远大于写入, 所以性能好. 但是呢, golang 在stack write 的时候, 不是使用 stack write barrier, 而是 STW期间进行re scan. 这样的话, 如果有大量的活跃的goroutine, re scan stack 会消耗大概 10-100 毫秒的时间.<br>
golang后面的版本采用 混合屏障策略, 所有新分配的对象都是 黑色的 (以前都是白色的), 这样, 就不需要进行 stack rescan, 从而节省了大量时间.
hybrid barrier 通过初始scan 的时候stack标黑, 实现在标记阶段的时候, 允许并发scan,
相比而言, Dijkstra barrier 允许并发mark, 但是在 标记的最终时候-&gt;rescan stack 需要一次STW. Yuasa 在标记mark的一开始的时候, 需要一次 STW(获取stack快照), 但是在 mark最终的时候, 并不需要rescan.
参考文献: <a href="https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md">elimate rescan</a></p>
<h2 id="参考">参考</h2>
<ol>
<li><a href="https://segmentfault.com/a/1190000018161588?utm_source=tag-newest">一个比较清晰的版本梳理</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Syncmap</title>
            <link>https://xujianhai.fun/posts/golang-syncmap/</link>
            <pubDate>Mon, 20 May 2019 10:00:02 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-syncmap/</guid>
            <description>数据结构 type Map struct { mu Mutex // read contains the portion of the map&#39;s contents that are safe for // concurrent access (with or without mu held). // // The read field itself is always safe to load, but must only be stored with // mu held. // // Entries stored in read may be updated concurrently without mu, but updating // a previously-expunged entry requires that the entry be copied to the dirty // map and unexpunged with mu held.</description>
            <content type="html"><![CDATA[<h3 id="数据结构">数据结构</h3>
<pre><code>type Map struct {
    mu Mutex

    // read contains the portion of the map's contents that are safe for
    // concurrent access (with or without mu held).
    //
    // The read field itself is always safe to load, but must only be stored with
    // mu held.
    //
    // Entries stored in read may be updated concurrently without mu, but updating
    // a previously-expunged entry requires that the entry be copied to the dirty
    // map and unexpunged with mu held.
    read atomic.Value // readOnly

    // dirty contains the portion of the map's contents that require mu to be
    // held. To ensure that the dirty map can be promoted to the read map quickly,
    // it also includes all of the non-expunged entries in the read map.
    //
    // Expunged entries are not stored in the dirty map. An expunged entry in the
    // clean map must be unexpunged and added to the dirty map before a new value
    // can be stored to it.
    //
    // If the dirty map is nil, the next write to the map will initialize it by
    // making a shallow copy of the clean map, omitting stale entries.
    dirty map[interface{}]*entry

    // misses counts the number of loads since the read map was last updated that
    // needed to lock mu to determine whether the key was present.
    //
    // Once enough misses have occurred to cover the cost of copying the dirty
    // map, the dirty map will be promoted to the read map (in the unamended
    // state) and the next store to the map will make a new dirty copy.
    misses int
}
</code></pre><p>使用 m.read 是只读视图减少加锁, m.dirty是 写视图, m.dirty 会同步到 m.read, m.dirty的操作是需要加锁的, misses的统计作为 m.dirty同步到 m.read的条件</p>
<h3 id="常规实现">常规实现</h3>
<p>1.Store
Store(k, v)的时候, 会先将数据写入到 m.dirty 中. Load的时候, 因为只有m.dirty中有数据, 所以最终是从 m.dirty中获取数据的. 当频繁查询数据的时候, 临界条件: m.misses &gt;= len(m.dirty), 就会将数据 放入 m.read 中, 实现加速查询, 因为 m.read 是只读的, 所以实现上并没有加锁. 这个时候有几个状态需要处理.</p>
<ol>
<li>
<p>又加入/修改新的key-value, m.read 并不会立即反映出这个变化, 内部怎么实现呢? 这种情况下， m.read 就会标记上 amended: true 的标记, 这样, 在读取的时候, 如果 m.read 查询不到, 并且 m.read 是  amended: true 的标记, 就会从 m.dirty中寻找,同样, 如果m.dirty 查询次数超过了临界值,  会将dirty的数据放到 m.read 中.</p>
</li>
<li>
<p>如果修改 read中已经存在的数据呢? 会修改m.read中的数据, 如果这个时候m.dirty正在从 m.read同步数据的话, 存在静态条件.
2.1 m.read 需要修改数据,
2.2 m.dirty需要复制 m.read 的数据, 如果是nil不复制.
2.3 冲突的场景: 当delete 在 store 的情况下, m.read的修改的数据会丢失, 导致数据丢失.
2.4 解决方法: 新增一个状态值: expunged = unsafe.Pointer(new(interface{})), 通过cas实现,  同时, 在 m.read修改数据的时候, 如果 m.read的数据是 nil 或者  expunged, 就在dirty中添加key/value, 保证数据对齐.</p>
</li>
<li>
<p>Load
会先尝试从 m.read 中读取, 这种不需要加锁. m.read中不存在, 通过标记判断是否存在 m.dirty 中, 尝试获取</p>
</li>
<li>
<p>Delete
同上, 不过变成了 delete 操作</p>
</li>
</ol>
<h3 id="特殊的处理">特殊的处理:</h3>
<p>为了减少内存的压力, 当进行m.read同步的时候, m.dirty会被清空. 当重新加入新的值, m.dirty 会从m.read同步回数据, 在添加新的值.
在同步的时候, 需要处理delete的情况, 如果 之前key被删除过, 在 m.read 中还是存在的, 只是内部p的指针是nil, 所以, 在同步处理的时候, 需要通过cas操作将 p的值 重置成 expunged, 避免了加锁.</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Wg</title>
            <link>https://xujianhai.fun/posts/golang-wg/</link>
            <pubDate>Sat, 18 May 2019 23:20:37 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-wg/</guid>
            <description>waitGroup在并发访问下游的时候会被频发用到.
数据结构 type WaitGroup struct { noCopy noCopy // 64-bit value: high 32 bits are counter, low 32 bits are waiter count.  // 64-bit atomic operations require 64-bit alignment, but 32-bit  // compilers do not ensure it. So we allocate 12 bytes and then use  // the aligned 8 bytes in them as state, and the other 4 as storage  // for the sema.  state1 [3]uint32 } state1 字段第一个字节表示 all counter, 第二个字节表示 waiter count , 最后一个字节是 sema 的指针, 我们认作statep (根据前面的分析可以知道, sema使用avl + 链表的方式实现的).</description>
            <content type="html"><![CDATA[<p>waitGroup在并发访问下游的时候会被频发用到.</p>
<h2 id="数据结构">数据结构</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-waitgroup.go" data-lang="waitgroup.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">WaitGroup</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">noCopy</span> <span style="color:#a6e22e">noCopy</span>

    <span style="color:#75715e">// 64-bit value: high 32 bits are counter, low 32 bits are waiter count.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// 64-bit atomic operations require 64-bit alignment, but 32-bit
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// compilers do not ensure it. So we allocate 12 bytes and then use
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// the aligned 8 bytes in them as state, and the other 4 as storage
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// for the sema.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">state1</span> [<span style="color:#ae81ff">3</span>]<span style="color:#66d9ef">uint32</span>
}
</code></pre></div><p>state1 字段第一个字节表示 all counter, 第二个字节表示 waiter count , 最后一个字节是 sema 的指针, 我们认作statep (根据前面的分析可以知道, sema使用avl + 链表的方式实现的).</p>
<p>实现上, state的最后一个指针 (称呼“statep”) 用来内部sema的地址, 挂载阻塞的goroutine. 第二个字节表示的waiter count, 当执行wait的时候, waitercount +1, 然后阻塞在statep上.  当执行Addd操作的时候,区分两种:</p>
<ol>
<li>表示等待资源数量, 这个时候 delta &gt; 0, 同时, waiter count = 0, 实现的时候只进行 all count +1</li>
<li>表示Done的语义, 这个时候 delta &lt; 0, 同时, waiter count &gt; 0, 因为 wg 的语义保障了 Wait 一定在 Done之前执行. 这个时候, all count - 1, 一直到 all count == 0的时候, 会触发唤醒.</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Rwmutex</title>
            <link>https://xujianhai.fun/posts/golang-rwmutex/</link>
            <pubDate>Sat, 18 May 2019 21:53:50 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-rwmutex/</guid>
            <description>除了mutex, rwmutex也会经常被使用.
数据结构 type RWMutex struct { w Mutex // held if there are pending writers  writerSem uint32 // semaphore for writers to wait for completing readers  readerSem uint32 // semaphore for readers to wait for completing writers  readerCount int32 // number of pending readers  readerWait int32 // number of departing readers } RWMutex中使用 Mutex 来实现writer 排队, 只有一个writer操作. 使用 writerSem 用来 readers通知 正在阻塞的 writer. readerSem 用来 reader/writer Unlock/RUnlock 的时候释放阻塞的reader.</description>
            <content type="html"><![CDATA[<p>除了mutex, rwmutex也会经常被使用.</p>
<h2 id="数据结构">数据结构</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rwmutex.go" data-lang="rwmutex.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">RWMutex</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">w</span>           <span style="color:#a6e22e">Mutex</span>  <span style="color:#75715e">// held if there are pending writers
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">writerSem</span>   <span style="color:#66d9ef">uint32</span> <span style="color:#75715e">// semaphore for writers to wait for completing readers
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">readerSem</span>   <span style="color:#66d9ef">uint32</span> <span style="color:#75715e">// semaphore for readers to wait for completing writers
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">readerCount</span> <span style="color:#66d9ef">int32</span>  <span style="color:#75715e">// number of pending readers
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">readerWait</span>  <span style="color:#66d9ef">int32</span>  <span style="color:#75715e">// number of departing readers
</span><span style="color:#75715e"></span>}
</code></pre></div><p>RWMutex中使用 Mutex 来实现writer 排队, 只有一个writer操作. 使用 writerSem 用来 readers通知 正在阻塞的 writer. readerSem 用来 reader/writer Unlock/RUnlock 的时候释放阻塞的reader. readerCount 用来区别 当前是reader还是writer占用锁. readerWait表示writer前面还有等待的reader数量(这样, reader在释放锁的时候, 就可以通过判断 readerWait是否有writer以及是否需要通知writer).</p>
<p>关键方法实现:
1.RLock: 读者计数加1, 没有写的情况下，直接成功返回. 有写入的情况下, 放入 readerSem 的goroutine队列, 挂在semaRoot的平衡树上, 在writer释放锁的时候, 会进行遍历释放 readerSem 上阻塞的goroutine. 其中, writer的判断是通过 readerWait实现的, readerWait 在writer lock的时候会减去最大的reader数量, 这样, 如果reader cas +1小于0, 表示有writer.
2.RUnlock: 注意, RUnlock一个未被加锁的rw, 是抛出异常. 如果是writer 等待了, 就进行唤醒
3.Lock: 每次只有一个writer并发操作, 只有写锁被释放了, 才有其他goroutine获取写锁, 通过mutex来实现. readerCount会减去maxReader, 让其他reader知道有writer. 如果有reader在前面, 就会修改变量 readerWait 通知reader在RUnlock有writer在等待, 并放入writerSem的等待列表.
4.Unlock: readerCount重新加上 rwmutexMaxReaders, 遍历唤醒 readerSem上阻塞的goroutine, 释放内部的mutex.</p>
<p>状态表示:
&hellip;.</p>
<p>有意思的地方, RWMutex 通过 Mutex来实现 writer 的互斥操作, 但是, Mutex本身就会挂载在 semaRoot的平衡树上, RWMutex的 writerSem 也会挂载在 semaRoot的平衡树上. 感觉多了一个 sema. Mutex 可以 writerSem 公用一个吗？</p>
<h2 id="有趣的实现">有趣的实现</h2>
<p>RLocker将 RWMutex 转换成 别名, 还重写了方法. 通过返回的是接口, 限制了方法的使用.</p>
<pre><code>// RLocker returns a Locker interface that implements
// the Lock and Unlock methods by calling rw.RLock and rw.RUnlock.
func (rw *RWMutex) RLocker() Locker {
    return (*rlocker)(rw)
}

type rlocker RWMutex

func (r *rlocker) Lock()   { (*RWMutex)(r).RLock() }
func (r *rlocker) Unlock() { (*RWMutex)(r).RUnlock() }
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Golang Condition</title>
            <link>https://xujianhai.fun/posts/golang-condition/</link>
            <pubDate>Wed, 15 May 2019 21:58:55 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-condition/</guid>
            <description>源代码参考: sync/cond.go 数据结构:
type Cond struct { noCopy noCopy // L is held while observing or changing the condition  L Locker notify notifyList checker copyChecker } 重点关注下 notifyList 的实现.
notifyList 实现原理 数据结构 需要注意的是, 这里的notifyList最终的实现其实是 sema.go 中 notifyList.
type notifyList struct { // wait is the ticket number of the next waiter. It is atomically  // incremented outside the lock.  wait uint32 // notify is the ticket number of the next waiter to be notified.</description>
            <content type="html"><![CDATA[<p>源代码参考: sync/cond.go
数据结构:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cond.go" data-lang="cond.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Cond</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">noCopy</span> <span style="color:#a6e22e">noCopy</span>

    <span style="color:#75715e">// L is held while observing or changing the condition
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">L</span> <span style="color:#a6e22e">Locker</span>

    <span style="color:#a6e22e">notify</span>  <span style="color:#a6e22e">notifyList</span>
    <span style="color:#a6e22e">checker</span> <span style="color:#a6e22e">copyChecker</span>
}
</code></pre></div><p>重点关注下 notifyList 的实现.</p>
<h2 id="notifylist-实现原理">notifyList 实现原理</h2>
<h3 id="数据结构">数据结构</h3>
<p>需要注意的是, 这里的notifyList最终的实现其实是 sema.go 中 notifyList.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sema.go" data-lang="sema.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">notifyList</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#75715e">// wait is the ticket number of the next waiter. It is atomically
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// incremented outside the lock.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">wait</span> <span style="color:#66d9ef">uint32</span>

    <span style="color:#75715e">// notify is the ticket number of the next waiter to be notified. It can
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// be read outside the lock, but is only written to with lock held.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">//
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// Both wait &amp; notify can wrap around, and such cases will be correctly
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// handled as long as their &#34;unwrapped&#34; difference is bounded by 2^31.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// For this not to be the case, we&#39;d need to have 2^31+ goroutines
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// blocked on the same condvar, which is currently not possible.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">notify</span> <span style="color:#66d9ef">uint32</span>

    <span style="color:#75715e">// List of parked waiters.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">lock</span> <span style="color:#a6e22e">mutex</span>
    <span style="color:#a6e22e">head</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">sudog</span>
    <span style="color:#a6e22e">tail</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">sudog</span>
}
</code></pre></div><p>通过注释, 我们能够发现, notifyList 有一个ticket概念, 本质上就是一个 自增id, waiter表示下一个要分配的ticket, notify 则指向下一个需要被通知的waiter, 需要注意的是, notify之前的ticket都是已经被通知过的, 所以, 可以通过比较大小避免无效的通知. 甚至, 可以比较wait和notify的大小, 判断是否需要notify(相等的情况下, 表示自从上次notify之后, 没有waiter). head/tail则是 sudog(就是等待的goroutine的表示, 通过双链实现), 那么, 等待/通知的本质上就是挂起/唤醒goroutine.</p>
<p>通过下面的流程看下 notifyList 的实现方式.</p>
<ol>
<li>notifyListAdd: 将等待着分配一个wait id.</li>
<li>notifyListWait: 获取sudog, 进行实例化, 添加到 notifyList上面. 挂起当前goroutine. (如果开启了pprof, 会进行block time的采样. releatime-to, 唤醒的时候会设置releaseTime为cputicks())</li>
<li>notifyListNotifyAll: 修改notify, 循环双链通知等待的goroutine.</li>
<li>notifyListNotifyOne: 双重检查, 寻找对应的goroutine进行唤醒(注意goroutine的挂起和分配wait顺序不是一致的, goroutine的挂起并不是按照wait排序的, 所以找到wait对应的goroutine需要一次遍历)</li>
</ol>
<h3 id="notifylist使用">notifyList使用</h3>
<p>在cond中, 常用的函数的实现如下:</p>
<ol>
<li>Wait: 调用 runtime_notifyListAdd + runtime_notifyListWait, 分配wait ticket 并挂起当前goroutine, 实现中细化锁的粒度. 在被唤醒后, 这里会归还 sudog.</li>
<li>Signal: 调用 runtime_notifyListNotifyOne 唤醒下一个goroutine</li>
<li>Broadcast: 调用runtime_notifyListNotifyAll, 唤醒所有等待的goroutine</li>
</ol>
<h3 id="有意思的地方">有意思的地方</h3>
<ol>
<li>acquireSudog避免循环调用的方式很有意思. 通过获取m实现. 细节需要在深入</li>
<li>acquireSudog: 从队列的sudoCache中获取一个sudog, 相当于一个 分配行为,
acquireSudog在当前q没有sudog的时候, 会去central cache(sched对象的cache)拿一个. 如果还是没有,  就直接分配一个新的. 在归还的时候, 也就是releaseSudog, 会判断是否满了, 满的情况下会讲一半sudog迁移到 central上 (sched对象).</li>
<li>sudog 专门为 sema做了适配, 添加了 ticket 字段, 用来 notifyListNotifyOne 判断当前goroutine是否是需要唤醒的那个.</li>
</ol>
<h2 id="内部mutex">内部mutex</h2>
<p>在实现condition语义的时候, 在wait方法的调用开始的地方, 会先尝试拿到锁. 这里的锁是内部的mutex.
sema.go中定义了mutex. 数据结构如下:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-runtime2.go" data-lang="runtime2.go"><span style="color:#75715e">// Mutual exclusion locks.  In the uncontended case,
</span><span style="color:#75715e"></span><span style="color:#75715e">// as fast as spin locks (just a few user-level instructions),
</span><span style="color:#75715e"></span><span style="color:#75715e">// but on the contention path they sleep in the kernel.
</span><span style="color:#75715e"></span><span style="color:#75715e">// A zeroed Mutex is unlocked (no need to initialize each lock).
</span><span style="color:#75715e"></span><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">mutex</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#75715e">// Futex-based impl treats it as uint32 key,
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// while sema-based impl as M* waitm.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// Used to be a union, but unions break precise GC.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">key</span> <span style="color:#66d9ef">uintptr</span>
}
</code></pre></div><p>通过注释可以知道, 在未冲突的情况下, 只是用户级别的指令: 自旋. 在冲突的情况下, 会在内核中睡眠.
执行加锁、释放的逻辑参照 lock_sema.go, mutex使用key有下面几个状态:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-lock_sema.go" data-lang="lock_sema.go"><span style="color:#66d9ef">const</span> (
    <span style="color:#a6e22e">locked</span> <span style="color:#66d9ef">uintptr</span> = <span style="color:#ae81ff">1</span>

    <span style="color:#a6e22e">active_spin</span>     = <span style="color:#ae81ff">4</span>
    <span style="color:#a6e22e">active_spin_cnt</span> = <span style="color:#ae81ff">30</span>
    <span style="color:#a6e22e">passive_spin</span>    = <span style="color:#ae81ff">1</span>
)
</code></pre></div><h3 id="常用函数如下">常用函数如下:</h3>
<ol>
<li>lock: 如果cas成功, 就直接返回; 实例化系统的semaphore, 循环判断当前锁是否被释放, 释放的情况下, 尝试cas加锁; 循环的次数, 会先 采用空循环的策略(procyild), 乐观策略; 仍然失败的情况下, 会采用释放线程的cpu控制权(osyield); 还是失败, 是多个线程抢占, 尝试M排队, 使用 nextwaitm记录上一个排队的m的值, 使用key字段传递semaphore语义, 让unlock的时候通知释放key, 同时将下一个等待的m放入key信息中.</li>
<li>unlock: lock状态下会通过cas将状态重置为 unlock. 非lock状态, 有其他线程排队, 通知其他m.
其中, semacreate是平台相关的, ; atomic.Casuintptr 方法是映射到汇编方法执行的.</li>
</ol>
<h3 id="yield">yield</h3>
<ol>
<li>procyield</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yield.c" data-lang="yield.c"><span style="color:#75715e">#</span><span style="color:#75715e">if defined (__i386__) || defined (__x86_64__)</span><span style="color:#75715e">
</span><span style="color:#75715e"></span><span style="color:#75715e">#</span><span style="color:#75715e">include</span> <span style="color:#75715e">&lt;xmmintrin.h&gt;</span><span style="color:#75715e">
</span><span style="color:#75715e"></span><span style="color:#75715e">#</span><span style="color:#75715e">endif</span><span style="color:#75715e">
</span><span style="color:#75715e"></span><span style="color:#75715e">/* Spin wait.  */</span>

<span style="color:#66d9ef">void</span>
<span style="color:#a6e22e">runtime_procyield</span> (uint32 cnt)
{
  <span style="color:#66d9ef">volatile</span> uint32 i;

  <span style="color:#66d9ef">for</span> (i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> cnt; <span style="color:#f92672">+</span><span style="color:#f92672">+</span>i)
    {
<span style="color:#75715e">#</span><span style="color:#75715e">if defined (__i386__) || defined (__x86_64__)</span><span style="color:#75715e">
</span><span style="color:#75715e"></span>      _mm_pause ();
<span style="color:#75715e">#</span><span style="color:#75715e">endif</span><span style="color:#75715e">
</span><span style="color:#75715e"></span>    }
}
</code></pre></div><p>参照<a href="https://stackoverflow.com/questions/7371869/minimum-time-a-thread-can-pause-in-linux">stackOverflow</a>上的回答, _mm_pause 本质上就是一个 pause汇编指令. pause汇编指令参考 <a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">linux manual page</a> 的 11.4.4.4, 按照文章的说法, 用来提升 Intel Xeon 处理器性能， 以及减少 Pentium 4 的能量损耗.</p>
<ol start="2">
<li>osyield</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yield.c" data-lang="yield.c"><span style="color:#75715e">/* Ask the OS to reschedule this thread.  */</span>

<span style="color:#66d9ef">void</span> <span style="color:#a6e22e">runtime_osyield</span>(<span style="color:#66d9ef">void</span>)
  __attribute__ ((no_split_stack));

<span style="color:#66d9ef">void</span>
<span style="color:#a6e22e">runtime_osyield</span> (<span style="color:#66d9ef">void</span>)
{
  sched_yield ();
}
</code></pre></div><p>sched_yield参照[linux manual page]](<a href="http://www.man7.org/linux/man-pages/man2/sched_yield.2.html)">http://www.man7.org/linux/man-pages/man2/sched_yield.2.html)</a>. 按照描述, 调用sched_yield之后, 会放弃cpu, 当前线程会被移动到静态优先级的队列的末尾. 需要注意的, 如果当前线程是唯一一个在最高优先级列表的, 那么, 在调用sched_yield()之后, 仍然会运行。</p>
<h2 id="sema">sema</h2>
<p>lock的时候使用 semasleep, unlock的时候使用 semawakeup 通知其他m. 这里使用的是操作系统的 semasleep(-1) 和 semawakeup. 这里使用 mOS.waitsema 的指针 作为 semaphore字段.</p>
<h3 id="平台创建">平台创建</h3>
<ol>
<li>semaphore
这里以 solaris平台为例子. 方法如下:</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-os_solaris.go" data-lang="os_solaris.go"><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">semacreate</span>(<span style="color:#a6e22e">mp</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">m</span>) {
    <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">mp</span>.<span style="color:#a6e22e">mos</span>.<span style="color:#a6e22e">waitsema</span> <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span> {
        <span style="color:#66d9ef">return</span>
    }

    <span style="color:#66d9ef">var</span> <span style="color:#a6e22e">sem</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">semt</span>

    <span style="color:#75715e">// Call libc&#39;s malloc rather than malloc. This will
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// allocate space on the C heap. We can&#39;t call malloc
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// here because it could cause a deadlock.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">sem</span> = (<span style="color:#f92672">*</span><span style="color:#a6e22e">semt</span>)(<span style="color:#a6e22e">libc_malloc</span>(<span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Sizeof</span>(<span style="color:#f92672">*</span><span style="color:#a6e22e">sem</span>)))
    <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">sem_init</span>(<span style="color:#a6e22e">sem</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>) <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span> {
        <span style="color:#a6e22e">throw</span>(<span style="color:#e6db74">&#34;sem_init&#34;</span>)
    }
    <span style="color:#a6e22e">mp</span>.<span style="color:#a6e22e">mos</span>.<span style="color:#a6e22e">waitsema</span> = uintptr(<span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Pointer</span>(<span style="color:#a6e22e">sem</span>))
}
</code></pre></div><p>这里，先是堆上创建创建了对象, 然后实例化semaphore对象. sem_init参照<a href="http://www.man7.org/linux/man-pages/man3/sem_init.3.html">linux manual</a></p>
<h2 id="参考">参考:</h2>
<ol>
<li><a href="http://www.man7.org/linux/man-pages/man3/sem_init.3.html">sema init</a></li>
<li><a href="http://man7.org/linux/man-pages/man3/sem_wait.3.html">sema wait</a></li>
<li><a href="http://www.man7.org/linux/man-pages/man3/sem_post.3.html">sem_post</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Mutex</title>
            <link>https://xujianhai.fun/posts/golang-mutex/</link>
            <pubDate>Tue, 14 May 2019 15:39:34 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-mutex/</guid>
            <description>Mutex实现原理 Mutex的设计参照了 plan9, linux也有相关实现: futex
先看mutex.go的注释:
 // Mutex fairness. // // Mutex can be in 2 modes of operations: normal and starvation. // In normal mode waiters are queued in FIFO order, but a woken up waiter // does not own the mutex and competes with new arriving goroutines over // the ownership. New arriving goroutines have an advantage -- they are // already running on CPU and there can be lots of them, so a woken up // waiter has good chances of losing.</description>
            <content type="html"><![CDATA[<h2 id="mutex实现原理">Mutex实现原理</h2>
<p>Mutex的设计参照了 <a href="https://swtch.com/semaphore.pdf">plan9</a>, linux也有相关实现: <a href="http://man7.org/linux/man-pages/man2/futex.2.html">futex</a></p>
<p>先看mutex.go的注释:</p>
<pre><code>    // Mutex fairness.
    //
    // Mutex can be in 2 modes of operations: normal and starvation.
    // In normal mode waiters are queued in FIFO order, but a woken up waiter
    // does not own the mutex and competes with new arriving goroutines over
    // the ownership. New arriving goroutines have an advantage -- they are
    // already running on CPU and there can be lots of them, so a woken up
    // waiter has good chances of losing. In such case it is queued at front
    // of the wait queue. If a waiter fails to acquire the mutex for more than 1ms,
    // it switches mutex to the starvation mode.
    //
    // In starvation mode ownership of the mutex is directly handed off from
    // the unlocking goroutine to the waiter at the front of the queue.
    // New arriving goroutines don't try to acquire the mutex even if it appears
    // to be unlocked, and don't try to spin. Instead they queue themselves at
    // the tail of the wait queue.
    //
    // If a waiter receives ownership of the mutex and sees that either
    // (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms,
    // it switches mutex back to normal operation mode.
    //
    // Normal mode has considerably better performance as a goroutine can acquire
    // a mutex several times in a row even if there are blocked waiters.
    // Starvation mode is important to prevent pathological cases of tail latency.
</code></pre><p>其实, 就是在处理锁的公平性的问题. 普通模式下, 其实就是不公平的, 因为后来的goroutine可能会持有锁, 饥饿模式下, 就是公平的, 因为后来的活跃的goroutine一定是在先放到队列的尾部, 并不会进行自旋或者拿锁.</p>
<p>看下Mutex的结构:</p>
<pre><code>type Mutex struct {
    state int32
    sema  uint32
}
</code></pre><p>其中, state表示当前的状态, 比如 mutexLocked、mutexWoken、mutexStarving、mutexWaiterShiftstarvationThresholdNs, 通过atomic.CompareAndSwapInt32 实现状态的变更, 判断是否抢锁成功. sema 是 抢锁过程中操作的对象, 通过对sema的cas操作表示是否抢占成功.
常用的几个函数的实现:</p>
<ol>
<li>Lock: 原子更新状态lock, 失败直接返回. 第一次不进行排队, 尝试抢锁(参看下面sema实现原理). 针对唤醒、饥饿模式、自旋 做了实现. (自旋的goroutine是直接排到队列前面的)</li>
<li>Unlock: 去除锁状态, 释放锁(参看下面sema实现原理). 注意在starving模式下, 释放锁会唤醒下一个goroutine, 非starving模式下, 不进行唤醒.(唤醒下一个的操作在 semacquire1 中实现, 通过sudog的ticket实现. Starving -&gt; handoff:true -&gt; ticket==1 -&gt; semacquire1 不进行循环了. NoStarving -&gt; handoff:false -&gt; ticket==0 -&gt; semacquire1 进行循环: 从队列中拿下一个, 尝试抢锁)</li>
</ol>
<p>Mutex是基于sema实现的, 先关注下 Mutex.</p>
<h2 id="sema-实现原理">sema 实现原理:</h2>
<h3 id="主要的数据结构">主要的数据结构</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sema.go" data-lang="sema.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">semaRoot</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">lock</span>  <span style="color:#a6e22e">mutex</span>
    <span style="color:#a6e22e">treap</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">sudog</span> <span style="color:#75715e">// root of balanced tree of unique waiters.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">nwait</span> <span style="color:#66d9ef">uint32</span> <span style="color:#75715e">// Number of waiters. Read w/o the lock.
</span><span style="color:#75715e"></span>}

<span style="color:#66d9ef">var</span> <span style="color:#a6e22e">semtable</span> [<span style="color:#a6e22e">semTabSize</span>]<span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">root</span> <span style="color:#a6e22e">semaRoot</span>
    <span style="color:#a6e22e">pad</span>  [<span style="color:#a6e22e">cpu</span>.<span style="color:#a6e22e">CacheLinePadSize</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Sizeof</span>(<span style="color:#a6e22e">semaRoot</span>{})]<span style="color:#66d9ef">byte</span>
}

</code></pre></div><p>通过注释, 在semaRoot的实现中, semaRoot用平衡树的方式组织sudog, sudog在这里表示阻塞在同一个地址上的goroutine, 寻找不同地址的sudog就是log(n)的.系统中使用semtable维护了251个semaRoot。 ??? 怎么维护的 ??? 为啥是这个数字? , nwait维护了等待的goroutine数量.</p>
<h3 id="实现的相关函数">实现的相关函数:</h3>
<ol>
<li>cansemacquire: 通过对uint32 元素进行cas实现抢锁的功能. 在Mutex的使用场景中, sema初始化是0, 第一次抢Mutex, goroutine是直接成功, 在释放Mutex的时候, sema会递增加一. 保证了 并发抢锁情况下 cansemacquire 是可以运行的.</li>
<li>semaRoot#queue: 将元素放入semaRoot的平衡树中.</li>
<li>semaRoot#dequeue: 获取semaRoot指定地址的第一个元素, 涉及到转换到叶子节点</li>
<li>semacquire1: 参照内部注释就可以了:</li>
</ol>
<pre><code>    // Harder case:
    //  increment waiter count
    //  try cansemacquire one more time, return if succeeded
    //  enqueue itself as a waiter
    //  sleep
    //  (waiter descriptor is dequeued by signaler)
</code></pre><p>放入等待列表的操作, 就是 将sudog放到semaRoot的平衡树+挂起当前goroutine
3. semrelease1: 参考注释:</p>
<pre><code>// Harder case: search for a waiter and wake it.
</code></pre><p>补充sudog为了支持Mutex的做的实现:</p>
<ol>
<li>sudog 添加了 waittail 和 waitlink、parent, waittail 指向sema链表里的最后一个元素, 方便实现添加到最后的语义; waitlink指向sema链表中下一个, 方便实现dequeue语义. parent 指向sema二叉树的父节点.</li>
<li>关于runtime_canSpin, 其实链接到 sync_runtime_canSpin, 判断条件参看 注释</li>
</ol>
<pre><code>func sync_runtime_canSpin(i int) bool {
    // sync.Mutex is cooperative, so we are conservative with spinning.
    // Spin only few times and only if running on a multicore machine and
    // GOMAXPROCS&gt;1 and there is at least one other running P and local runq is empty.
    // As opposed to runtime mutex we don't do passive spinning here,
    // because there can be work on global runq or on other Ps.
</code></pre><ol start="3">
<li>关于runtime_doSpin, 其实是链接到sync_runtime_doSpin.</li>
</ol>
<h3 id="问题">问题:</h3>
<ol>
<li>定位semroot 的算法, 不是很理解</li>
</ol>
<pre><code>func semroot(addr *uint32) *semaRoot {
    return &amp;semtable[(uintptr(unsafe.Pointer(addr))&gt;&gt;3)%semTabSize].root
}
</code></pre><ol start="2">
<li>Mutex 的sema每次goroutine阻塞、唤醒都要加减1, sema.go中有使用了nwait 表示等待着数量, 是否可以统一呢?</li>
</ol>
<h3 id="汇编指令">汇编指令</h3>
<p>1.cas
汇编指令cas等参考asm_amd64.s,
这里以atomic·Casuintptr为例, 对应的asm代码:</p>
<pre><code>// bool Cas(int32 *val, int32 old, int32 new)
// Atomically:
//  if(*val == old){
//      *val = new;
//      return 1;
//  } else
//      return 0;
TEXT runtime∕internal∕atomic·Cas(SB),NOSPLIT,$0-17
    MOVQ    ptr+0(FP), BX
    MOVL    old+8(FP), AX
    MOVL    new+12(FP), CX
    LOCK
    CMPXCHGL    CX, 0(BX)
    SETEQ   ret+16(FP)
    RET

TEXT runtime∕internal∕atomic·Casuintptr(SB), NOSPLIT, $0-13
    JMP runtime∕internal∕atomic·Cas(SB)

// bool runtime∕internal∕atomic·Cas64(uint64 *val, uint64 old, uint64 new)
// Atomically:
//  if(*val == *old){
//      *val = new;
//      return 1;
//  } else {
//      return 0;
//  }
TEXT runtime∕internal∕atomic·Cas64(SB), NOSPLIT, $0-25
    MOVQ    ptr+0(FP), BX
    MOVQ    old+8(FP), AX
    MOVQ    new+16(FP), CX
    LOCK
    CMPXCHGQ    CX, 0(BX)
    SETEQ   ret+24(FP)
    RET
</code></pre><p>显然还是使用了 LOCK + CMPXCHGQ 两条汇编指令, 所以, 加锁其实是 线程级别的. 这里的细说两个指令.
LOCK: 保持缓存行处于 M/E 状态(参看补充知识点).
CMPXCHGQ/CMPXCHGL: CMPXCHGL 是32bit操作, CMPXCHG的64bit操作. 但单处理器中, 是不需要切换到Level 0层的. 多核处理器中, 一定是搭配LOCK前缀实现cas操作原子的语义</p>
<h2 id="补充知识点">补充知识点:</h2>
<h3 id="mesi缓存协议">MESI缓存协议:</h3>
<p>缓存行64byte, 每个缓存行有四种状态:</p>
<ol>
<li>M[Modified]: 缓存行独占, 尚未写会内存,</li>
<li>E[Exclusive]: 缓存行独占, 与内存一致</li>
<li>S[Shared]: 缓存行多核共享, 与内存一致</li>
<li>I[Invalid]: 使缓存无效</li>
</ol>
<ul>
<li>通信
多核之间通过L3总线进行通信.</li>
<li>读取流程
当一个cpu核的线程准备读取某个缓存行的内容时, 如果状态处于MES,就直接读取; 如果处于I, 就需要和其他cpu核进行通信, 广播读消息,在收到读响应后, 更新缓存行, 并将状态更新成S.</li>
<li>写入流程
当一个cpu核的线程准备写入某个缓存行的内容时, 如果状态处于M, 直接写; 如果状态处于E, 直接写入, 更新成M; 如果状态处于S/I, 向其他cpu核广播使无效消息, 转换状态E, 直接写入, 进入M.</li>
</ul>
<h2 id="汇编指令lock的优化">汇编指令lock的优化</h2>
<p>具体参考:  “Locked Atomic Operations” in Chapter 8 in <a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">Intel Developer mannual</a></p>
<ol>
<li>总线锁. 系统总线锁.</li>
<li>缓存锁: 缓存一致性协议实现 缓存的数据结构的原子操作. 对于P6和更新的处理器系列, 如果LOCK操作的内存位置被缓存在处理器中, 并且完全在一个缓存行上, 那么, 在总线上并不会生成LOCK指令, 相反, 内存位置的修改的修改内部执行的, 并使用缓存一致性机制保证操作是原子执行的</li>
</ol>
<p>参考:</p>
<ol>
<li><a href="https://en.wikipedia.org/wiki/MESI_protocol">MESI</a></li>
<li><a href="https://www.felixcloutier.com/x86/lock">LOCK</a></li>
<li><a href="https://www.felixcloutier.com/x86/cmpxchg">CMPXCHG</a></li>
<li><a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">Intel Developer mannual</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Blog</title>
            <link>https://xujianhai.fun/posts/blog/</link>
            <pubDate>Thu, 09 May 2019 19:07:10 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/blog/</guid>
            <description>Russ Cox: 以golang的思考为主 https://research.swtch.com/ Dave cheney: https://dave.cheney.net/about go doc: https://talks.golang.org/ horror: https://blog.codinghorror.com Rob Pike: https://commandcenter.blogspot.com/ 各个公司的blog
jeff dean 的ppt: http://static.googleusercontent.com/media/research.google.com/zh-CN//people/jeff/Berkeley-Latency-Mar2012.pdf</description>
            <content type="html"><![CDATA[<p>Russ Cox: 以golang的思考为主
<a href="https://research.swtch.com/">https://research.swtch.com/</a>
Dave cheney:
<a href="https://dave.cheney.net/about">https://dave.cheney.net/about</a>
go doc:
<a href="https://talks.golang.org/">https://talks.golang.org/</a>
horror:
<a href="https://blog.codinghorror.com">https://blog.codinghorror.com</a>
Rob Pike:
<a href="https://commandcenter.blogspot.com/">https://commandcenter.blogspot.com/</a>
各个公司的blog</p>
<p>jeff dean 的ppt:
<a href="http://static.googleusercontent.com/media/research.google.com/zh-CN//people/jeff/Berkeley-Latency-Mar2012.pdf">http://static.googleusercontent.com/media/research.google.com/zh-CN//people/jeff/Berkeley-Latency-Mar2012.pdf</a></p>
]]></content>
        </item>
        
        <item>
            <title>Golang For</title>
            <link>https://xujianhai.fun/posts/golang-for/</link>
            <pubDate>Wed, 01 May 2019 10:11:45 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-for/</guid>
            <description>小伙伴写for循环, 提出了一个问题: 为啥下面的不可以这么写 (虽然是新手问题, 但是还是得从源代码解释):
arr := []int32{1, 2, 3, 4, 5} for _, a := range arr { go func(){ fmt.Println(&amp;quot;a: %v&amp;quot;, a) }() } 我们编译查看下
go build -gcflags &#39;-l&#39; -o main main.go go tool objdump -s &amp;quot;main\.test0&amp;quot; main 我们可以看到
TEXT main.test0(SB) /Users/snow_young/go/src/code.byted.org/im_cloud/teq_stack/main.go main.go:14 0x1091cf0 65488b0c2530000000 MOVQ GS:0x30, CX main.go:14 0x1091cf9 483b6110 CMPQ 0x10(CX), SP main.go:14 0x1091cfd 0f868a000000 JBE 0x1091d8d main.go:14 0x1091d03 4883ec48 SUBQ $0x48, SP main.go:14 0x1091d07 48896c2440 MOVQ BP, 0x40(SP) main.</description>
            <content type="html"><![CDATA[<p>小伙伴写for循环, 提出了一个问题: 为啥下面的不可以这么写 (虽然是新手问题, 但是还是得从源代码解释):</p>
<pre><code>arr := []int32{1, 2, 3, 4, 5}
for _, a := range arr {
   go func(){
      fmt.Println(&quot;a: %v&quot;, a)
   }()
}
</code></pre><p>我们编译查看下</p>
<pre><code>go build -gcflags '-l' -o main main.go
go tool objdump -s &quot;main\.test0&quot;  main
</code></pre><p>我们可以看到</p>
<pre><code>TEXT main.test0(SB) /Users/snow_young/go/src/code.byted.org/im_cloud/teq_stack/main.go
  main.go:14                0x1091cf0                65488b0c2530000000        MOVQ GS:0x30, CX
  main.go:14                0x1091cf9                483b6110                CMPQ 0x10(CX), SP
  main.go:14                0x1091cfd                0f868a000000                JBE 0x1091d8d
  main.go:14                0x1091d03                4883ec48                SUBQ $0x48, SP
  main.go:14                0x1091d07                48896c2440                MOVQ BP, 0x40(SP)
  main.go:14                0x1091d0c                488d6c2440                LEAQ 0x40(SP), BP
  main.go:15                0x1091d11                488b05785e0400                MOVQ main.statictmp_1(SB), AX
  main.go:15                0x1091d18                4889442424                MOVQ AX, 0x24(SP)
  main.go:15                0x1091d1d                0f1005705e0400                MOVUPS main.statictmp_1+4(SB), X0
  main.go:15                0x1091d24                0f11442428                MOVUPS X0, 0x28(SP)
  main.go:16                0x1091d29                488d0550fc0000                LEAQ type.*+63872(SB), AX
  main.go:16                0x1091d30                48890424                MOVQ AX, 0(SP)
  main.go:16                0x1091d34                e8978ff7ff                CALL runtime.newobject(SB)
  main.go:16                0x1091d39                488b442408                MOVQ 0x8(SP), AX
  main.go:16                0x1091d3e                4889442438                MOVQ AX, 0x38(SP)
  main.go:16                0x1091d43                31c9                        XORL CX, CX
  main.go:16                0x1091d45                eb36                        JMP 0x1091d7d
  main.go:16                0x1091d47                48894c2418                MOVQ CX, 0x18(SP)
  main.go:16                0x1091d4c                8b548c24                MOVL 0x24(SP)(CX*4), DX
  main.go:16                0x1091d50                8910                        MOVL DX, 0(AX)
  main.go:19                0x1091d52                4889442410                MOVQ AX, 0x10(SP)
  main.go:17                0x1091d57                c7042408000000                MOVL $0x8, 0(SP)
  main.go:17                0x1091d5e                488d159b7e0300                LEAQ go.func.*+120(SB), DX
  main.go:17                0x1091d65                4889542408                MOVQ DX, 0x8(SP)
  main.go:17                0x1091d6a                e8b1e4f9ff                CALL runtime.newproc(SB)
  main.go:16                0x1091d6f                488b442418                MOVQ 0x18(SP), AX
  main.go:16                0x1091d74                488d4801                LEAQ 0x1(AX), CX
  main.go:16                0x1091d78                488b442438                MOVQ 0x38(SP), AX
  main.go:16                0x1091d7d                4883f905                CMPQ $0x5, CX
  main.go:16                0x1091d81                7cc4                        JL 0x1091d47
  main.go:16                0x1091d83                488b6c2440                MOVQ 0x40(SP), BP
  main.go:16                0x1091d88                4883c448                ADDQ $0x48, SP
  main.go:16                0x1091d8c                c3                        RET
  main.go:14                0x1091d8d                e87ed7fbff                CALL runtime.morestack_noctxt(SB)
  main.go:14                0x1091d92                e959ffffff                JMP main.test0(SB)
</code></pre><p>可以发下，只实例化了一个对象</p>
<pre><code>main.go:16                0x1091d34                e8978ff7ff                CALL runtime.newobject(SB)
</code></pre><p>并且执行的时候, 也没有复制对象, 直接就把当前地址传递了过去</p>
<pre><code>  main.go:17                0x1091d6a                e8b1e4f9ff                CALL runtime.newproc(SB)
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Golang Mod</title>
            <link>https://xujianhai.fun/posts/golang-mod/</link>
            <pubDate>Wed, 01 May 2019 10:11:38 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-mod/</guid>
            <description>缘起 最近, 打算升级govendor到go mod, 为什么呢?
 版本化的语义, govendor中是 revision 和 revisionTime 本地多版本共存, 不再像之前kite升级后, 导致 生成 kite客户端 很费力(因为生成模板是老版本, 新老不兼容) 清晰的依赖管理, go module 是将依赖作为一个group的, 使用 govendor 无法很好的管理依赖, 升级往往是 升级完kite, 再升级kite依赖, 导致每次升级都是一次耗费时间的辛苦活 官方的toolchain replace语义, 方便本地替换 exclude语义, 方便拦截一些bug的版本  参考 首先, 阅读来自swtch的依赖管理的三篇文章 0. 入口索引: https://research.swtch.com/vgo
 第一篇: https://research.swtch.com/vgo-intro  讲述了依赖管理的演进 09 nothing -&amp;gt; 2010 go install -&amp;gt; 2011 go get, 存在没有版本的问题: 1. api 稳定性变更 2. 可重复构建. 未来的规划: dep/glide 不将支持, 发布了vgo(兼容目前的依赖管理), go mod 是未来 go mod 的演示   第二篇: https://research.</description>
            <content type="html"><![CDATA[<h2 id="缘起">缘起</h2>
<p>最近, 打算升级govendor到go mod, 为什么呢?</p>
<ol>
<li>版本化的语义, govendor中是 revision 和 revisionTime</li>
<li>本地多版本共存, 不再像之前kite升级后, 导致 生成 kite客户端 很费力(因为生成模板是老版本, 新老不兼容)</li>
<li>清晰的依赖管理, go module 是将依赖作为一个group的, 使用 govendor 无法很好的管理依赖, 升级往往是 升级完kite, 再升级kite依赖, 导致每次升级都是一次耗费时间的辛苦活</li>
<li>官方的toolchain</li>
<li>replace语义, 方便本地替换</li>
<li>exclude语义, 方便拦截一些bug的版本</li>
</ol>
<h2 id="参考">参考</h2>
<p>首先, 阅读来自swtch的依赖管理的三篇文章
0. 入口索引: <a href="https://research.swtch.com/vgo">https://research.swtch.com/vgo</a></p>
<ol>
<li>第一篇: <a href="https://research.swtch.com/vgo-intro">https://research.swtch.com/vgo-intro</a>
<ol>
<li>讲述了依赖管理的演进
09 nothing -&gt; 2010 go install -&gt; 2011 go get, 存在没有版本的问题: 1. api 稳定性变更 2. 可重复构建.</li>
<li>未来的规划: dep/glide 不将支持, 发布了vgo(兼容目前的依赖管理), go mod 是未来</li>
<li>go mod 的演示</li>
</ol>
</li>
<li>第二篇: <a href="https://research.swtch.com/vgo-tour">https://research.swtch.com/vgo-tour</a>
<ol>
<li>讲述了vgo的使用 (没有明显的依赖树, mod文件、版本检查都是有的、执行依赖的测试!、可降级、可以指定拒绝版本、替换、后退兼容)
可以不用看了</li>
</ol>
</li>
<li>第三篇: <a href="https://research.swtch.com/vgo-import">https://research.swtch.com/vgo-import</a>  保证函数语义不可变, 避免单例(防止重复注册导致崩溃, 以http handler注册为例, 升级v1到v2, handler可以使用 /v2结尾) 原子API更新(//go:fix 的注释)</li>
<li>第四篇: <a href="https://research.swtch.com/vgo-mvs">https://research.swtch.com/vgo-mvs</a><br>
讲述了 build列表构建、升级所有模块(go get -u)、升级单个模块(单个升级, 并不是简单的拉去最新的依赖, 还要避免之前的依赖降级)、降级一个模块
formulas: Horn、 dual-Horn、 dual-Horn、
2-SAT  NL-complete  NP-complete
4.1 The unique minimal downgrade does not use an older version of a given module unless absolutely necessary
4.2 The unique minimal upgrade does not use a newer version of a given module unless absolutely necessary
4.3 Minimal version selection always selects the minimal (oldest) module version that satisfies the overall requirements of a build
4.4 replace 和 exlude 都是模块内部的 (替换是顶级模块的.)
4.5 A module author is therefore in complete control of that module&rsquo;s build when it is the main program being built, but not in complete control of other users&rsquo; builds that depend on the module</li>
<li>第5篇: <a href="https://research.swtch.com/vgo-repro">https://research.swtch.com/vgo-repro</a>
可重复的构建, 可验证的构建. 引入 goversion 工具. 主要是vgo</li>
<li>第6篇: <a href="https://research.swtch.com/vgo-module">https://research.swtch.com/vgo-module</a>
定义了go modules.
讨论了go mododule的特性: 有版本的release. go mod 文件格式. 一个仓库一个模块, 下载协议, 代理服务, vendor兼容</li>
<li>第7篇: <a href="https://research.swtch.com/vgo-cmd">https://research.swtch.com/vgo-cmd</a>
讨论了构建隔离级别、自动下载, vgo的特性</li>
</ol>
<ul>
<li>总结:</li>
</ul>
<ol>
<li>提出了 import compatibility rule(相同import路径的向后兼容)、minimal version selection(默认选择最老的)</li>
<li>大部分依赖管理工具 只处理了 依赖copy, 并没有解决版本依赖, 建议使用go mod</li>
<li>glide、godep 停止开发, dep、vgo作为官方的试验品, 也建议升级到 go mod.</li>
<li>gomodule的意思是:a group of packages versioned as a single unit</li>
<li>提出了 Hyrum&rsquo;s law: <a href="http://www.hyrumslaw.com/">http://www.hyrumslaw.com/</a></li>
</ol>
<h2 id="操作">操作</h2>
<h3 id="创建项目">创建项目</h3>
<ol>
<li>任意目录创建项目, 假设文件名 project</li>
<li>在project下创建doc.go (可以是其他文件), doc.go 写入 &ldquo;package main // import &ldquo;a.b.c&rdquo; (如果是goapth路径下创建, 就不需要这一次的操作)</li>
<li>执行 go mod init project</li>
<li>编写代码, 确实依赖的时候, 执行 go mod tidy.</li>
<li>准备打包编译的时候, 建议使用vendor, 命令: go mod vendor</li>
</ol>
<h3 id="govendor-go-mod">govendor-&gt;go mod</h3>
<ol>
<li>go mod init (因为govendor的项目, 大都是gopath下面的, 所以单独使用doc.go那种申明方式)</li>
<li>go mod vendor, 实现将原来的vendor目录进行重写.</li>
</ol>
<h2 id="新增">新增</h2>
<p>官方出了相关资料:
略微提及规划和使用方式: <a href="https://blog.golang.org/modules2019">https://blog.golang.org/modules2019</a>
使用姿势: <a href="https://blog.golang.org/using-go-modules">https://blog.golang.org/using-go-modules</a>
起草go module的一个历程总述: 写了个原型, 然后写草案了, 然后成了 go module: <a href="https://blog.golang.org/versioning-proposal">https://blog.golang.org/versioning-proposal</a>
官方的proposal:
<a href="https://github.com/golang/go/issues/24301">https://github.com/golang/go/issues/24301</a>
????
<a href="https://github.com/golang/proposal/blob/master/design/24301-versioned-go.md">https://github.com/golang/proposal/blob/master/design/24301-versioned-go.md</a></p>
]]></content>
        </item>
        
        <item>
            <title>Golang Init</title>
            <link>https://xujianhai.fun/posts/golang-init/</link>
            <pubDate>Sun, 21 Apr 2019 12:04:00 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-init/</guid>
            <description>初始化根据平台的不同而不同, 这里参考 adm64 的汇编. 入口参考 runtime#asm_amd64.s, _rt0_amd64(SB)、main(SB)、_rt0_amd64_lib(SB) 分别对应了 内部链接、外部连接以及共享库的三种不同的启动方式. 但是, 最终都会调用 rt0_go.
TEXT runtime·rt0_go(SB),NOSPLIT,$0 // copy arguments forward on an even stack MOVQ DI, AX // argc MOVQ SI, BX // argv SUBQ $(4*8+7), SP // 2args 2auto ANDQ $~15, SP MOVQ AX, 16(SP) MOVQ BX, 24(SP) // create istack out of the given (operating system) stack. // _cgo_init may update stackguard. MOVQ $runtime·g0(SB), DI LEAQ (-64*1024+104)(SP), BX MOVQ BX, g_stackguard0(DI) MOVQ BX, g_stackguard1(DI) MOVQ BX, (g_stack+stack_lo)(DI) MOVQ SP, (g_stack+stack_hi)(DI) .</description>
            <content type="html"><![CDATA[<p>初始化根据平台的不同而不同, 这里参考 adm64 的汇编.
入口参考 runtime#asm_amd64.s, _rt0_amd64(SB)、main(SB)、_rt0_amd64_lib(SB) 分别对应了 内部链接、外部连接以及共享库的三种不同的启动方式. 但是, 最终都会调用 rt0_go.</p>
<pre><code>TEXT runtime·rt0_go(SB),NOSPLIT,$0
    // copy arguments forward on an even stack
    MOVQ    DI, AX      // argc
    MOVQ    SI, BX      // argv
    SUBQ    $(4*8+7), SP        // 2args 2auto
    ANDQ    $~15, SP
    MOVQ    AX, 16(SP)
    MOVQ    BX, 24(SP)
    
    // create istack out of the given (operating system) stack.
    // _cgo_init may update stackguard.
    MOVQ    $runtime·g0(SB), DI
    LEAQ    (-64*1024+104)(SP), BX
    MOVQ    BX, g_stackguard0(DI)
    MOVQ    BX, g_stackguard1(DI)
    MOVQ    BX, (g_stack+stack_lo)(DI)
    MOVQ    SP, (g_stack+stack_hi)(DI)

    .... 
ok:
    // set the per-goroutine and per-mach &quot;registers&quot;
    get_tls(BX)
    LEAQ    runtime·g0(SB), CX
    MOVQ    CX, g(BX)
    LEAQ    runtime·m0(SB), AX

    // save m-&gt;g0 = g0
    MOVQ    CX, m_g0(AX)
    // save m0 to g0-&gt;m
    MOVQ    AX, g_m(CX)

    CLD             // convention is D is always left cleared
    CALL    runtime·check(SB)

    MOVL    16(SP), AX      // copy argc
    MOVL    AX, 0(SP)
    MOVQ    24(SP), AX      // copy argv
    MOVQ    AX, 8(SP)
    CALL    runtime·args(SB)
    CALL    runtime·osinit(SB)
    CALL    runtime·schedinit(SB)

    // create a new goroutine to start program
    MOVQ    $runtime·mainPC(SB), AX     // entry
    PUSHQ   AX
    PUSHQ   $0          // arg size
    CALL    runtime·newproc(SB)
    POPQ    AX
    POPQ    AX

    // start this M
    CALL    runtime·mstart(SB)

    CALL    runtime·abort(SB)   // mstart should never return
    RET

    // Prevent dead-code elimination of debugCallV1, which is
    // intended to be called by debuggers.
    MOVQ    $runtime·debugCallV1(SB), AX
    RET

DATA    runtime·mainPC+0(SB)/8,$runtime·main(SB) // runtime·mainPC指向了 runtime.main, 也就是第一个goroutine
</code></pre><p>主要执行了下面几件事情:</p>
<pre><code>CALL    runtime·args(SB)   // 命令行参数传递, 参考 runtime1.go  
CALL    runtime·osinit(SB) // 平台相关, 获取cpu数目, physPageSize, 参考os_freebsd.go 
CALL    runtime·schedinit(SB) // 负责了 栈 、内存分配和垃圾回收、 并发等的初始化
MOVQ    $runtime·mainPC(SB), AX // 指出了第一只goroutine 
CALL    runtime·newproc(SB) // 启动新的goroutine 
CALL    runtime·mstart(SB) // schedule: 发现可运行的goroutine并进行执行, 循环永远不返回 
</code></pre>
<p>这里面, 重点是 mstart, 通过后面的 <a href="/post/golang-sch">golang调度器分析</a>可以知道, schedule是通过自旋线程来避免频繁的线程切换的, 并保证至少一个自旋线程-&gt;意味着至少有一个空闲线程可以用来处理任务, 初始化的时候, 只有一个线程, 当准备进行goroutine执行的时候, 就会创建新的线程. 参看方法 #resetspinning-&gt;#wakep-&gt;#startm, 通过代码发现, 想要仅仅创建自旋m的话, 还需要有p. 创建m的方法参看 #newm</p>
<p>这里面, newproc 也是golang内部创建 goroutine的函数入口.</p>
<p>第一个执行的goroutine:
runtime#main, 负责了 runtime_init、gcenable、用户main函数入口. 其中  runtime_init 通过看连接可以知道, 实际上是启动了新的goroutine进行 forcegchelper, 通过定时器触发gc操作. gcenable 和gc有关, 启动新的goroutine进行bgsweep</p>
]]></content>
        </item>
        
        <item>
            <title>Golang Channel</title>
            <link>https://xujianhai.fun/posts/golang-channel/</link>
            <pubDate>Sun, 21 Apr 2019 11:23:31 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-channel/</guid>
            <description>channel的源代码: chan.go
关键的数据结构 type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters lock mutex } type waitq struct { first *sudog last *sudog } sudog是等待的goroutine的表示, 参考 runtime2.</description>
            <content type="html"><![CDATA[<p>channel的源代码: chan.go</p>
<h2 id="关键的数据结构">关键的数据结构</h2>
<pre><code>type hchan struct {
    qcount   uint           // total data in the queue
    dataqsiz uint           // size of the circular queue
    buf      unsafe.Pointer // points to an array of dataqsiz elements
    elemsize uint16
    closed   uint32
    elemtype *_type // element type
    sendx    uint   // send index
    recvx    uint   // receive index
    recvq    waitq  // list of recv waiters
    sendq    waitq  // list of send waiters

    lock mutex
}

type waitq struct {
    first *sudog
    last  *sudog
}
</code></pre><p>sudog是等待的goroutine的表示, 参考 runtime2.go</p>
<pre><code>type sudog struct {
    g *g

    isSelect bool
    next     *sudog
    prev     *sudog
    elem     unsafe.Pointer // data element (may point to stack)

    acquiretime int64
    releasetime int64
    ticket      uint32
    parent      *sudog // semaRoot binary tree
    waitlink    *sudog // g.waiting list or semaRoot
    waittail    *sudog // semaRoot
    c           *hchan // channel
}
</code></pre><p>通过数据结构, 可以发现, hchan 使用环形队列组织数据, 发送和接受都是用 双向队列维护.</p>
<h2 id="创建">创建</h2>
<p>channel的创建, 会调用 #makechan</p>
<ol>
<li>如果是无界channel, 只分配基本的内存空间, 用首地址实现同步</li>
<li>如果是基本类型的channel, 直接分配完整的内存空间, 数据buf指向 内存空间首地址+hchanSize</li>
<li>指针类型的channel, 直接new方法分配</li>
</ol>
<h2 id="发送">发送</h2>
<p>当我们执行 c &lt;- x 的语句的时候, 发生了什么?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-chan.go" data-lang="chan.go"><span style="color:#a6e22e">chansend1</span>(<span style="color:#a6e22e">汇编入口</span> ) <span style="color:#f92672">-</span>&gt; <span style="color:#a6e22e">chansend</span> <span style="color:#f92672">-</span>&gt; <span style="color:#a6e22e">sendDirect</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">goready</span>(<span style="color:#a6e22e">唤醒</span>)
</code></pre></div><ol>
<li>当channel是nil, 并且是阻塞调用, 挂起当前goroutine</li>
<li>当channel是nil, 并且是非阻塞调用, 直接返回</li>
<li>发送的时候会进行锁操作, 给已经关闭的channel 发送数据会抛出异常(不符合认知)</li>
<li>如果有 recvq有等待的goroutine, 直接发送给它, 并进行唤醒</li>
<li>databuf还没有满的情况下, 将数据放入环形缓冲区,</li>
<li>放不下的情况, 放入senq的等待队列,</li>
</ol>
<h2 id="接收">接收</h2>
<p>当我们执行 x &lt;- c, 执行流程</p>
<pre><code>chanrecv1(汇编入口) -&gt; chanrecv -&gt; recv -&gt; recvDirect + goready(唤醒)
</code></pre><p>快速检测: 针对channel没有关闭, 并且是非阻塞调用的情况下.</p>
<ol>
<li>unbuffer channel没有发送的队列(sendq)</li>
<li>buffer channel没有数据发送(qcount=0)
流程:</li>
<li>当channel是nil, 并且是阻塞调用, 会挂起</li>
<li>当channel是nil, 并且是非阻塞调用, 直接返回</li>
<li>senq有阻塞的sender, 直接接收, 同时唤醒sender goroutine</li>
<li>有数据的情况下, 复制数据,</li>
<li>非阻塞调用, 直接返回</li>
<li>构建sudog, 放入recvq.</li>
</ol>
<h2 id="close">close</h2>
<p>方法入口 #closechan</p>
<ol>
<li>关闭多次, 会panic</li>
<li>遍历recvq中阻塞的等待, 进行释放</li>
<li>遍历sendq中阻塞的等待, 进行释放</li>
<li>遍历之前需要释放的goroutine, 进行唤醒</li>
</ol>
<h2 id="select-模型">select 模型:</h2>
<p>select的send:</p>
<pre><code>selectnbsend(编译入口) -&gt; chansend, 不阻塞的调用 
</code></pre><pre><code>selectnbrecv(编译入口) -&gt; chanrecv, 不阻塞的调用
</code></pre><p>ok类型的receive:</p>
<pre><code>selectnbrecv2(编译入口) -&gt; chanrecv, 不阻塞的调用
</code></pre><p>相比之前的receive操作, 多了对nil的判断, 意义不是特别大.</p>
<p>faq: 那么, 在使用上, 向一个nil的 channel 进行 receive 和 send会怎么样?
原理分析:
1. receive 情况:
1. select模型下, 都是使用 非阻塞的调用, 会返回失败
2. 常规使用, 没有ok的情况下, 都是阻塞调用, block forever(gopark), 可能没有线程执行, 会抛出异常
3. 常规使用, 有ok的情况下, 不阻塞, ok是false
2. send 情况:
1. select模型下, 使用非阻塞调用, 会返回失败
2. 常规使用, 都是阻塞调用, block forever (gopark), 可能没有线程执行, 会抛出异常
因为不是golang风格的异常, 所以, 使用defer也不会被检查出来</p>
<h2 id="实验">实验:</h2>
<ol>
<li>
<p>常规空指针接收:
<img src="/recvnil.png" alt="receive nil" title=""></p>
</li>
<li>
<p>常规空指针发送:
<img src="/sendnil.png" alt="send nil" title=""></p>
</li>
<li>
<p>空指针 + select send
<img src="/select-send-nil.png" alt="select send nil" title=""></p>
</li>
<li>
<p>空指针 + select receive
<img src="/select-recv-nil.png" alt="select receive nil" title=""></p>
</li>
</ol>
<h2 id="补充说明">补充说明:</h2>
<p>上面的4个实验, 看上去都有报错, 其实是因为 当前的main函数的goroutine被阻塞, 导致没有可执行的goroutine导致的报错, 添加一个可运行的goroutine就可以了, 实验如下图:</p>
<p><img src="/just-sleep.png" alt="just-sleep" title=""></p>
<p>参考:</p>
<ol>
<li><a href="https://blog.lab99.org/post/golang-2017-10-04-video-understanding-channels.html">Joshi的博客</a></li>
<li><a href="https://ninokop.github.io/2017/11/07/Go-Channel%E7%9A%84%E5%AE%9E%E7%8E%B0/">nino的博客</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Face</title>
            <link>https://xujianhai.fun/posts/golang-face/</link>
            <pubDate>Sun, 21 Apr 2019 10:54:16 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-face/</guid>
            <description>preface golang的interface是使用很广泛的一种手段, 相比java python 等语言, golang的interface不需要明显的继承和实现, 只需要实现相应的方法就可以了. 常常我们需要抽象struct的interface, 方便mock进行测试.
detail golang的interface主要有两种: eface 和 iface. eface是不带方法的interface,
定义参看 runtime2.go 和 type.go,
type iface struct { tab *itab data unsafe.Pointer } type eface struct { _type *_type data unsafe.Pointer } type itab struct { inter *interfacetype _type *_type hash uint32 // copy of _type.hash. Used for type switches.  _ [4]byte fun [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter. } type interfacetype struct { typ _type pkgpath name mhdr []imethod } type _type struct { size uintptr ptrdata uintptr // size of memory prefix holding all pointers  hash uint32 tflag tflag align uint8 fieldalign uint8 kind uint8 alg *typeAlg // gcdata stores the GC type data for the garbage collector.</description>
            <content type="html"><![CDATA[<h2 id="preface">preface</h2>
<p>golang的interface是使用很广泛的一种手段, 相比java python 等语言, golang的interface不需要明显的继承和实现, 只需要实现相应的方法就可以了. 常常我们需要抽象struct的interface, 方便mock进行测试.</p>
<h2 id="detail">detail</h2>
<p>golang的interface主要有两种: eface 和 iface. eface是不带方法的interface,</p>
<p>定义参看 runtime2.go 和 type.go,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-runtime2.go" data-lang="runtime2.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">iface</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">tab</span>  <span style="color:#f92672">*</span><span style="color:#a6e22e">itab</span>
    <span style="color:#a6e22e">data</span> <span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Pointer</span>
}

<span style="color:#66d9ef">type</span> <span style="color:#a6e22e">eface</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">_type</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">_type</span>
    <span style="color:#a6e22e">data</span>  <span style="color:#a6e22e">unsafe</span>.<span style="color:#a6e22e">Pointer</span>
}

<span style="color:#66d9ef">type</span> <span style="color:#a6e22e">itab</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">inter</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">interfacetype</span>
    <span style="color:#a6e22e">_type</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">_type</span>
    <span style="color:#a6e22e">hash</span>  <span style="color:#66d9ef">uint32</span> <span style="color:#75715e">// copy of _type.hash. Used for type switches.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">_</span>     [<span style="color:#ae81ff">4</span>]<span style="color:#66d9ef">byte</span>
    <span style="color:#a6e22e">fun</span>   [<span style="color:#ae81ff">1</span>]<span style="color:#66d9ef">uintptr</span> <span style="color:#75715e">// variable sized. fun[0]==0 means _type does not implement inter.
</span><span style="color:#75715e"></span>}
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-type.go" data-lang="type.go"><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">interfacetype</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">typ</span>     <span style="color:#a6e22e">_type</span>
    <span style="color:#a6e22e">pkgpath</span> <span style="color:#a6e22e">name</span>
    <span style="color:#a6e22e">mhdr</span>    []<span style="color:#a6e22e">imethod</span>
}
<span style="color:#66d9ef">type</span> <span style="color:#a6e22e">_type</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">size</span>       <span style="color:#66d9ef">uintptr</span>
    <span style="color:#a6e22e">ptrdata</span>    <span style="color:#66d9ef">uintptr</span> <span style="color:#75715e">// size of memory prefix holding all pointers
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">hash</span>       <span style="color:#66d9ef">uint32</span>
    <span style="color:#a6e22e">tflag</span>      <span style="color:#a6e22e">tflag</span>
    <span style="color:#a6e22e">align</span>      <span style="color:#66d9ef">uint8</span>
    <span style="color:#a6e22e">fieldalign</span> <span style="color:#66d9ef">uint8</span>
    <span style="color:#a6e22e">kind</span>       <span style="color:#66d9ef">uint8</span>
    <span style="color:#a6e22e">alg</span>        <span style="color:#f92672">*</span><span style="color:#a6e22e">typeAlg</span>
    <span style="color:#75715e">// gcdata stores the GC type data for the garbage collector.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// If the KindGCProg bit is set in kind, gcdata is a GC program.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// Otherwise it is a ptrmask bitmap. See mbitmap.go for details.
</span><span style="color:#75715e"></span>    <span style="color:#a6e22e">gcdata</span>    <span style="color:#f92672">*</span><span style="color:#66d9ef">byte</span>
    <span style="color:#a6e22e">str</span>       <span style="color:#a6e22e">nameOff</span>
    <span style="color:#a6e22e">ptrToThis</span> <span style="color:#a6e22e">typeOff</span>
}
<span style="color:#66d9ef">type</span> <span style="color:#a6e22e">imethod</span> <span style="color:#66d9ef">struct</span> {
    <span style="color:#a6e22e">name</span> <span style="color:#a6e22e">nameOff</span>
    <span style="color:#a6e22e">ityp</span> <span style="color:#a6e22e">typeOff</span>
}
</code></pre></div><p>在设计上, iface 比 eface多了一些设计:</p>
<ol>
<li>interfaceType: 存放interface的公共描述, 类似 maptype、arraytype、chantype.</li>
<li>iface使用了itab存储了类型相关信息.</li>
<li>iface和eface的data指向具体的数据</li>
<li>_type是所有类型信息结构的公共部分</li>
</ol>
<h2 id="类型转换">类型转换</h2>
<p>在 iface.go 中提供了 convT2E convT2I convI2I 等方法, E I T 分别表示 eface、iface、具体类型, 除了具体类型和eface iface之间的类型转换</p>
<p>通常我们会用interface定义接口类型, 然后通过不同的实现来开发. 比如存储层, 提供 mysql/redis/mem 三种形式, 亦或者 代理方式.</p>
<p>这里我们使用模拟rpc调用客户端进行演示, 如下:
第一种显示转换:</p>
<pre><code>type HelloClient interface{
    SayHello()
}

type MockHelloClient struct{}

func (MockHelloClient) SayHello() {
    println(&quot;mock hello&quot;)
}

func TestHelloClient(c HelloClient){
   c.SayHello()
} 

func main(){
    mc := HelloClient(MockHelloClient())
    TestHelloClient(mc)
}
</code></pre><p>其实, 更常见的是错误码/枚举的定制, 比如</p>
<pre><code>type ErrCode int32 


</code></pre><p>第二种被动转换:</p>
<pre><code>type HelloClient interface{
    SayHello()
}

type MockHelloClient struct{}

func (MockHelloClient) SayHello() {
    println(&quot;mock hello&quot;)
}

func TestHelloClient(c HelloClient){
   c.SayHello()
} 

func main(){
    var mc MockHelloClient
    TestHelloClient(mc)
}
</code></pre><p>看下汇编的实现</p>
<pre><code>go build -gcflags '-l' -o main main.go 
go tool objdump -s &quot;main\.TestHelloClient&quot; main
</code></pre><p>得到如下图的内容</p>
<pre><code>TEXT main.TestHelloClient(SB) /Users/snow_young/go/src/code.byted.org/awesomeProject1/inter/main.go
  main.go:13        0x104e1a0       65488b0c2530000000  MOVQ GS:0x30, CX
  main.go:13        0x104e1a9       483b6110        CMPQ 0x10(CX), SP
  main.go:13        0x104e1ad       762c            JBE 0x104e1db
  main.go:13        0x104e1af       4883ec10        SUBQ $0x10, SP
  main.go:13        0x104e1b3       48896c2408      MOVQ BP, 0x8(SP)
  main.go:13        0x104e1b8       488d6c2408      LEAQ 0x8(SP), BP
  main.go:14        0x104e1bd       488b442418      MOVQ 0x18(SP), AX
  main.go:14        0x104e1c2       488b4018        MOVQ 0x18(AX), AX
  main.go:14        0x104e1c6       488b4c2420      MOVQ 0x20(SP), CX
  main.go:14        0x104e1cb       48890c24        MOVQ CX, 0(SP)
  main.go:14        0x104e1cf       ffd0            CALL AX
  main.go:15        0x104e1d1       488b6c2408      MOVQ 0x8(SP), BP
  main.go:15        0x104e1d6       4883c410        ADDQ $0x10, SP
  main.go:15        0x104e1da       c3          RET
  main.go:13        0x104e1db       e80089ffff      CALL runtime.morestack_noctxt(SB)
  main.go:13        0x104e1e0       ebbe            JMP main.TestHelloClient(SB)
</code></pre><p>按照 这篇<a href="http://legendtkl.com/2017/07/01/golang-interface-implement/">文章</a>的说法, 0x18(SP) 是 itab的地址, 这里进行了函数地址的存储.</p>
<p>因为是link的缘故, 我们并不能直观的看到函数的调用.</p>
<h2 id="类型断言">类型断言</h2>
<p>iface.go中提供了 assertI2I assertI2I2 assertE2I assertE2I2 等断言方法.
什么时候使用断言呢?
在rpc调用中, 我们可能需要传递一些上下文信息, 通常我们会通过ctx传递, 如下:</p>
<pre><code>import &quot;context&quot;

type baseKey struct{}

type reqBase struct {
    LogId   int64
    from_ip string
    ip      string
}

func main() {
    c := context.Background()

    c = context.WithValue(c, baseKey{}, &amp;reqBase{LogId: 11, from_ip: &quot;127.0.0.1&quot;, ip: &quot;127.0.0.1&quot;})
    baseInfo := c.Value(baseKey{}).(*reqBase)
    println(&quot;base info: &quot;, baseInfo.LogId, baseInfo.from_ip)
}
</code></pre><p>如果ctx中明确传递了上下文信息的, 可以直接断言. 不确定的情况下, 建议使用:</p>
<pre><code>func main() {
    c := context.Background()

    c = context.WithValue(c, baseKey{}, &amp;reqBase{LogId: 11, from_ip: &quot;127.0.0.1&quot;, ip: &quot;127.0.0.1&quot;})
    baseInfo, ok := c.Value(baseKey{}).(*reqBase)
    if !ok{
        println(&quot;convert failed.&quot;)
        return 
    }
    println(&quot;base info: &quot;, baseInfo.LogId, baseInfo.from_ip)
}
</code></pre><p>如果使用objdump, 如果不是显式的转换, 很难找到相关的函数调用.</p>
<h2 id="动态分发">动态分发</h2>
<h2 id="反射">反射</h2>
<ol>
<li>[](<a href="https://research.swtch.com/interfaces">https://research.swtch.com/interfaces</a>)</li>
<li><a href="http://xargin.com/go-and-interface/">深度讲解的翻译</a></li>
<li><a href="http://legendtkl.com/2017/07/01/golang-interface-implement/">Legendtkl blog: Go Interfacey源码解析</a></li>
<li><a href="http://legendtkl.com/2017/06/12/understanding-golang-interface/">legendtkl blog: 深入理解 Go Interface</a></li>
<li><a href="https://www.youtube.com/watch?v=F4wUrj6pmSI&amp;t=2319s">youtube: understanding the interface</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Golang Sch</title>
            <link>https://xujianhai.fun/posts/golang-sch/</link>
            <pubDate>Sun, 14 Apr 2019 13:00:03 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/golang-sch/</guid>
            <description>2015上学的时候, golang正在兴起, 参加了一些会议和组织等, 但是工作关系, 一直无缘接触, 在饿了么打算开发分布式文件系统的时候, 也因为自身身体原因提出了离职在家休养, 错过了一次机会. 后来到头条上班, 发现golang承担了主要的系统语言, 重新激起了很强的学习兴趣. &amp;ndash; zero.xu
构成 通过proc.go可以知道, golang的调度器实现只要有以下三个部分构成:
 G: goroutine. M: worker线程, 或者机器. P: 指定Go代码块的资源  关系如下: M必须拥有P来执行Go代码, 但是, M可以在P上被阻塞或者读写的系统调用
个人的理解下来, 是
 G: golang里面的goroutine代码块, 以及对应的stack等信息 M: 对应着操作系统的物理线程 P: Goroutine队列, 也称作 logic processor. M执行的时候, 会选择一个P, 然后取出其中的G进行执行  设计的文档: Scalable Go Scheduler Design Doc
 设计的思考 proc.go 注释上的详细分析, 个人理解如下: worker线程挂起/唤醒的研究: 一方面, 为了提高并行度尽量多的保持work线程, 一方面, 为了节约cpu和电量, 要挂起运行的worker线程. 为了在这两个方面取得很好的平衡, 我们需要考虑:
 调度状态需要是分布式的(特殊状态下, 可以使用 每个P一个worker 队列), 所以, 不可能快速的计算出全局状态?</description>
            <content type="html"><![CDATA[<p>2015上学的时候, golang正在兴起, 参加了一些会议和组织等, 但是工作关系, 一直无缘接触, 在饿了么打算开发分布式文件系统的时候, 也因为自身身体原因提出了离职在家休养, 错过了一次机会. 后来到头条上班, 发现golang承担了主要的系统语言, 重新激起了很强的学习兴趣.
&ndash; zero.xu</p>
<h2 id="构成">构成</h2>
<p>通过proc.go可以知道, golang的调度器实现只要有以下三个部分构成:</p>
<ul>
<li>G: goroutine.</li>
<li>M: worker线程, 或者机器.</li>
<li>P: 指定Go代码块的资源</li>
</ul>
<p>关系如下: M必须拥有P来执行Go代码, 但是, M可以在P上被阻塞或者读写的系统调用</p>
<p>个人的理解下来, 是</p>
<ul>
<li>G: golang里面的goroutine代码块, 以及对应的stack等信息</li>
<li>M: 对应着操作系统的物理线程</li>
<li>P: Goroutine队列, 也称作 logic processor. M执行的时候, 会选择一个P, 然后取出其中的G进行执行</li>
</ul>
<p>设计的文档: <a href="https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit">Scalable Go Scheduler Design Doc</a></p>
<hr>
<h2 id="设计的思考">设计的思考</h2>
<p>proc.go 注释上的详细分析, 个人理解如下:
worker线程挂起/唤醒的研究:
一方面, 为了提高并行度尽量多的保持work线程, 一方面, 为了节约cpu和电量, 要挂起运行的worker线程. 为了在这两个方面取得很好的平衡, 我们需要考虑:</p>
<ol>
<li>调度状态需要是分布式的(特殊状态下, 可以使用 每个P一个worker 队列), 所以, 不可能快速的计算出全局状态?</li>
<li>对于最优的线程调度, 我们需要知道未来的状态 (如果一个新的goroutine在不就的未来会被读取, 不要挂起工作线程)</li>
</ol>
<p>三种被拒绝的方案:</p>
<ol>
<li>集中所有的调度器状态, 将抑制可扩展性</li>
<li>直接goroutine切换, 就是说, 当我们启动一个新的goroutine, 并且有空闲的P, 唤醒一个线程, 并且切换到那个被唤醒的线程和gorouitne. 这将会导致线程状态的频繁变更, 因为刚读取了goroutine的线程在下一刻可能就不工作了(切换到了其他线程), 我们需要唤醒他. 而且, 这也会破坏 计算的本地性 (线程切换的缘故)</li>
<li>当我们需要开始一个goroutine并且有空闲的P, 唤醒一个额外的线程, 但是不进行切换. 这将导致过度的线程唤醒和挂起, 因为额外的线程在没发现要做的工作的时候, 会被立即唤醒</li>
</ol>
<p>目前的策略:
当准备一个goroutine的时候, 如果满足下面的条件的时候, 我们会唤醒额外的线程</p>
<ol>
<li>有空闲的P, 但是没有自旋的工作线程,
工作线程没有在发现在全局的运行队列或者 netpooller发现工作的时候, 那么, 他就会脱离本地工作, 并进行自旋. 线程的唤醒也是通过自旋实现的; 不使用goroutine切换的策略以避免线程一开始脱离工作. 自选线程在挂起钱会减产每个P运行队列的工作. 如果自旋线程发现了工作, 那么他讲自己脱离自旋状态, 并进行执行. 如果没有发现工作, 那么就离开自旋状态并挂起.
如果至少有一个自旋线程, 当要准备gorouine的时候, 我们就不会挂起唤醒新的线程. 作为补偿, 如果最后的自旋线程发现任务并停止了自旋, 它必须唤醒一个新的自旋线程. 这样的处理方式抚平了线程唤醒的不必要的突刺, 但是同时保证了最终最大的cpu利用率.<br>
主要的实现的困难是线程 sprinning 自旋 -&gt; non-spinning 非自旋的线程切换. 这种切换会和新的goroutine的提交、唤醒其他的worker线程产生竞态. 如果不能很好的处理这种情况, 我们最终会得到很低的cpu利用率. 针对goroutine启动的通过方式是: 将goroutine 提交到本地工作队列, #StoreLoad-style 的内存屏障, 检查 sched.nmspinning 参数. 自旋-&gt;非自旋的转换的通用处理方式是: nmspinning减一, #StoreLoad-style内存屏障, 为了查找新的工作, 检查所有的 每个P的工作队列. 注意, 这些措施并不会运用到全局运行队列, 这样, 我们在提交任务给全局运行队列的时候, 并不会草率的进行线程唤醒.</li>
</ol>
<p>小结:</p>
<ol>
<li>自旋线程的设计来实现线程的唤醒和挂起</li>
<li>通过检查每个P的运行队列情况来判定是否需要挂起</li>
<li>最后一个自旋线程处理任务前, 需要唤醒一个新的自旋线程, 保证至少有一个自旋线程.</li>
</ol>
<hr>
<h2 id="实现的分析">实现的分析</h2>
<h3 id="基本概念">基本概念</h3>
<p>M、P、G的主体设计在 runtime2.go 中, 除去 m、p、g 的关键对象, 其他的讲解如下</p>
<ol>
<li>
<p>特殊对象</p>
<ol>
<li>g0: 有调度栈的goroutine, 负责管理任务. 每个m0都有自己的g0, 在调度的时候/系统调用的时候会使用g0的栈空间. g0本身不指向任何可执行函数. 全局变量g0是m0的g0</li>
<li>m0: 启动程序后的主线程, 负责初始化以及第一个g, 之后和其他的m一样了</li>
</ol>
</li>
<li>
<p>sudog</p>
<ol>
<li>sudog表示等待列表中的一个g, 比如在channel上的 sending/receiving. 因为 go&lt;-&gt;同步对象之间的关系是多对多的, 需要通过sudog进行维护. 一个可能在多个等待列表上, 这样一个g可能有多个 sudog; 很多g可能在相同的同步对象, 所以一个对象可能有多个sudog. sudog 通过特殊的pool进行分配的, 通过 acquireSudog 和 releaseSudog进行封装. 实现上, 通过双列表结构 pre + next 实现了列表.</li>
</ol>
</li>
<li>
<p>schedt
维护了全局的运行队列, sudog结构的集中缓存, 不同大小的defer结构的集中pool</p>
</li>
<li>
<p>栈</p>
</li>
<li>
<p>itab iface eface: interface相关, 后面讲</p>
</li>
<li>
<p>defer: 后面讲, 有趣的是:defer维护在P上</p>
</li>
<li>
<p>状态:</p>
<ul>
<li>g:</li>
</ul>
<pre><code>_Gidle _Grunnable _Grunning _Gsyscall _Gwaiting   _Gmoribund_unused _Gdead _Genqueue_unused _Gcopystack _Gscan _Gscanrunnable _Gscanrunning _Gscansyscall _Gscanwaiting
</code></pre><p>最后的五种状态和gc有关</p>
<ul>
<li>p: _Pidle _Prunning _Psyscall _Pgcstop _Pdead</li>
</ul>
</li>
</ol>
<h3 id="调度执行">调度执行</h3>
<ol>
<li>初始化:
参考<a href="/post/golang-init">初始化的文章</a>, 在初始化的时候, 会创建 m 执行第一个goroutine任务.</li>
<li>执行goroutine代码:
新建的goroutine: proc.go#newproc</li>
<li>阻塞调用
参考 runtime.go#entersyscallblock #reentersyscall</li>
<li>sudog的使用: 分析channel的时候用</li>
<li>切换
当发生系统调用/M锁住的情况下, 会切换P.#handleoff
goready goparkunlock  gopark releaseSudog/acquireSudog ready</li>
</ol>
<p>faq:</p>
<ol>
<li>新的goroutine什么时候放入全局队列, 什么时候放入 本地队列呢?
本地队列放不下的时候,</li>
<li>什么时候实现抢占:
sysmon函数, 无限循环 + sleep, 20us开始, sleep延迟*2 倍增, 最大不超过10ms. 处理 netpoll、retake(阻塞在系统调用上的P)、强制gc、内存收缩. 其中retake的实现中, 遍历所有的goroutine, 将连续执行10ms的goroutine的stackguard0设置为stackPreempt, 触发 stack check.</li>
</ol>
<p>更多参考:<a href="http://xiaorui.cc/2018/06/04/golang%E5%AF%86%E9%9B%86%E5%9C%BA%E6%99%AF%E4%B8%8B%E5%8D%8F%E7%A8%8B%E8%B0%83%E5%BA%A6%E9%A5%A5%E9%A5%BF%E9%97%AE%E9%A2%98/">golang密集场景下的协程调度饥饿问题</a></p>
<h2 id="写屏障">写屏障:</h2>
<p>具体参考 runtime2.go</p>
<ol>
<li>
<p>reachable:</p>
<p>P 和 G 通过  allgs 和  allp 列表的真正的指针或者 栈上变量 (在达到列表前分配的时候)实现了reachable
M 通过 allm 和 freem 的真正的指针实现了可达</p>
</li>
<li>
<p>gc指针
分别使用 gunitptr、munitpter、punitptr 用来传递写屏障.</p>
<ol>
<li>
<p>guintptr 存储了goroutine指针, 并通过 unitptr类型传递了写屏障. gunitptr 在 Gobuf goroutine状态 以及 没有P操作的调度列表中使用</p>
</li>
<li>
<p>muintptr 不是用来gc追踪的m指针, 因为在释放M的时候, 我们对muintptr做了约束:</p>
<ol>
<li>通过安全点不在本地持有 munitptr</li>
<li>muintptr 在堆上必须要被M自己持有, 这样, 在*m被释放的时候, mintptr不会被使用</li>
</ol>
</li>
<li>
<p>gobuf的设计, 存储sp, pc. 其中, ctxt 和gc有关, 并通过汇编进行设置和清除, 区别于写屏障. 但是 ctxt 是真实保存的, 活的寄存器, 只会在 真实的寄存器和gobuf之间进行交换. 因此, 在进行stack scanning的时候会被当做root, 也就是说不需要通过写屏障, 是通过汇编进行存储和恢复的. 同时, 他也会被当做指针, 这样其他的Go的写将会得到写屏障</p>
</li>
</ol>
<p>特殊处理:
在当前P被释放的情况下, 要避免写屏障? 因为GC会认为世界停止了, 而且不可预期的写屏障并不会和GC同步, 会导致 写屏障的半同步 (标记了对象, 但是没有将他放入队列). 如果GC跳过了这些对象并在入队之前完成 可能会发生, 那么 他将会不正确的释放对象. [细节需要参考下GC的设计]</p>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Tmq Oom</title>
            <link>https://xujianhai.fun/posts/tmq-oom/</link>
            <pubDate>Sat, 13 Apr 2019 14:10:45 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/tmq-oom/</guid>
            <description>周末上午, 突然接到业务方反馈 nsq-consumer 发生了 netty 内存泄露. 通过heap dump文件分析文件, 发下了大内存占用. 如下图
overall 界面
object-list 界面
通过点击 &amp;ldquo;overall 界面&amp;rdquo; 图的list object功能, 会进入 &amp;ldquo;object-list 界面&amp;rdquo;, 可以发现 nsqj 的内存占用最高, 问题定位到了, 那么, nsqj 线程是做什么的呢? nsqj实例化如下:
public AbstractNSQClient(BootstrapConfig config, String topic, String channel, int rdy, int workerNumber) { connections = new Connections(); executor = Executors.newFixedThreadPool(workerNumber, new ThreadFactoryBuilder().setNameFormat(&amp;quot;nsqj-&amp;quot;).setDaemon(true).build()); ...... } 可以发现, executor 使用了 fixed线程池, 这里的队列在sdk中使用的是 LinkedBlockingQueue, 队列是无限增长的. 这样看, 稳定是定位了, 就是因为 executor的队列一直在增长, 但是, 是谁再往队列中投递数据的呢? 还是无限投递! review代码如下:
public class NSQHandler extends SimpleChannelInboundHandler&amp;lt;NSQFrame&amp;gt; { .</description>
            <content type="html"><![CDATA[<p>周末上午, 突然接到业务方反馈 nsq-consumer 发生了 netty 内存泄露. 通过heap dump文件分析文件, 发下了大内存占用. 如下图</p>
<p><img src="/oom-ana.png" alt="oom-ana" title="oom-ana">
overall  界面</p>
<p><img src="/oom-list.png" alt="oom-list" title="oom-list">
object-list 界面</p>
<p>通过点击 &ldquo;overall  界面&rdquo; 图的list object功能, 会进入 &ldquo;object-list 界面&rdquo;, 可以发现 nsqj 的内存占用最高, 问题定位到了, 那么, nsqj 线程是做什么的呢? nsqj实例化如下:</p>
<pre><code>public AbstractNSQClient(BootstrapConfig config, String topic, String channel, int rdy,
      int workerNumber) {
    connections = new Connections();
    executor = Executors.newFixedThreadPool(workerNumber,
        new ThreadFactoryBuilder().setNameFormat(&quot;nsqj-&quot;).setDaemon(true).build());
    ......
}
</code></pre><p>可以发现, executor 使用了 fixed线程池, 这里的队列在sdk中使用的是 LinkedBlockingQueue, 队列是无限增长的.
这样看, 稳定是定位了, 就是因为 executor的队列一直在增长, 但是, 是谁再往队列中投递数据的呢? 还是无限投递!
review代码如下:</p>
<pre><code>public class NSQHandler extends SimpleChannelInboundHandler&lt;NSQFrame&gt; {
  .....
  protected void channelRead0(ChannelHandlerContext ctx, NSQFrame msg) throws Exception {
    final Connection con = Channels.getConnection(ctx.channel());
    if (con != null) {
      con.getParent().getExecutor().execute(() -&gt; con.incoming(msg));
    } else {
      if (!(msg instanceof ResponseFrame)) {
        throw new IllegalStateException(
            &quot;no connection attachment for channel : &quot; + ctx.channel().id());
      }
      logger.warn(&quot;unknow message: {}&quot;, msg);
    }
  }
  ......
</code></pre><p>通过查看引用关系, 发现是 nsq的consumer不断处理消息, 将收到的消息不断放入 executor 的队列. 但是nsq本身是有很好的 流控策略的, nsq 的 机制参考<a href="https://nsq.io/clients/tcp_protocol_spec.html">官方文档</a>, 结合现在发现的现象, 我直接怀疑是不是 rdy 计算上出现了问题, 这里看下rdy的流控的实现, 如下:</p>
<pre><code>public void incoming(NSQFrame frame) {
    ......
    if (frame instanceof MessageFrame) {
      MessageFrame msg = (MessageFrame) frame;
      long tot = this.totalMessages.incrementAndGet();
      if (tot % messagesPerBatch &gt; (messagesPerBatch / 2)) {
        this.write(new RDYCommand(this.messagesPerBatch));
      }
    .........
}
</code></pre><p>在上面的代码可以发现, 每次消费点位在 (messagesPerBatch / 2, messagesPerBatch) 的时候, 每消费一个消息, 都会触发向 服务端请求 messagesPerBatch 数量的消息. 这样, 消息量会叠加的很多, 如果消息体 还很大的话, oom现象就会很明显. 询问了业务方, 消息体确实很大, 为了快速修复问题, 先修改了新的 rdy策略 上线, 问题消息.</p>
]]></content>
        </item>
        
        <item>
            <title>Rust_learn</title>
            <link>https://xujianhai.fun/posts/rust_learn/</link>
            <pubDate>Thu, 11 Apr 2019 16:01:17 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rust_learn/</guid>
            <description>rust学习资料: rust每日新闻: https://github.com/RustStudy/rust_daily_news rust社区: https://rust.cc/ rust中文: https://rustlang-cn.org/ rust新闻: https://www.yuque.com/chaosbot/rustnews rust文档: https://doc.rust-lang.org/stable/book</description>
            <content type="html"><![CDATA[<p>rust学习资料:
rust每日新闻: <a href="https://github.com/RustStudy/rust_daily_news">https://github.com/RustStudy/rust_daily_news</a>
rust社区: <a href="https://rust.cc/">https://rust.cc/</a>
rust中文: <a href="https://rustlang-cn.org/">https://rustlang-cn.org/</a>
rust新闻: <a href="https://www.yuque.com/chaosbot/rustnews">https://www.yuque.com/chaosbot/rustnews</a>
rust文档: <a href="https://doc.rust-lang.org/stable/book">https://doc.rust-lang.org/stable/book</a></p>
]]></content>
        </item>
        
        <item>
            <title>Github_lb</title>
            <link>https://xujianhai.fun/posts/github_lb/</link>
            <pubDate>Sat, 06 Apr 2019 15:13:43 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/github_lb/</guid>
            <description>preface github 去年8月份开源了自家的 load balance策略, 主要解决ecmp情况下增删服务器导致的in-progress连接失败的情况: 新增proxy加上去之后, router将正在处理的连接转移到新的proxy上, 但是新的proxy并不清楚这些连接信息(应该交给那个服务器处理), 所以只能失败.
detail github使用了load balancer拆分成两层, 一层是 director, 负责和router层的ecmp协议打交道, 另一层是代理层. 这样的做法是怎么解决之前的问题的呢?
首先, director会维护一份 有primary/secondary 的 跳转表, status可以区分为 active、filling、draining 三种状态. 通过 Rendezvous hashing 算法将proxy映射到row上: 每个proxy + row number 进行hash运算选择前两个分别作为primary、secondary proxy.
 正常流程:  请求过来的时候, 通过对连接进行hash, 映射到指定的row, 将请求转发给 primary proxy proxy 检查本地的本地状态, 只接受新创建的连接/本地已经创建的连接.   添加  director 重新进行hash运算, 更新 跳转表, 因为使用了Rendezvous hashing, 会发现, 只有部分行发生了变化, 一种是新增的proxy成为了 secondary, 这种情况下, 依然是active状态, 另一种是新增的proxy成为了 primary, row status变成了 filling, 原有的请求到来的时候, 被分配到 primary proxy primary proxy 发现本地不存在这个连接, 并且也不是新创建的连接, 转发给 secondary proxy(也就是这个连接的owner)   删除  在跳转表中, director将 primary == 要删除的server的row的状态重置为 Draining, 同时, 将primary和secondary进行转换 原有的请求到来的时候, 被分配到 primary proxy primary proxy 发现本地不存在这个连接, 并且也不是新创建的连接, 转发给 secondary proxy(也就是这个连接的owner)    通过上面的流程发现, 因为 primary/secondary 的设计, 实现了 优雅关闭的特性.</description>
            <content type="html"><![CDATA[<h2 id="preface">preface</h2>
<p>github 去年8月份开源了自家的 load balance策略, 主要解决ecmp情况下增删服务器导致的in-progress连接失败的情况: 新增proxy加上去之后, router将正在处理的连接转移到新的proxy上, 但是新的proxy并不清楚这些连接信息(应该交给那个服务器处理), 所以只能失败.</p>
<h2 id="detail">detail</h2>
<p>github使用了load balancer拆分成两层, 一层是 director, 负责和router层的ecmp协议打交道, 另一层是代理层. 这样的做法是怎么解决之前的问题的呢?</p>
<p>首先, director会维护一份 有primary/secondary 的 跳转表, status可以区分为 active、filling、draining 三种状态. 通过 Rendezvous hashing 算法将proxy映射到row上: 每个proxy + row number 进行hash运算选择前两个分别作为primary、secondary proxy.</p>
<p><img src="https://github.blog/wp-content/uploads/2019/02/forwarding-table-active.png" alt="跳转表"></p>
<p><img src="https://github.blog/wp-content/uploads/2019/02/glb-proxy-state-machine.png" alt="状态迁移"></p>
<ol>
<li>正常流程:
<ul>
<li>请求过来的时候, 通过对连接进行hash, 映射到指定的row, 将请求转发给 primary proxy</li>
<li>proxy 检查本地的本地状态, 只接受新创建的连接/本地已经创建的连接.</li>
</ul>
</li>
<li>添加
<ul>
<li>director 重新进行hash运算, 更新 跳转表, 因为使用了Rendezvous hashing, 会发现, 只有部分行发生了变化, 一种是新增的proxy成为了 secondary, 这种情况下, 依然是active状态, 另一种是新增的proxy成为了 primary, row status变成了 filling,</li>
<li>原有的请求到来的时候, 被分配到 primary proxy</li>
<li>primary proxy 发现本地不存在这个连接, 并且也不是新创建的连接, 转发给 secondary proxy(也就是这个连接的owner)</li>
</ul>
</li>
<li>删除
<ul>
<li>在跳转表中, director将 primary == 要删除的server的row的状态重置为 Draining, 同时, 将primary和secondary进行转换</li>
<li>原有的请求到来的时候, 被分配到 primary proxy</li>
<li>primary proxy 发现本地不存在这个连接, 并且也不是新创建的连接, 转发给 secondary proxy(也就是这个连接的owner)</li>
</ul>
</li>
</ol>
<p>通过上面的流程发现, 因为 primary/secondary 的设计, 实现了 优雅关闭的特性. 但是缺点也很明显: 每次只能有一个变化发生, 但是够用的.</p>
<h2 id="other">other</h2>
<ol>
<li>dpdk的使用与优化, 通过dpdk绕过内核, 直接在用户层和nic打交道</li>
<li>通过gue(generic udp encapsulation) 将secondary ip放到了udp 当中, 实现二次分发; 除此之外, proxy 还可以将内容直接分发给客户端, 绕过director.</li>
<li>没有采用ipip策略, 而是使用了gue, 因为ipip无法放入其他server metadata.而且, 如果传递了未知ip选项, 处理速度会从几百万下降到几千; 而且大部分datacenter并不支持</li>
</ol>
<h2 id="reference">reference</h2>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1076654">数据中心内负载均衡-ECMP的使用分析</a></li>
<li><a href="https://github.blog/2018-08-08-glb-director-open-source-load-balancer/">官方blog</a></li>
<li><a href="https://github.com/github/glb-director">github开源地址</a></li>
<li><a href="https://lwn.net/Articles/614348/">foo over udp</a></li>
<li><a href="https://tools.ietf.org/html/draft-ietf-nvo3-gue-05">generic udp encapsulation</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Consistent Hash Overall</title>
            <link>https://xujianhai.fun/posts/consistent-hash-overall/</link>
            <pubDate>Fri, 05 Apr 2019 17:44:49 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/consistent-hash-overall/</guid>
            <description>preface 一致性hash最早提出, 是为了解决缓存服务器高热、节点增删导致缓存数据大量变动的问题, 减少大型web应用中的部分系统失败的影响.
在缓存服务中, 如果使用 hash(key) % n 的方式, 那么, 机器节点的增删 都会导致 所有节点重新映射到新的位置, 使用了一致性hash之后, 节点的添加/删除 只影响到部分的key.
theory   先看下一致性hash的实现
 将机器/bucket 映射到 circle/环 的多个伪随机分布的point/点, 也就是说 bucket对应着ciricle上的多个point 将请求的key映射到hash 环的一个位置, 顺序遍历找到最近的一个有bucket的point 当机器节点删除的时候: 机器节点在circle/环上响应的point也会删除, 那么, 之前在 point上的数据会迁移到原来point下一个有bucket的point 当机器节点增加的时候: 机器节点/bucket在circle/环上添加映射的point/点, 将这个point 和 上一个smaller point上的资源迁移到这个point上. 因为是一个point是负责 (smaller_point, point] range的point的资源, 所以, 当在smaller_point和point中间插入一个point的时候, 就会产生分裂.    在衡量一致性算法的质量方面, 提出了四个特性:
 Balance/: 对象均衡的分布在bucket里面。 Monotonicity/单调性: 缩扩容bucket情况下, key要么在原来的bucket的位置, 要么映射到新的bucket, 而不能在原来的bucket集合内迁移, 保证均匀分布 Spread/分散性: 同一个key被分散到不同bucket的严重程度, 因为client端看到的视图是不一致的, 所以, 同一个key, 在不同的client会被映射到不同的bucket Load/负载： 是分散性在bucket的视角.</description>
            <content type="html"><![CDATA[<h2 id="preface">preface</h2>
<p>一致性hash最早提出, 是为了解决缓存服务器高热、节点增删导致缓存数据大量变动的问题, 减少大型web应用中的部分系统失败的影响.</p>
<p>在缓存服务中, 如果使用 hash(key) % n 的方式, 那么, 机器节点的增删 都会导致 所有节点重新映射到新的位置, 使用了一致性hash之后, 节点的添加/删除 只影响到部分的key.</p>
<h2 id="theory">theory</h2>
<ol>
<li>
<p>先看下一致性hash的实现</p>
<ul>
<li>将机器/bucket 映射到 circle/环 的多个伪随机分布的point/点, 也就是说 bucket对应着ciricle上的多个point</li>
<li>将请求的key映射到hash 环的一个位置, 顺序遍历找到最近的一个有bucket的point</li>
<li>当机器节点删除的时候:
机器节点在circle/环上响应的point也会删除, 那么, 之前在 point上的数据会迁移到原来point下一个有bucket的point</li>
<li>当机器节点增加的时候:
机器节点/bucket在circle/环上添加映射的point/点, 将这个point 和 上一个smaller point上的资源迁移到这个point上. 因为是一个point是负责 (smaller_point, point] range的point的资源, 所以, 当在smaller_point和point中间插入一个point的时候, 就会产生分裂.</li>
</ul>
</li>
<li>
<p>在衡量一致性算法的质量方面, 提出了四个特性:</p>
<ul>
<li>Balance/: 对象均衡的分布在bucket里面。</li>
<li>Monotonicity/单调性: 缩扩容bucket情况下, key要么在原来的bucket的位置, 要么映射到新的bucket, 而不能在原来的bucket集合内迁移, 保证均匀分布</li>
<li>Spread/分散性: 同一个key被分散到不同bucket的严重程度, 因为client端看到的视图是不一致的, 所以, 同一个key, 在不同的client会被映射到不同的bucket</li>
<li>Load/负载： 是分散性在bucket的视角.</li>
</ul>
<p>好的一致性算法, 应该满足 高balance、monotonicity、低spread、低load.</p>
</li>
</ol>
<h2 id="problem">problem</h2>
<ol>
<li>当节点太少的时候, 存在负载不均衡的现象, 也就是说, 大量数据/请求在同一台机器上, 这样, 高负载的机器crash的话, 大量缓存数据就丢失了, 为了解决这个问题, 可以引入虚拟节点的概念, 避免这个问题的产生.</li>
</ol>
<h2 id="apply">apply</h2>
<ol>
<li>适合路由层/缓存层/域名服务器场景, hash并没有很好的数据迁移的亲和性, 所以 不适合存储层.</li>
<li>redis tweproxy 一致性hash算法实现</li>
<li>mapReduce运算的负载均衡</li>
<li>去中心化的DHT文件系统</li>
<li>Dynamo系统</li>
</ol>
<h2 id="impl">impl</h2>
<ol>
<li>Ketama: 一致性hash的标准实现</li>
<li>jump consistent hash, 实现简单, 但是没有实现节点挂掉不移除节点的场景,需要进行改造, 当选择的节点不可用的情况, 需要在进行hash, 并设置hash的上限</li>
</ol>
<h2 id="improve">improve</h2>
<ol>
<li>负载有界的一致性哈希算法: 通过计算 每个服务器的平均负载*tension(压力百分比)=threshold(阈值), 当key hash 到环上找到指定的server, 需要判断server的当前load是否 超过 threshold, 超过了就找下一个. 通过这种方式, 避免了服务过载的情况. google使用这种方式节省了缓存带宽近8倍. 相关论文参考Reference.7, 实现可以参考</li>
</ol>
<h2 id="alternative">alternative</h2>
<ol>
<li>Rendezvous hash: 也叫最高随机权重hash(hrw hashing): 存储的时候, 去除候选的bucket集合, 计算每个候选bucket的值, 选择最大的进行存储; 耗时较长, 如果优化的话, rebalance的效果较差. github glb就使用了 hrw hashing.</li>
</ol>
<h2 id="reference">reference</h2>
<ol>
<li><a href="https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf">一致性hash的论文</a></li>
<li><a href="http://nyzr.njupt.edu.cn/ch/reader/create_pdf.aspx?file_no=201803014&amp;year_id=2018&amp;quarter_id=3&amp;falg=1">基于贪心算法的一致性哈希负载均衡优化</a></li>
<li><a href="http://or.nsfc.gov.cn/bitstream/00001903-5/153135/1/1000014097848.pdf">vpch(virtual partition consistent hashing)</a></li>
<li><a href="https://colobu.com/2016/03/22/jump-consistent-hash/">鸟窝blog</a></li>
<li><a href="https://colobu.com/2016/03/22/jump-consistent-hash/">jump consistent hash</a></li>
<li><a href="https://github.blog/2016-09-22-introducing-glb/">github load balancer</a></li>
<li><a href="http://ai.googleblog.com/2017/04/consistent-hashing-with-bounded-loads.html">google 负载游街一致性hash算法</a></li>
<li><a href="https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed">google 负载一致性hash blog</a></li>
<li><a href="https://en.wikipedia.org/wiki/Rendezvous_hashing">Rendezvous hashing</a></li>
<li><a href="https://pdfs.semanticscholar.org/018f/f8e9783cbd6c05d22d504d7da3692e91c92b.pdf">不错的ppt</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Raft Paper</title>
            <link>https://xujianhai.fun/posts/raft-paper/</link>
            <pubDate>Wed, 03 Apr 2019 20:31:08 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/raft-paper/</guid>
            <description>摘要 raft: 比paxos简单的共识算法, 相比paxos, 有什么特殊的点:
 共识问题简化成三个独立的问题: leader election、log replication、safety[logIndex-&amp;gt;log唯一且不可变] 不允许类似paxos的乱序提交 使用 Randomization 算法简化leader election问题. 使用term概念代替原子钟的概念  细节 1. leader选举   raft中有三个角色: leader、candidate、follower, 只有 candidate可以竞选leader, 竞选leader的时候, 竞选特点如下:
 需要得到包括自己在内一半以上的投票 Candidate的term比voter大, 在相同的情况下, candidate的logEntry的sn比voter大  这里存在问题: 当一个节点被隔离了, 会出现不断投票给自己的情况, 导致term非常大, 隔离的节点重新加入集群后, 会触发集群多次选举, 直到集群中的节点的term和被隔离的节点一样大
  raft在leader当选成功后, 会执行下面几个特点和操作:
 leader立即发送一个 no-op entry, 保证leader commit index是最新的, 使整个集群快速进入可读状态. 当follower 发送自身的commit index 比leader大, 会进行删除操作, 删除本地 leader commit index 之后的内容 leader不能提交之前term的entry, 必须当entry已经得到集群节点半数的响应, 才能将之前的entry提交    leader存活期间, 有下面几个特点</description>
            <content type="html"><![CDATA[<h2 id="摘要">摘要</h2>
<p>raft:  比paxos简单的共识算法, 相比paxos, 有什么特殊的点:</p>
<ol>
<li>共识问题简化成三个独立的问题: leader election、log replication、safety[logIndex-&gt;log唯一且不可变]</li>
<li>不允许类似paxos的乱序提交</li>
<li>使用 Randomization 算法简化leader election问题.</li>
<li>使用term概念代替原子钟的概念</li>
</ol>
<h2 id="细节">细节</h2>
<h3 id="1-leader选举">1. leader选举</h3>
<ol>
<li>
<p>raft中有三个角色: leader、candidate、follower, 只有 candidate可以竞选leader, 竞选leader的时候, 竞选特点如下:</p>
<ol>
<li>需要得到包括自己在内一半以上的投票</li>
<li>Candidate的term比voter大, 在相同的情况下, candidate的logEntry的sn比voter大</li>
</ol>
<p>这里存在问题: 当一个节点被隔离了, 会出现不断投票给自己的情况, 导致term非常大, 隔离的节点重新加入集群后, 会触发集群多次选举, 直到集群中的节点的term和被隔离的节点一样大</p>
</li>
<li>
<p>raft在leader当选成功后, 会执行下面几个特点和操作:</p>
<ol>
<li>leader立即发送一个 no-op entry, 保证leader commit index是最新的, 使整个集群快速进入可读状态.</li>
<li>当follower 发送自身的commit index 比leader大, 会进行删除操作, 删除本地 leader commit index 之后的内容</li>
<li>leader不能提交之前term的entry, 必须当entry已经得到集群节点半数的响应, 才能将之前的entry提交</li>
</ol>
</li>
<li>
<p>leader存活期间, 有下面几个特点</p>
<ol>
<li>有心跳租约的概念, 心跳租约时间内, 请求不能被处理</li>
<li>存活期间, leader 与 follower需要通过心跳维持关系</li>
<li>follower 在指定时间内收不到心跳的情况下, 会重新发起选举</li>
</ol>
</li>
</ol>
<h3 id="2-内容复制">2. 内容复制</h3>
<ol>
<li>
<p>写入的执行流程如下:</p>
<ol>
<li>client -&gt; leader: 客户端请求leader写入kv</li>
<li>leader append 本地日志</li>
<li>leader 并行发送日志给 follower</li>
<li>follower收到日志, 写入本地 commit log, 并 apply 本地的 FSM, 返回成功给 leader</li>
<li>leader收到follower超过半数以上的成功响应, 本地apply 日志到 FSM.</li>
</ol>
<p>写入流程中有几个关键的术语:</p>
<ul>
<li>commit log: 提交的日志, 所有的日志都会先写入 commit log.</li>
<li>fsm: 有限状态机, 确认达成一致的内容会写入fsm.</li>
</ul>
<p>这两者在 follower层基本上顺序执行的, 在 append commit log后立即 apply fsm. 在leader层, append commit log后, 需要并发发送请求给follower, 需要半数以上follower返回成功响应后, 才能写入 fsm.</p>
</li>
<li>
<p>读取的执行流程如下:</p>
<ol>
<li>client -&gt; leader: 客户端请求leader写入kv</li>
<li>leader 通过 lease 检查自己是否是 leader,</li>
<li>检查是leader的情况下, 检查本地 apply index 和 客户端的 index, apply index大的话, 读取本地的状态机的数据 + apply index返回</li>
<li>不是leader的情况下, 就请求leader获取最新的 apply index, 和 客户单的index 比较,  apply index 大的话, 读取本地的状态机的数据 + apply index 返回</li>
</ol>
<p>通过流程发现, 有client index的概念, 这个是server(leader/follower)返回的apply index. 除此之外, server(leader/follower)都是从本地的fsm中返回数据的, 而不像其他的方式那样，follower从leader中获取数据.</p>
</li>
</ol>
<h2 id="3异常情况考虑">3.异常情况考虑</h2>
<ol>
<li>
<p>写入的数据在半数响应后, 还没来得及响应给客户端, 就挂了
在重新leader选举后, 数据依旧保持在 raft中, 但是客户端因为之前没有收到响应, 会认为操作失败, 所以, 客户端必须得重试内容, 不然会出现 客户单认为失败，但是存储层成功的情况
那么, 客户端重试请求, 会导致重复内容存储吗? 不会, client需要给每一个请求添加一个唯一的编号, 服务端保证幂等, 一个编号的请求值处理一次</p>
</li>
<li>
<p>是否存在新选举的leader日志是不全的? 以至于leader切换内容丢失?
Leader Completeness Property 保证了 leader是有所有提交的信息的, leader刚当选成功, 会发送 no-op entry来保证自己是新的, 如果不是最新的, 会被拒绝掉.</p>
</li>
</ol>
<h2 id="其他">其他:</h2>
<ol>
<li>
<p>三种时间</p>
<p>广播时间（broadcastTime） &laquo; 选举超时时间（electionTimeout） &laquo; 平均故障间隔时间（MTBF）</p>
<p>广播时间指的是从一个服务器并行的发送 RPC请求 给集群中的其他服务器并接收响应的平均时间, 论文中指出, 关闭时间在 0.5ms 到 20ms的范围, 选举时间在 10ms 到 500ms范围.</p>
</li>
<li>
<p>日志压缩</p>
<p>为了避免日志无限增长的问题, 使用快照方式, 将之前的日志和快照删除, 只保留最新的快照</p>
</li>
<li>
<p>群成员管理</p>
<p>通过引入 joint consensus的概念, 实现了过渡期的概念, 只有 拥有c-old+c-new 的server才有可能被选举成leader, 然后c-old+c-new的日志提交后, leader在使用相同的方式提交 c-new的log entry, 避免 c-old 和 c-new的分裂.</p>
</li>
</ol>
<p>但是, etcd/tikv并没有这么做, 他们通过引入 learner 的概念解决, learner 不能进行投票, 只能够同步日志, 当日志能够跟上follower, 可以指定learner提升到 follower.</p>
<h2 id="优化实现">优化&amp;实现</h2>
<ol>
<li>pingcap raft: 实现了 batch&amp;pipeline leader append、 append log parallelly、Asynchronous Apply、Asynchronous Lease Read</li>
<li>etcd/tikv 使用learner而不是 joint consensus来保证成员更新的可靠性</li>
</ol>
<h2 id="参考信息">参考信息:</h2>
<ul>
<li>(官网)[https://raft.github.io/]</li>
<li>(论文)[https://raft.github.io/raft.pdf]</li>
<li>(可视化动图)[http://thesecretlivesofdata.com/raft/]</li>
<li>(raft大规模使用)[https://zhuanlan.zhihu.com/p/23872141]</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Tidb Debug</title>
            <link>https://xujianhai.fun/posts/tidb-debug/</link>
            <pubDate>Wed, 03 Apr 2019 10:00:02 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/tidb-debug/</guid>
            <description> 准备  参考前面的文章
编译&amp;amp;启动  获取与编译:
go get github.com/pingcap/tidb cd ~/go/src/github.com/pingcap/pd] make server 启动:
./tidb-server -p 4000:4000 \ -p 10080:10080 \ -v /etc/localtime:/etc/localtime:ro \ pingcap/tidb:latest \ --store=tikv \ --path=&amp;quot;192.168.1.101:2379&amp;quot; 客户端连接:
mysql -h 127.0.0.1 -P 4000 -u root -D test 然后出现如下提示, 就成功了
Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 2 Server version: 5.7.25-TiDB-v3.0.0-beta.1-49-g4cbe896 MySQL Community Server (Apache License 2.0) </description>
            <content type="html"><![CDATA[<ol>
<li>准备</li>
</ol>
<p>参考前面的<a href="/post/tikv-debug">文章</a></p>
<ol start="2">
<li>编译&amp;启动</li>
</ol>
<p>获取与编译:</p>
<pre><code>go get github.com/pingcap/tidb 
cd ~/go/src/github.com/pingcap/pd]
make server
</code></pre><p>启动:</p>
<pre><code>./tidb-server
  -p 4000:4000 \
  -p 10080:10080 \
  -v /etc/localtime:/etc/localtime:ro \
  pingcap/tidb:latest \
  --store=tikv \
  --path=&quot;192.168.1.101:2379&quot;
</code></pre><p>客户端连接:</p>
<pre><code>mysql -h 127.0.0.1 -P 4000 -u root -D test 
</code></pre><p>然后出现如下提示, 就成功了</p>
<pre><code>Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.7.25-TiDB-v3.0.0-beta.1-49-g4cbe896 MySQL Community Server (Apache License 2.0)
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Perf Rafka</title>
            <link>https://xujianhai.fun/posts/perf-rafka/</link>
            <pubDate>Sun, 31 Mar 2019 18:25:36 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/perf-rafka/</guid>
            <description>因为研发rafka的需要, 进行了线上的压测, 这里记录下诊断的使用.
简介 rafka是在kafka上封装的实时流数据库概念性质产品, 通过内嵌rocksdb实现了数据的get set scan接口.
诊断使用 1.top 通过top看下服务的整体情况: 内存 cpu等情况
2.查看是哪个线程 top -Hp 5608 转换16进制
printf &amp;quot;%x\n&amp;quot; 5816 16b8 jstack 5608 | grep 16b8 &amp;quot;kafka-request-handler-0&amp;quot; #65 daemon prio=5 os_prio=0 tid=0x00007fd9db72d000 nid=0x16b8 runnable [0x00007fd80c4f4000] 看线程信息, 可以知道, handler线程一直忙着处理
3.整体分布 这里使用过火焰图查看性能的分布 可以发现, 主要的性能分布在 kafka日制写入 和 rafka的rocksdb写入. 符合预期
4.jni做了什么 因为rafka使用了jni调用rocksdb实现了存储, 那么, jni下面做了什么呢?我们使用perf进行诊断. 工具 上面的诊断方式中, 主要使用了三个工具: 火焰图 java-profiler 和 perf
1.火焰图可视化 无论是 perf 还是 java-profiler, 生成的数据都是缺乏可读性的, 可以使用 flamegraph 工具生成图片. 项目地址: https://github.com/brendangregg/FlameGraph, 下载下来后建议将可执行目录放到 PATH路径.</description>
            <content type="html"><![CDATA[<p>因为研发rafka的需要, 进行了线上的压测, 这里记录下诊断的使用.</p>
<h2 id="简介">简介</h2>
<p>rafka是在kafka上封装的实时流数据库概念性质产品, 通过内嵌rocksdb实现了数据的get set scan接口.</p>
<h2 id="诊断使用">诊断使用</h2>
<h3 id="1top">1.top</h3>
<p>通过top看下服务的整体情况: 内存 cpu等情况</p>
<p><img src="/rafka-top.png" alt="rafka-top/png" title=""></p>
<h3 id="2查看是哪个线程">2.查看是哪个线程</h3>
<p>top -Hp 5608
<img src="/rafka-toph.png" alt="rafka-toph/png" title="">
转换16进制</p>
<pre><code>printf &quot;%x\n&quot; 5816
16b8
</code></pre><pre><code>jstack 5608 | grep 16b8
&quot;kafka-request-handler-0&quot; #65 daemon prio=5 os_prio=0 tid=0x00007fd9db72d000 nid=0x16b8 runnable [0x00007fd80c4f4000]
</code></pre><p>看线程信息, 可以知道, handler线程一直忙着处理</p>
<h3 id="3整体分布">3.整体分布</h3>
<p>这里使用过火焰图查看性能的分布
<img src="/rafka-falme.png" alt="rafka-falme/png" title="">
可以发现, 主要的性能分布在 kafka日制写入 和 rafka的rocksdb写入. 符合预期</p>
<h3 id="4jni做了什么">4.jni做了什么</h3>
<p>因为rafka使用了jni调用rocksdb实现了存储, 那么, jni下面做了什么呢?我们使用perf进行诊断.
<img src="/rafka-perf.png" alt="rafka-perf/png" title=""></p>
<h2 id="工具">工具</h2>
<p>上面的诊断方式中, 主要使用了三个工具: 火焰图 java-profiler 和 perf</p>
<h3 id="1火焰图可视化">1.火焰图可视化</h3>
<p>无论是 perf 还是 java-profiler,  生成的数据都是缺乏可读性的, 可以使用 flamegraph 工具生成图片. 项目地址: <a href="https://github.com/brendangregg/FlameGraph">https://github.com/brendangregg/FlameGraph</a>, 下载下来后建议将可执行目录放到 PATH路径.</p>
<h3 id="1java">1.java</h3>
<p>项目路径: <a href="https://github.com/dcapwell/lightweight-java-profiler">https://github.com/dcapwell/lightweight-java-profiler</a></p>
<p>正常情况下不需要修改, 直接执行命令 make all  生成镜像包 liblagent.so.</p>
<p>kafka使用的时候, 修改bin/kafka-server-start.sh, 将镜像放到javaagent路径.</p>
<pre><code>    export KAFKA_OPTS=&quot; -javaagent:$base_dir/jmx_prometheus_javaagent-0.11.0.jar=9990:$base_dir/kafka-agent.yaml -agentpath:/home/tiger/tools/perf/lightweight-java-profiler/build-64/liblagent.so&quot;
</code></pre><p>部署上线kafka之后, 经过压测一段时间后, kill 杀死进程, 会在启动路径下生成 traces.txt.</p>
<p>使用flame进行可视化</p>
<pre><code>stackcollapse-ljp.awk &lt; traces.txt | flamegraph.pl &gt; traces.svg 
</code></pre><p>浏览器打开生成的trace.svg 就可以了</p>
<h3 id="2-perf">2. perf</h3>
<p>机器上执行如下命令:</p>
<pre><code>perf record -e cpu-clock -g -p 5608
ctrl+c
perf script -i perf.data &amp;&gt; perf.unfold
stackcollapse-perf.pl perf.unfold &amp;&gt; perf.folded
</code></pre><p>生成火焰图:</p>
<pre><code>flamegraph.pl perf.folded &gt; perf.svg
</code></pre><p>浏览器打开生成的perf.svg 就可以了</p>
<p>perf的原理分析可以参考: <a href="https://zhuanlan.zhihu.com/p/22194920">知乎</a></p>
<h2 id="总结">总结</h2>
<p>rafka最终作为概念性产品，并没有实际产出, 因为kafka本身的硬伤, 比如isr机制. 只能够探索其他的产品了.</p>
<h2 id="其他">其他</h2>
<ul>
<li>
<p>机器配置信息参考 <a href="/post/kafka-config-online">文章</a></p>
</li>
<li>
<p>纯kafka的压测结果: [三次压测]</p>
</li>
</ul>
<pre><code>50000000 records sent, 1175585.441550 records/sec (143.50 MB/sec), 4.12 ms avg latency, 182.00 ms max latency, 1 ms 50th, 8 ms 95th, 121 ms 99th, 178 ms 99.9th.
50000000 records sent, 1202732.608486 records/sec (146.82 MB/sec), 3.59 ms avg latency, 292.00 ms max latency, 1 ms 50th, 8 ms 95th, 87 ms 99th, 255 ms 99.9th.
50000000 records sent, 1141995.751776 records/sec (139.40 MB/sec), 0.96 ms avg latency, 136.00 ms max latency, 1 ms 50th, 1 ms 95th, 16 ms 99th, 35 ms 99.9th.
</code></pre><ul>
<li>添加rocksdb后的压测结果: [三次压测]</li>
</ul>
<pre><code>50000000 records sent, 857206.535343 records/sec (104.64 MB/sec), 158.13 ms avg latency, 741.00 ms max latency, 37 ms 50th, 668 ms 95th, 720 ms 99th, 733 ms 99.9th.
50000000 records sent, 941601.853072 records/sec (114.94 MB/sec), 8.63 ms avg latency, 346.00 ms max latency, 1 ms 50th, 57 ms 95th, 144 ms 99th, 270 ms 99.9th.
50000000 records sent, 949252.937938 records/sec (115.88 MB/sec), 21.12 ms avg latency, 151.00 ms max latency, 1 ms 50th, 108 ms 95th, 126 ms 99th, 148 ms 99.9th.
</code></pre><p>对比压测数据, 以第二次压测结果计算, 941601/1202732=78%, 损失了22%的性能. 所以,rocksdb的造成的性能还是可以接受的.</p>
<ul>
<li>压测脚本:
使用kafka本身bin目录下的脚本</li>
</ul>
<pre><code>bin/kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance --topic test8 --num-records 50000000 --record-size 128  --throughput -1 --producer-props acks=1 bootstrap.servers=xxxx buffer.memory=67108864
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Kafka Config Online</title>
            <link>https://xujianhai.fun/posts/kafka-config-online/</link>
            <pubDate>Sun, 31 Mar 2019 18:11:37 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-config-online/</guid>
            <description>这里记录下经过压测后, 我们推荐的线上配置
bin/kafka-server-start.sh if [ &amp;quot;x$KAFKA_HEAP_OPTS&amp;quot; = &amp;quot;x&amp;quot; ]; then export KAFKA_HEAP_OPTS=&amp;quot;-Xmx16G -Xms16G&amp;quot; export KAFKA_JVM_PERFORMANCE_OPTS=&amp;quot;-server -XX:PermSize=48m -XX:MaxPermSize=48m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -XX:+ExitOnOutOfMemoryError&amp;quot; fi 其实, 最主要的重点是, 内存的大小设置, 官方推荐的8G内存并不比16G高效, 当然也不是越大越好, 30G的内存配置跑分并没有16G高.
server.config配置 broker.id=0 port=9098 advertised.host.name= num.network.threads=32 num.io.threads=200 socket.send.buffer.bytes=1048576 socket.receive.buffer.bytes=1048576 socket.request.max.bytes=104857600 log.dirs=xxxxxx num.partitions=40 num.recovery.threads.per.data.dir=2 log.retention.hours=48 log.retention.bytes=274877906944 log.segment.bytes=536870912 log.retention.check.interval.ms=300000 log.cleaner.enable=true zookeeper.connect=xxxxx zookeeper.session.timeout.ms=120000 zookeeper.connection.timeout.ms=60000 auto.create.topics.enable=false num.replica.fetchers=4 replica.fetch.max.bytes=4194304 message.max.bytes=4194304 queued.max.requests=200 auto.leader.rebalance.enable=false replica.lag.max.messages=4000000 inter.broker.protocol.version=0.10.1.0 log.message.format.version=0.8.2.1 delete.topic.enable=true offsets.retention.minutes=4320 controller.socket.timeout.ms=120000 advertised.listeners=SASL_PLAINTEXT://:,PLAINTEXT://: listeners=SASL_PLAINTEXT://:,PLAINTEXT://: security.inter.broker.protocol=PLAINTEXT sasl.mechanism.inter.broker.protocol=PLAIN sasl.enabled.mechanisms=PLAIN 其中, log.dirs 每个磁盘一个目录, 保证磁盘的高效.</description>
            <content type="html"><![CDATA[<p>这里记录下经过压测后, 我们推荐的线上配置</p>
<h2 id="binkafka-server-startsh">bin/kafka-server-start.sh</h2>
<pre><code>if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then
    export KAFKA_HEAP_OPTS=&quot;-Xmx16G -Xms16G&quot;
    export KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server -XX:PermSize=48m -XX:MaxPermSize=48m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -XX:+ExitOnOutOfMemoryError&quot;
fi
</code></pre><p>其实, 最主要的重点是, 内存的大小设置, 官方推荐的8G内存并不比16G高效, 当然也不是越大越好, 30G的内存配置跑分并没有16G高.</p>
<h2 id="serverconfig配置">server.config配置</h2>
<pre><code>broker.id=0
port=9098
advertised.host.name=
num.network.threads=32
num.io.threads=200
socket.send.buffer.bytes=1048576
socket.receive.buffer.bytes=1048576
socket.request.max.bytes=104857600
log.dirs=xxxxxx 
num.partitions=40
num.recovery.threads.per.data.dir=2
log.retention.hours=48
log.retention.bytes=274877906944
log.segment.bytes=536870912
log.retention.check.interval.ms=300000
log.cleaner.enable=true   
zookeeper.connect=xxxxx 
zookeeper.session.timeout.ms=120000
zookeeper.connection.timeout.ms=60000
auto.create.topics.enable=false 
num.replica.fetchers=4
replica.fetch.max.bytes=4194304
message.max.bytes=4194304
queued.max.requests=200
auto.leader.rebalance.enable=false
replica.lag.max.messages=4000000
inter.broker.protocol.version=0.10.1.0
log.message.format.version=0.8.2.1
delete.topic.enable=true
offsets.retention.minutes=4320
controller.socket.timeout.ms=120000
advertised.listeners=SASL_PLAINTEXT://:,PLAINTEXT://:
listeners=SASL_PLAINTEXT://:,PLAINTEXT://:
security.inter.broker.protocol=PLAINTEXT
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN
</code></pre><p>其中, log.dirs 每个磁盘一个目录, 保证磁盘的高效. 磁盘使用HDD就可以, 没必要使用ssd, 即使HDD, 在压测情况下也能达到 100+qps, 已经很高效了, 没必要浪费资源.</p>
<p>这里配置处理线程数目 num.io.threads=20. 单partition压测的时候, 建议设置成1, 因为partiton的写操作有锁, 频繁的等待、唤醒 和上下文切换会造成很大的代价.</p>
<h2 id="机器信息">机器信息</h2>
<p>256GB 48C 10HDD*10T</p>
]]></content>
        </item>
        
        <item>
            <title>Phx Oversall</title>
            <link>https://xujianhai.fun/posts/phx-oversall/</link>
            <pubDate>Sun, 31 Mar 2019 11:52:43 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/phx-oversall/</guid>
            <description>phxQueue的组件模块：  store service: 依赖Paxos 协议作副本同步, 实现消息存储, 通过租约保证master模型 lock service: 为scheduler 选主、consumer-queue的分配 scheduler service: 根据负载调度consumer和queue的映射关系  模型:  producer: 生产者 consumer: 消费者 store: 负责消息/cursor的存储 queue: 消息队列 paxos-group: 存储的一组集合, 集合中所有的store数据一致. 使用scheduler避免consumer的负载过高, 进行负载均衡  交互上: producer/consumer使用了rpc方式和broker进行交互, broker的消息存储使用了leveldb, 通过paxos实现内容的一致性.
逻辑层面:  queueId 和 queue的关系比较特殊, queue是一段连续的queueId, queue之间可以进行排序 topic负责部分queue consumer层通过拉取queue实现消息的拉取  特殊的地方:  在paxos上进行了大量的优化: broker层的批提交 相比传统的mq, 添加了consumer层的scheduler: 负载调度  不足:  开源的版本没有broker负载调度模型, 避免broker负载过重, 相对pulsar欠缺. 怀疑queueId/queue模型是可以切分负载的, qmq有类似的功能. leveldb可以考虑使用rocksdb, 在review代码的时候, 发现很多针对leveldb的优化. 开源的版本没有运维的方式说明. 比如 queue的切分, 部署模型  参考文档:</description>
            <content type="html"><![CDATA[<h2 id="phxqueue的组件模块">phxQueue的组件模块：</h2>
<ol>
<li>store service: 依赖Paxos 协议作副本同步, 实现消息存储, 通过租约保证master模型</li>
<li>lock service: 为scheduler 选主、consumer-queue的分配</li>
<li>scheduler service: 根据负载调度consumer和queue的映射关系</li>
</ol>
<h2 id="模型">模型:</h2>
<ul>
<li>producer: 生产者</li>
<li>consumer: 消费者</li>
<li>store: 负责消息/cursor的存储</li>
<li>queue: 消息队列</li>
<li>paxos-group: 存储的一组集合, 集合中所有的store数据一致.</li>
<li>使用scheduler避免consumer的负载过高, 进行负载均衡</li>
</ul>
<h2 id="交互上">交互上:</h2>
<p>producer/consumer使用了rpc方式和broker进行交互, broker的消息存储使用了leveldb, 通过paxos实现内容的一致性.</p>
<h2 id="逻辑层面">逻辑层面:</h2>
<ol>
<li>queueId 和 queue的关系比较特殊, queue是一段连续的queueId, queue之间可以进行排序</li>
<li>topic负责部分queue</li>
<li>consumer层通过拉取queue实现消息的拉取</li>
</ol>
<h2 id="特殊的地方">特殊的地方:</h2>
<ol>
<li>在paxos上进行了大量的优化: broker层的批提交</li>
<li>相比传统的mq, 添加了consumer层的scheduler: 负载调度</li>
</ol>
<h2 id="不足">不足:</h2>
<ol>
<li>开源的版本没有broker负载调度模型, 避免broker负载过重, 相对pulsar欠缺. 怀疑queueId/queue模型是可以切分负载的, qmq有类似的功能.</li>
<li>leveldb可以考虑使用rocksdb, 在review代码的时候, 发现很多针对leveldb的优化.</li>
<li>开源的版本没有运维的方式说明. 比如 queue的切分, 部署模型</li>
</ol>
<p>参考文档:</p>
<ol>
<li><a href="https://github.com/Tencent/phxqueue">https://github.com/Tencent/phxqueue</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1005954">https://cloud.tencent.com/developer/article/1005954</a></li>
<li><a href="https://myslide.cn/slides/1588">https://myslide.cn/slides/1588</a></li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Compact</title>
            <link>https://xujianhai.fun/posts/pulsar-compact/</link>
            <pubDate>Sat, 30 Mar 2019 23:33:11 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-compact/</guid>
            <description>Topic-Compaction 的设计比较特殊, 他不是简单的删除历史消息的操作, 相反，他却是一个移动 topic Ledger的操作, 通过将每个key的最新的消息移动到新的leader实现.
整个执行分两步执行:
 pre: 获取到最老的消息最为迭代key的起点, 最新的消息作为迭代key的终点 first round: 遍历两个key范围内的消息, 获取每个key最新的 messageId second round: 遍历两个key范围内的消息, 过滤出是 key最新的 message, 添加到新的leader.  显然, 从设计上来讲, compact ledger 的缺点还是很明显的
 topic-compaction 期间不能够有新的消息 key的重复率需要比较高, 否则没有效果  目前比较合适的场景也就是 股票场景，只关心每个key最新的消息</description>
            <content type="html"><![CDATA[<p>Topic-Compaction 的设计比较特殊, 他不是简单的删除历史消息的操作, 相反，他却是一个移动 topic  Ledger的操作, 通过将每个key的最新的消息移动到新的leader实现.</p>
<p>整个执行分两步执行:</p>
<ul>
<li>pre: 获取到最老的消息最为迭代key的起点, 最新的消息作为迭代key的终点</li>
<li>first round: 遍历两个key范围内的消息, 获取每个key最新的 messageId</li>
<li>second round: 遍历两个key范围内的消息, 过滤出是 key最新的 message, 添加到新的leader.</li>
</ul>
<p>显然, 从设计上来讲, compact ledger 的缺点还是很明显的</p>
<ol>
<li>topic-compaction 期间不能够有新的消息</li>
<li>key的重复率需要比较高, 否则没有效果</li>
</ol>
<p>目前比较合适的场景也就是 股票场景，只关心每个key最新的消息</p>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Component</title>
            <link>https://xujianhai.fun/posts/pulsar-component/</link>
            <pubDate>Sat, 30 Mar 2019 23:24:53 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-component/</guid>
            <description>通过整体的阅读, pulsar-broker内部可以区分一下几个组件:
 NamespaceService: 负责管理 NamespaceBundles. PulsarAdmin: 负责admin管理的http服务, 比如 split bundle, unload bundle. 逻辑上的Lookup service: 负责和客户端交互, 提供topic的元数据信息 Leader broker: 负责资源统计、负载迁移(触发删除bundle)  </description>
            <content type="html"><![CDATA[<p>通过整体的阅读, pulsar-broker内部可以区分一下几个组件:</p>
<ul>
<li>NamespaceService: 负责管理 NamespaceBundles.</li>
<li>PulsarAdmin: 负责admin管理的http服务, 比如 split bundle, unload bundle.</li>
<li>逻辑上的Lookup service: 负责和客户端交互, 提供topic的元数据信息</li>
<li>Leader broker: 负责资源统计、负载迁移(触发删除bundle)</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Lookup</title>
            <link>https://xujianhai.fun/posts/pulsar-lookup/</link>
            <pubDate>Sat, 30 Mar 2019 14:39:39 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-lookup/</guid>
            <description>根据前面producer交互以及consumer交互的分析, producer/consumer 会先和 Lookup服务交互, 获取topic的信息以及对应的 broker, 但是呢, 获取topic的对应的信息的时候, 如果topic没有own的情况下, topic会在当前的Lookup broker上own. 所以, Lookup broker会承担了大量的topic服务,
但是根据这篇文章, topic 是以 NamespaceBundle作为一个单位进行 资源分配的, 当资源负载过重, 并且有多个 NamespaceBundle 和 低负载的broker的时候, 会进行负载迁移.
在配置的时候, 建议使用http方式部署 lookup broker, 这样可以实现高可用.</description>
            <content type="html"><![CDATA[<p>根据前面producer<a href="/post/pulsar-broker-consumer-proto">交互</a>以及consumer<a href="/post/pulsar-broker-producer-proto">交互</a>的分析, producer/consumer 会先和 Lookup服务交互, 获取topic的信息以及对应的 broker, 但是呢, 获取topic的对应的信息的时候, 如果topic没有own的情况下, topic会在当前的Lookup broker上own. 所以, Lookup broker会承担了大量的topic服务,</p>
<p>但是根据这篇<a href="/post/pulsar-bundle">文章</a>, topic 是以 NamespaceBundle作为一个单位进行 资源分配的, 当资源负载过重, 并且有多个 NamespaceBundle 和 低负载的broker的时候, 会进行负载迁移.</p>
<p>在配置的时候, 建议使用http方式部署 lookup broker, 这样可以实现高可用.</p>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Bundle</title>
            <link>https://xujianhai.fun/posts/pulsar-bundle/</link>
            <pubDate>Fri, 29 Mar 2019 23:11:49 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-bundle/</guid>
            <description>基本概念 在pulsar中, 为了支持多租户, 有 tenant和namespace 概念. tenant和租户相关, 一个tenant一般是一个 team. namespace 可以区别不同的项目, 一个项目可以有多个topic. 在实现的过程中, 为了更好的支持资源调度的概念, 使用 NamespaceBundles 对象, NamespaceBundles 中有多个 NamespaceBundle, 根据负载, NamespaceBundle 会进行迁移和split. topic在资源上跟着bundle走. 查看NamespaceBundles定义:
public class NamespaceBundles { private final NamespaceName nsname; private final ArrayList&amp;lt;NamespaceBundle&amp;gt; bundles; private final NamespaceBundleFactory factory; private final long version; protected final long[] partitions; public static final Long FULL_LOWER_BOUND = 0x00000000L; public static final Long FULL_UPPER_BOUND = 0xffffffffL; private final NamespaceBundle fullBundle; 每个bundle负责一段range. 一开始的时候, 只有一个bound, range是 [0x00000000L,0xffffffffL], 后面随着负载过高, 会进行split, 拆分成多个bundle, 每个bundle由一个broker负责, 一个broker可能负责多个bundle.</description>
            <content type="html"><![CDATA[<h2 id="基本概念">基本概念</h2>
<p>在pulsar中, 为了支持多租户, 有 tenant和namespace 概念. tenant和租户相关, 一个tenant一般是一个 team. namespace 可以区别不同的项目, 一个项目可以有多个topic.
在实现的过程中, 为了更好的支持资源调度的概念, 使用 NamespaceBundles 对象, NamespaceBundles 中有多个 NamespaceBundle, 根据负载, NamespaceBundle 会进行迁移和split. topic在资源上跟着bundle走.
查看NamespaceBundles定义:</p>
<pre><code>public class NamespaceBundles {
    private final NamespaceName nsname;
    private final ArrayList&lt;NamespaceBundle&gt; bundles;
    private final NamespaceBundleFactory factory;
    private final long version;

    protected final long[] partitions;

    public static final Long FULL_LOWER_BOUND = 0x00000000L;
    public static final Long FULL_UPPER_BOUND = 0xffffffffL;
    private final NamespaceBundle fullBundle;
</code></pre><p>每个bundle负责一段range. 一开始的时候, 只有一个bound, range是 [0x00000000L,0xffffffffL], 后面随着负载过高, 会进行split, 拆分成多个bundle, 每个bundle由一个broker负责, 一个broker可能负责多个bundle. 当topic创建的时候, 会根据算法选择 bundle, 落地到bundle所在的broker上,<br>
那么, 使用range有什么好处呢?
通过bundle抽象topic的资源概念, 可以更好的进行调度. 使用range的方式, 更容易扩容. 但是, 目前的实现上,没办法缩容.</p>
<p>分析NamespaceBundles的代码的时候, 重点关注下面几个流程:</p>
<h2 id="bundle元数据">bundle元数据</h2>
<p>元数据存储在zk上, path: /namespace/NamespaceName/lowKey_highKey, 存储的value对象是 NamespaceEphemeralData, 也就是broker的信息.</p>
<h2 id="bundle创建">bundle创建</h2>
<p>刚创建服务的时候, 调用NamespaceService#registerBootstrapNamespaces, 会先注册 heartbeatNamespace, 在尝试注册其他Namespace, 每次获取到namespace 的 own, 就需要加载namespace下所有的topic.<br>
一个Namespace只有一个owner</p>
<h2 id="bundle-split">bundle split</h2>
<p>split流程参考 NamespaceService#splitAndOwnBundle 的注释</p>
<pre><code>/**
     * 1. split the given bundle into two bundles 2. assign ownership of both the bundles to current broker 3. update
     * policies with newly created bundles into LocalZK 4. disable original bundle and refresh the cache.
     *
     * It will call splitAndOwnBundleOnceAndRetry to do the real retry work, which will retry &quot;retryTimes&quot;.
*/
</code></pre><p>split的调用路径:</p>
<ol>
<li>手动触发split
admin url path:/{tenant}/{namespace}/{bundle}/split
调用路径:</li>
</ol>
<pre><code>Namespaces#splitNamespaceBundle
-&gt; NamespaceBase#internalSplitNamespaceBundle -&gt; NamespaceService#splitAndOwnBundle -&gt; #splitAndOwnBundleOnceAndRetry 
</code></pre><p>实现上, 就是将 新分裂的 NamespaceBundle 分派给 当前的broker.</p>
<h2 id="选主">选主</h2>
<p>整个服务只有一个leader broker, 所有tenant只有一个leaderBroker.</p>
<p>pulsar broker启动的时候, 会先竞争 namespace owner, 竞选通过zk临时节点路径 /loadbalance/leader 实现. 竞争成功/失败则获取成功之后, 记录leader broker信息, 触发监听器, 进行 LoadManager#doLoadShedding 和 LoadManager#writeResourceQuotasToZooKeeper, 实现负载动态迁移.
启动参照 PulsarService#start,</p>
<pre><code>public void start() throws PulsarServerException {
    ...
    this.loadManager.set(LoadManager.create(this));
    startLeaderElectionService();
    ....
}
</code></pre><p>竞选参照 PulsarService#startLeaderElectionService 和 LeaderElectionService#start.<br>
其中, 选主后使用的功能:</p>
<ul>
<li>LoadManager#doLoadShedding: 找到高负载的broker, 将broker中的bundle迁移到 Namespace中其他的broker, 然后 http服务调用 admin路径 {tenant}/{namespace}/{bundle}/unload  实现.</li>
<li>LoadManager#writeResourceQuotasToZooKeeper: 将 namespace bundle的resource quotas写入到ZooKeeper路径: /loadbalance/resource-quota/namespace/NamespaceBundleName</li>
</ul>
<p>补充admin的处理:
url路径实现是 Namespaces#unloadNamespaceBundle, 调用链</p>
<pre><code>Namespaces#unloadNamespaceBundle -&gt; NamespaceBase#internalUnloadNamespaceBundle -&gt; NamespaceService#unloadNamespaceBundle -&gt; #unloadNamespaceBundle -&gt; OwnedBundle#handleUnloadRequest -&gt; OwnershipCache#removeOwnership
</code></pre><p>最终通过删除zk节点 /namespace/namespaceName/lowKey_highKey 实现</p>
<p>补充 admin broker是怎么选取出来的呢?
就是当前broker的地址.</p>
<p>补充 resource-quota 是怎么计算出来的呢?
计算的触发流程:</p>
<pre><code>ZooKeeperChildrenCache#reloadCache/ZooKeeperDataCache#reloadCache -&gt; SimpleLoadManangerImpl#onUpdate -&gt; #updateRanking -&gt; #updateRealtimeResourceQuota
</code></pre><p>计算的算法参考:
SimpleLoadManangerImpl#updateRealtimeResourceQuota</p>
<h2 id="bundle迁移">bundle迁移</h2>
<p>参照选主流程: LoadManager#doLoadShedding, 计算高负载的时候, 只会删除bundle的owner在zk节点上的信息.
当bundle不存在的时候, ????</p>
<h2 id="topic查找">topic查找</h2>
<p>每个broker都会创建 OwnershipCache, 通过zk的交互, 获取 brokers上own的NamespaceBundle.  查看注释信息</p>
<pre><code>/**
 * This class provides a cache service for all the service unit ownership among the brokers. It provide a cache service
 * as well as ZooKeeper read/write functions for a) lookup of a service unit ownership to a broker; b) take ownership of
 * a service unit by the local broker
 *
 *
 */
</code></pre><p>在实现上, 将自己的信息注册到zk的临时节点, 当broker挂了, own的信息就没有了, 那么, bundle就可以迁移了.</p>
<pre><code>private class OwnedServiceUnitCacheLoader implements AsyncCacheLoader&lt;String, OwnedBundle&gt; {

    public OwnershipCache(PulsarService pulsar, NamespaceBundleFactory bundleFactory) {
        this.ownerBrokerUrl = pulsar.getBrokerServiceUrl();
        this.ownerBrokerUrlTls = pulsar.getBrokerServiceUrlTls();
        this.selfOwnerInfo = new NamespaceEphemeralData(ownerBrokerUrl, ownerBrokerUrlTls,
                pulsar.getWebServiceAddress(), pulsar.getWebServiceAddressTls(), false);
        this.selfOwnerInfoDisabled = new NamespaceEphemeralData(ownerBrokerUrl, ownerBrokerUrlTls,
                pulsar.getWebServiceAddress(), pulsar.getWebServiceAddressTls(), true);
        this.bundleFactory = bundleFactory;
        this.localZkCache = pulsar.getLocalZkCache();
        this.ownershipReadOnlyCache = pulsar.getLocalZkCacheService().ownerInfoCache();
        // ownedBundlesCache contains all namespaces that are owned by the local broker
        this.ownedBundlesCache = Caffeine.newBuilder().executor(MoreExecutors.directExecutor())
                .buildAsync(new OwnedServiceUnitCacheLoader());
    }

    @SuppressWarnings(&quot;deprecation&quot;)
    @Override
    public CompletableFuture&lt;OwnedBundle&gt; asyncLoad(String namespaceBundleZNode, Executor executor) {
        if (LOG.isDebugEnabled()) {
            LOG.debug(&quot;Acquiring zk lock on namespace {}&quot;, namespaceBundleZNode);
        }

        byte[] znodeContent;
        try {
            znodeContent = jsonMapper.writeValueAsBytes(selfOwnerInfo);
        } catch (JsonProcessingException e) {
            // Failed to serialize to JSON
            return FutureUtil.failedFuture(e);
        }

        CompletableFuture&lt;OwnedBundle&gt; future = new CompletableFuture&lt;&gt;();
        ZkUtils.asyncCreateFullPathOptimistic(localZkCache.getZooKeeper(), namespaceBundleZNode, znodeContent,
                Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, (rc, path, ctx, name) -&gt; {
                    if (rc == KeeperException.Code.OK.intValue()) {
                        if (LOG.isDebugEnabled()) {
                            LOG.debug(&quot;Successfully acquired zk lock on {}&quot;, namespaceBundleZNode);
                        }
                        ownershipReadOnlyCache.invalidate(namespaceBundleZNode);
                        // ??
                        future.complete(new OwnedBundle(
                                ServiceUnitZkUtils.suBundleFromPath(namespaceBundleZNode, bundleFactory)));
                    } else {
                        // Failed to acquire lock
                        future.completeExceptionally(KeeperException.create(rc));
                    }
                }, null);

        return future;
    }
}
</code></pre><p>在上面的实现中, 有两个有趣的缓存对象:</p>
<ul>
<li>ownershipReadOnlyCache: 查看bundle的owner信息.</li>
<li>ownedBundlesCache: 自己的 bundle own信息, 本地缓存.
两个缓存都是基于zk实现的, ownershipReadOnlyCache 是普通的实现, ownedBundlesCache做了定制化, 在asyncLoad函数中, 用自己的broker信息创建临时节点, 实现namespace self owned 的功能.</li>
</ul>
<h2 id="topic的创建落地broker">topic的创建、落地broker</h2>
<ul>
<li>topic创建的时候是不会绑定到NamespaceBundle上的,  会在zk上存储 /admin/partitioned-topics/namespace/domain/topicName -&gt; partitions 的json数据.</li>
<li>在Lookup交互协议的时候, 尝试获取 topic 的 own broker 地址的时候, 如果topic没有 owner broker, 会进行选举最低负载的broker. 方法可以参看
NamespaceService#findBrokerServiceUrl 和 NamespaceService#searchForCandidateBroker.</li>
</ul>
<p>在方法 NamespaceService#findBrokerServiceUrl 上, 会先选择topic要分配的bundle. 分配算法如下:</p>
<pre><code>// topic的hash算法在初始化NamespaceService的时候确定的, 目前是写死的crc32.
public NamespaceService(PulsarService pulsar) {
    this.pulsar = pulsar;
    host = pulsar.getAdvertisedAddress();
    this.config = pulsar.getConfiguration();
    this.loadManager = pulsar.getLoadManager();
    ServiceUnitZkUtils.initZK(pulsar.getLocalZkCache().getZooKeeper(), pulsar.getBrokerServiceUrl());
    this.bundleFactory = new NamespaceBundleFactory(pulsar, Hashing.crc32()); // 这里竟然分配了
    this.ownershipCache = new OwnershipCache(pulsar, bundleFactory);
    this.namespaceClients = new ConcurrentOpenHashMap&lt;&gt;();
}

// 分配Bundle的算法 
protected NamespaceBundle getBundle(long hash) {
    int idx = Arrays.binarySearch(partitions, hash);
    int lowerIdx = idx &lt; 0 ? -(idx + 2) : idx;
    return bundles.get(lowerIdx);
}
</code></pre><p>在方法 NamespaceService#searchForCandidateBroker 上, 会先选择 candidateBroker， 然后将结果返回.
candidateBroker的选择有如下几种:</p>
<ol>
<li>Heartbeat or SLAMonitor namespace, 直接获取信息中broker地址</li>
<li>(!this.loadManager.get().isCentralized() || pulsar.getLeaderElectionService().isLeader()); 会选择最低负载的broker.</li>
<li>如果2不满足, 也就是 this.loadManager.get().isCentralized() &amp;&amp;!pulsar.getLeaderElectionService().isLeader(), 如果是 authoritative mode, 就选择当前的broker, 不然选取 leader broker.</li>
<li>如果是 选择了当前的broker, 会尝试通过zookkeeper进行own bundle. 成功own的情况下, 会获取Namespace下所有的topic, 过滤获取 当前bundle的所有topic, 在本地创建信息, 建立ManagedLedger信息.</li>
<li>如果没有选择当前的broker, 就会通过 zk 获取响应的broker信息. done</li>
</ol>
<h2 id="broker挂了">broker挂了</h2>
<p>前面说到, bundle 是在一个broker, broker通过注册临时节点, 通知了其他broker. 所以, 当broker挂掉的时候, 临时节点就会注销, 那么可以根据这个实现 bundle迁移, 但是, 是怎么实现的呢?</p>
<h3 id="broker怎么办">broker怎么办?</h3>
<p>producer/consumer启动的时候, 有两种方式连接到 LOOKUP 服务, 一种是直连到 pulsar broker, 另一种是连接到 http服务上, 如果是前面一种方式, broker挂了, 整个服务就不可用了, 而且如果使用第一种方式, 一开始创建的所有topic的bundle都会在一个broker上.  如果是http服务, 那么, 如果服务后面有多个broker, 一个broker挂了的情况, 后面的topic请求过来的时候, 根据topic创建的流程走, 会落在当前的broker上.</p>
<h2 id="faq">faq</h2>
<p>一个问题, authoritative 改怎么设置呢? loadManager.get().isCentralized()? 为什么这么设计?</p>
<h2 id="总结">总结</h2>
<ol>
<li>整个服务只有一个 leader broker, 负责进行资源统计、bundle迁移.</li>
<li>NamespaceBundle对资源进行了抽象, NamespaceBundle负责多个topic, 一个Namespace下有多个NamespaceBundle</li>
<li>topic创建的时候, 会按照字符串hash + 取余的操作将topic放在一个NamespaceBundle中,方便后面资源调度</li>
<li>bundle没有merge操作</li>
<li>broker如果挂了, bundle会由当前Lookup服务的broker负责当前bundle</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Redis Overall</title>
            <link>https://xujianhai.fun/posts/redis-overall/</link>
            <pubDate>Mon, 25 Mar 2019 09:58:36 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/redis-overall/</guid>
            <description>第一次学习redis和深入redis源代码的时候, 已经是大学的光景, 当时memcached还很盛行, 新浪在csdn blog上分享关于redis实操免费的视频(当时应该果断到官网学习呀&amp;hellip;), 后来redis设计与实现一书发版, 也跟进了学习. 算下来, 从接触到现在生产使用, 已经5年光景. 这里附上自己的一些体会.
 常见的数据结构 主从复制 主从切换  参考书籍 &amp;ldquo;Redis设计与实现&amp;rdquo;</description>
            <content type="html"><![CDATA[<p>第一次学习redis和深入redis源代码的时候, 已经是大学的光景, 当时memcached还很盛行, 新浪在csdn blog上分享关于redis实操免费的视频(当时应该果断到官网学习呀&hellip;), 后来redis设计与实现一书发版, 也跟进了学习. 算下来, 从接触到现在生产使用, 已经5年光景. 这里附上自己的一些体会.</p>
<ol>
<li>常见的数据结构</li>
<li>主从复制</li>
<li>主从切换</li>
</ol>
<p>参考书籍 &ldquo;Redis设计与实现&rdquo;</p>
]]></content>
        </item>
        
        <item>
            <title>Qmq Overall</title>
            <link>https://xujianhai.fun/posts/qmq-overall/</link>
            <pubDate>Sun, 24 Mar 2019 00:42:13 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/qmq-overall/</guid>
            <description>之前, 去哪儿开源了MQ中间件, 有下面几个fature:
 延迟/定时消息 广播消息 支持按条ack消息 死信消息 Consumer/Server 随意扩容 有序消息(没有开源) 历史消息的自动备份(没有开源) 消息投递轨迹(没有开源)  arch 从qmq的架构上来讲, 如下图, 其中, meta server 使用mysql存储了 topic/consumer/producer 的关系. debug debug参照github, 不赘述
社区活跃度 最近, qmq的活跃度不错, 还支持企业的落地.
官方文章  https://mp.weixin.qq.com/s/v2JkSxxAUurXNqb28cwPmA https://mp.weixin.qq.com/s/bkHD00HD6hiIApKx_-gBBA https://mp.weixin.qq.com/s/ND1Fpu9zhpLzc87YMq3T8g  </description>
            <content type="html"><![CDATA[<p>之前, 去哪儿开源了<a href="https://github.com/qunarcorp/qmq.git">MQ中间件</a>, 有下面几个fature:</p>
<ul>
<li>延迟/定时消息</li>
<li>广播消息</li>
<li>支持按条ack消息</li>
<li>死信消息</li>
<li>Consumer/Server 随意扩容</li>
<li>有序消息(没有开源)</li>
<li>历史消息的自动备份(没有开源)</li>
<li>消息投递轨迹(没有开源)</li>
</ul>
<h2 id="arch">arch</h2>
<p>从qmq的架构上来讲, 如下图, 其中, meta server 使用mysql存储了 topic/consumer/producer 的关系.
<img src="https://raw.githubusercontent.com/qunarcorp/qmq/master/docs/images/arch1.png" alt="qmq" title="arch"></p>
<h2 id="debug">debug</h2>
<p>debug参照<a href="https://github.com/qunarcorp/qmq/blob/master/docs/cn/debug.md">github</a>, 不赘述</p>
<h2 id="社区活跃度">社区活跃度</h2>
<p>最近, qmq的活跃度不错, 还支持企业的落地.</p>
<h2 id="官方文章">官方文章</h2>
<ul>
<li><a href="https://mp.weixin.qq.com/s/v2JkSxxAUurXNqb28cwPmA">https://mp.weixin.qq.com/s/v2JkSxxAUurXNqb28cwPmA</a></li>
<li><a href="https://mp.weixin.qq.com/s/bkHD00HD6hiIApKx_-gBBA">https://mp.weixin.qq.com/s/bkHD00HD6hiIApKx_-gBBA</a></li>
<li><a href="https://mp.weixin.qq.com/s/ND1Fpu9zhpLzc87YMq3T8g">https://mp.weixin.qq.com/s/ND1Fpu9zhpLzc87YMq3T8g</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Mq Overall</title>
            <link>https://xujianhai.fun/posts/mq-overall/</link>
            <pubDate>Sun, 24 Mar 2019 00:08:23 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/mq-overall/</guid>
            <description>因为做IM的缘故, 需要使用Queue进行削谷填峰, 这里对比市面上不同的queue, 从不同角度观察Queue.
feature 对比 Kafka/RocketMQ/qmq/pulsar, 不考虑consumer/producer 客户端上的特性, 针对broker实现上对比
   Type/feature kafka rocketmq qmq pulsar     delay message false true true design   consume mode group 广播/集群 广播/group 广播/group   ack mode acc acc acc/One acc/One   ordered message partition queue queue sub-topic   history msg local local backup offload   高可用 partition rebalance Master/Slave Master/Slave bookkeeper + broker leader base zk   transaction true true false false    从定位上来说, kafka是实时流, rocketmq、qmq、pulsar 是MQ, pulsar 比较特殊, 很多feature和常规的MQ不一样, 但是实现了feature的效果, 比如, 在 消息消费模式中, pulsar 本身支持 Shared/Exclusive/Failover, 其中, Shared 和 广播模式一致, 除此之外, pulsar还支持 sub topic 模式, 将topic路由成多个 sub topic, 结合 Exclusive/Failover, 实现 group 模式.</description>
            <content type="html"><![CDATA[<p>因为做IM的缘故, 需要使用Queue进行削谷填峰, 这里对比市面上不同的queue, 从不同角度观察Queue.</p>
<h2 id="feature">feature</h2>
<p>对比 Kafka/RocketMQ/qmq/pulsar, 不考虑consumer/producer 客户端上的特性, 针对broker实现上对比</p>
<table>
<thead>
<tr>
<th align="center">Type/feature</th>
<th align="center">kafka</th>
<th align="center">rocketmq</th>
<th align="center">qmq</th>
<th align="center">pulsar</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">delay message</td>
<td align="center">false</td>
<td align="center">true</td>
<td align="center">true</td>
<td align="center">design</td>
</tr>
<tr>
<td align="center">consume mode</td>
<td align="center">group</td>
<td align="center">广播/集群</td>
<td align="center">广播/group</td>
<td align="center">广播/group</td>
</tr>
<tr>
<td align="center">ack mode</td>
<td align="center">acc</td>
<td align="center">acc</td>
<td align="center">acc/One</td>
<td align="center">acc/One</td>
</tr>
<tr>
<td align="center">ordered message</td>
<td align="center">partition</td>
<td align="center">queue</td>
<td align="center">queue</td>
<td align="center">sub-topic</td>
</tr>
<tr>
<td align="center">history msg</td>
<td align="center">local</td>
<td align="center">local</td>
<td align="center">backup</td>
<td align="center">offload</td>
</tr>
<tr>
<td align="center">高可用</td>
<td align="center">partition rebalance</td>
<td align="center">Master/Slave</td>
<td align="center">Master/Slave</td>
<td align="center">bookkeeper + broker leader base zk</td>
</tr>
<tr>
<td align="center">transaction</td>
<td align="center">true</td>
<td align="center">true</td>
<td align="center">false</td>
<td align="center">false</td>
</tr>
</tbody>
</table>
<p>从定位上来说, kafka是实时流, rocketmq、qmq、pulsar 是MQ, pulsar 比较特殊, 很多feature和常规的MQ不一样, 但是实现了feature的效果, 比如, 在 消息消费模式中, pulsar 本身支持 Shared/Exclusive/Failover, 其中, Shared 和 广播模式一致, 除此之外, pulsar还支持 sub topic 模式, 将topic路由成多个 sub topic, 结合 Exclusive/Failover, 实现 group 模式.</p>
<h2 id="基本思想">基本思想</h2>
<ul>
<li>kafka:
<ul>
<li>提供topic、partition视图, 一个topic由多个partition组成</li>
<li>broker上, 每个partition对应一个存储文件, 每个partition独立分散在不同的broker上, 每个broker上有多个partition存储</li>
<li>producer通过partition策略, 指定消息分发给topic下具体的某个partition, 然后发送给partition的leader broker.</li>
<li>consumer消费的时候, 以consumer group的方式进行订阅, broker会先执行join流程, 早期是broker内部分配, v2的版本通过client计算, consumer获取订阅的partition 列表和相应的broker地址, 连接broker地址进行pull方式消息拉取</li>
</ul>
</li>
<li>Rocketmq
<ul>
<li>提供 topic、queue视图, 一个topic有多个queue组成</li>
<li>broker上, 所有的topic/queue都写入一个 commit log文件, 通过线程异步的从commit log文件读取构建每个queue的consumer queue文件 + time index 文件.</li>
<li>producer通过selector策略, 确定发送给topic的一个queue, 然后发送给queue的broker</li>
<li>consumer以consumer group方式进行订阅, 从name server获取topic的路由信息,  值得注意的是, 订阅关系queue的分配是在consumer端维护的, consumer获取到要消费的queue, consumer会连接到broker上进行消息的拉取</li>
</ul>
</li>
<li>pulsar
<ul>
<li>提供topic、subTopic视图, 一个topic由多个subTopic组成</li>
<li>broker上, broker负责处理消息的&quot;存储&rdquo;(依赖bookkeeper实现)和分发, broker承担着类似代理的角色.</li>
<li>producer通过 partition router选择指定broker发送</li>
<li>consumer获取partition的元数据信息 ??????</li>
</ul>
</li>
<li>qmq
<ul>
<li>类似Rocketmq, 提供 topic、queue视图, 一个topic有多个queue组成,</li>
<li>类似Rocketmq, 所有的topic/queue消息存储在一个commit log, 异构构建consumer queue.</li>
<li>producer ????</li>
<li>consumer ????</li>
</ul>
</li>
</ul>
<h2 id="组件设计">组件设计</h2>
<ul>
<li>Kafka:
<ul>
<li>zookeeper: 存储元数据</li>
<li>kafka controller: 监听zk的变化, 处理 分区/副本状态, 每个kafka集群使用一个kafka controller.</li>
<li>group coordinator: 管理 consumer group状态, 比如offset管理和consumer rebalance, 每个consumer group有一个group coordinator.</li>
<li>broker: 提供文件存储</li>
</ul>
</li>
<li>Rocketmq:
<ul>
<li>namesrv负责元数据的存储, 只有简单的文件/内存实现, 不依赖zk</li>
<li>filter: 提供消息过滤服务</li>
<li>broker: 提供文件存储</li>
</ul>
</li>
<li>pulsar:
<ul>
<li>zookeeper: 存储元数据</li>
</ul>
<ul>
<li>bookkeeper: 负责存储消息、offset</li>
<li>broker: broker和bookkeeper交互, 实现 消息+offset的存储</li>
<li>存储的实现上, 依赖bookkeeper, bookkeeper能够高效的实现追加写, broker需要维护bookkeeper的topic和bookkeeper bookie的ledger信息, 映射维护在zk中.</li>
<li>function: 轻量的计算引擎</li>
</ul>
</li>
<li>qmq:
<ul>
<li>metaserver提供元数据存储, 依赖mysql</li>
<li>delay-server: 基于时间轮提供了定时任务</li>
</ul>
</li>
</ul>
<h2 id="扩容">扩容</h2>
<p>在扩容方式中, 有两种, 希望增加consumer的消费能力需要扩容 和 降低单个broker的负载需要扩容.</p>
<h3 id="consumer扩容">consumer扩容</h3>
<p>业务执行过程中, 单个consumer的执行能力是有上限的, 除了优化单个consumer的下游耗时, 比如rpc调用、本地计算等, 还有细化consumer的消费粒度, 比如partition级别的consumer, 通过并行partition的消费, 提升consumer的消费能力. 但是, partition consumer的消费能力也是有上限的, 这里分开讨论下.</p>
<ul>
<li>在kafka中, consumer最细的粒度是 partition级别的, 如果需要增加consumer的消费能力, 最终只能够扩容partition, 但是过多的partition, 在consumer发布中, 会导致频繁的rebalance.</li>
<li>在Rocketmq, consumer最细的粒度queue, 如果需要增加consumer的消费能力, 只需要 producer用新的queueId向新的broker列表发送消息就可以. consumer端只需要连上新的broker即可, 发布过程无影响.</li>
<li>在pulsar中, consumer最细的粒度是宿便 topic, 如果需要增加consumer的消费能力, 只需要添加更多的sub topic, 没有类似kafka的副作用</li>
<li>qmq中, 模式偏向Rocketmq, 也是异步构建consumer queue, 所以, 增加consumer的消费能力, 也需要添加更多的queue. 没有类似kafkade 副作用</li>
</ul>
<h3 id="broker扩容">broker扩容</h3>
<p>在线上运行时, 会有很多topic/partition, 导致读写文件的OS Cache的红利降低, 这个时候, 需要扩容 broker, 降低单个broker的负载.</p>
<ul>
<li>
<p>Kafka: 纯粹的添加broker到Kafka集群并不会引起broker的负载降低, 还需要配合脚本 kafka-reassign-partitions.sh</p>
</li>
<li>
<p>Rocketmq: 官方没有提供很好的方式, 可以参照<a href="https://yq.aliyun.com/articles/636883">文章</a>, 需要自己改造</p>
</li>
<li>
<p>pulsar: 因为存储和分发分离的缘故, 所以只需要修改broker和topic的映射关系, 参见<a href="https://pulsar.apache.org/docs/en/administration-load-distribution/#load-distribution-across-pulsar-brokers">文章</a></p>
</li>
<li>
<p>qmq: qmq 使用range映射逻辑queue到物理queue的映射, 扩容中, 添加broker后, 还需要修改逻辑queue的映射, 之后qmq会完成整个流程. qmq扩容的实现中, 通过版本号区分不同时候的topic/queue -&gt; broker的映射, 并且, 为了保证消息的顺序新, 提出了控制消息的想法, consumer在消费到控制消息之后, 取出里面的版本号, 拉取相应版本的映射关系, 再进行消费.</p>
</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Qmq Consumer</title>
            <link>https://xujianhai.fun/posts/qmq-consumer/</link>
            <pubDate>Sat, 23 Mar 2019 09:34:58 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/qmq-consumer/</guid>
            <description>启动协议  consumer -&amp;gt; meta server: CLIENT_REGISTER, 注册, 获取topic原信息 consumer -&amp;gt; broker: PULL_MESSAGE: 拉取消息 consumer -&amp;gt; broker: ACK_REQUEST: 消息消费完确认  消息的消费流程 consumer consume 获取消息
 PullRegister#regist...registPullEntry...createAndSubmitPullEntry -&amp;gt; PullEntry#run...#doPull -&amp;gt; AbstractPullEntry#pull -&amp;gt; PullService#pull -&amp;gt; NettyClient#sendAsync 消息的处理
PullEntry#doPull -&amp;gt; PushConsumerImpl#push -&amp;gt; HandleTaskImpl#run-&amp;gt; HandleTask#run -&amp;gt; GeneratedListener#onMessage -&amp;gt; @QmqConsumer注解的方法 DefaultPullConsumer: 通过queue设计的生产消费模型, PlainPullEntry: 对broker进行负载均衡，从broker拉取消息 WeightLoadBalance: 按权重随机分配要消费的queue的默认策略
ack
HandleTask#run...triggerAfterCompletion -&amp;gt; PushConsumerImpl#ack -&amp;gt; AckHelper#ackWithTrace...#ack -&amp;gt; AckEntry#ack...completed -&amp;gt; AckSendQueue#ackCompleted -&amp;gt; LinkedBlockingQueue#offer -&amp;gt; AckSendQueue#sendAck...doSendAck -&amp;gt; AckService#sendAck...sendRequest -&amp;gt; NettyClient.sendAsync 处理完消息之后, 在开启auto commit的时候[ps:并没有不开启的方法], 会进行ack.</description>
            <content type="html"><![CDATA[<h2 id="启动协议">启动协议</h2>
<ol>
<li>consumer -&gt; meta server: CLIENT_REGISTER, 注册, 获取topic原信息</li>
<li>consumer -&gt; broker: PULL_MESSAGE: 拉取消息</li>
<li>consumer -&gt; broker: ACK_REQUEST: 消息消费完确认</li>
</ol>
<h2 id="消息的消费流程">消息的消费流程</h2>
<h3 id="consumer">consumer</h3>
<p><strong>consume</strong>
获取消息</p>
<pre><code>   PullRegister#regist...registPullEntry...createAndSubmitPullEntry -&gt;  PullEntry#run...#doPull -&gt; AbstractPullEntry#pull -&gt; PullService#pull -&gt; NettyClient#sendAsync
</code></pre><p>消息的处理</p>
<pre><code>PullEntry#doPull -&gt; PushConsumerImpl#push -&gt; HandleTaskImpl#run-&gt; HandleTask#run -&gt; GeneratedListener#onMessage -&gt; @QmqConsumer注解的方法
</code></pre><p>DefaultPullConsumer: 通过queue设计的生产消费模型,
PlainPullEntry: 对broker进行负载均衡，从broker拉取消息
WeightLoadBalance: 按权重随机分配要消费的queue的默认策略</p>
<p><strong>ack</strong></p>
<pre><code>HandleTask#run...triggerAfterCompletion -&gt; PushConsumerImpl#ack -&gt; AckHelper#ackWithTrace...#ack -&gt; AckEntry#ack...completed -&gt; AckSendQueue#ackCompleted -&gt; LinkedBlockingQueue#offer -&gt; AckSendQueue#sendAck...doSendAck -&gt; AckService#sendAck...sendRequest -&gt;  NettyClient.sendAsync   
</code></pre><p>处理完消息之后, 在开启auto commit的时候[ps:并没有不开启的方法], 会进行ack.</p>
<h3 id="broker">broker</h3>
<p><strong>consume</strong></p>
<pre><code>PullMessageProcessor#processRequest -&gt; PullMessageWorker#pull -&gt; PullMessageWorker#process -&gt; MessageStoreWrapper#findMessages -&gt; ConsumeQueue#pollMessages -&gt; DefaultStorage#pollMessages -&gt; ConsumerLog#selectIndexBuffer -&gt; MessageLog#getMessage -&gt; LogSegment#selectSegmentBuffer 
</code></pre><p>消息消费过程中, 因为支持按条ack的mode, 会处理unAck的消息. 其中, 会涉及到Ack的pull log, pull log如下面的ack.</p>
<p><strong>ack</strong></p>
<pre><code>AckMessageProcessor#processRequest -&gt; AckMessageWorker#process -&gt; ConsumerSequenceManager#putAckActions -&gt; ConsumerSequence#setAckSequence + DefaultStorage#putAction -&gt; ActionLog#addAction -&gt; LogSegment#append -&gt; ActionAppender#doAppend 
</code></pre><p>pullLog 是在Ack过程中进行异步构建的. 流程如下</p>
<pre><code>BuildConsumerLogEventListener#onEvent -&gt; FixedExecOrderEventBus#post -&gt;PullLogBuilder#onEvent -&gt; DefaultStorage#putPullLogs -&gt; PullLog#putPullLogMessages -&gt; LogSegment#append -&gt; PullLogMessageAppender#doAppend 
</code></pre><p>注册到ActionLogIterateService中, 通过不断读取 ActionLog 构建PullLog
很奇怪的是, 为什么consumer log记录offset, 还记录 pull log.</p>
<h2 id="扩容怎么处理">扩容怎么处理</h2>
<p>扩容的时候, consumer先消费完之前的broker的消息, 在接收到扩容指令的消息后, 拉取指定版本的路由关系, 再获取消息</p>
]]></content>
        </item>
        
        <item>
            <title>Qmq Producer</title>
            <link>https://xujianhai.fun/posts/qmq-producer/</link>
            <pubDate>Sat, 23 Mar 2019 09:34:54 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/qmq-producer/</guid>
            <description>启动协议  producer -&amp;gt; meta-server: CLIENT_REGISTER: 注册, 获取topic原信息 producer -&amp;gt; broker: SEND_MESSAGE: 发消息到指定的broker  消息的发送流程 producer MessageProducerProvider#sendMessage -&amp;gt; ProduceMessageImpl#send...#doSend...#sendSync -&amp;gt; RPCQueueSender#send...process -&amp;gt; MessageSenderGroup#send -&amp;gt; NettyConnection#send...#doSend -&amp;gt; NettyProducerClient#sendMessage -&amp;gt; NettyClient#sendSync...sendAsync 主要的代码:
 NettyClient: 负责底层连接的创建和消息的发送 NettyProducerClient: 简单的封装 BrokerLoadBalance: 负载均衡消息到集群中broker NettyConnection: queue的消息处理器 MessageSenderGroup: 消息发送和异常处理的封装 RPCQueueSender: 通过RouterManager将消息遍历投递给NettyConnection RouterManager: 路由消息应该进入哪一个队列  broker 处理流程如下
 SendMessageProcessor#processRequest -&amp;gt; SendMessageWorker#receive -&amp;gt; MessageStoreWrapper#putMessage -&amp;gt; DefaultStorage#appendMessage -&amp;gt; MessageLog#putMessage -&amp;gt; LogSegment#append -&amp;gt; RawMessageAppender#doAppend broker使用LogManager维护log, 提供最新的log进行mmap方式写入.
除了正常的消息投递, broker还会异步构建consumer queue. 相关处理流程如下:
Dispatcher#run...processLog -&amp;gt; FixedExecOrderEventBus#post -&amp;gt; BuildConsumerLogEventListener#onEvent -&amp;gt; ConsumerLog#putMessageLogOffset -&amp;gt; LogSegment#append -&amp;gt; ConsumerLogMessageAppender#doAppend 入口是在 ActionLogIterateService内部类 Dispatcher中, 通过不断读取 message Log构建consumer log.</description>
            <content type="html"><![CDATA[<h2 id="启动协议">启动协议</h2>
<ol>
<li>producer -&gt; meta-server: CLIENT_REGISTER: 注册, 获取topic原信息</li>
<li>producer -&gt; broker: SEND_MESSAGE: 发消息到指定的broker</li>
</ol>
<h2 id="消息的发送流程">消息的发送流程</h2>
<h3 id="producer">producer</h3>
<pre><code>MessageProducerProvider#sendMessage -&gt; ProduceMessageImpl#send...#doSend...#sendSync -&gt; RPCQueueSender#send...process -&gt; MessageSenderGroup#send -&gt; NettyConnection#send...#doSend -&gt; NettyProducerClient#sendMessage -&gt; NettyClient#sendSync...sendAsync
</code></pre><p>主要的代码:</p>
<ul>
<li>NettyClient: 负责底层连接的创建和消息的发送</li>
<li>NettyProducerClient: 简单的封装</li>
<li>BrokerLoadBalance: 负载均衡消息到集群中broker</li>
<li>NettyConnection: queue的消息处理器</li>
<li>MessageSenderGroup: 消息发送和异常处理的封装</li>
<li>RPCQueueSender: 通过RouterManager将消息遍历投递给NettyConnection</li>
<li>RouterManager: 路由消息应该进入哪一个队列</li>
</ul>
<h3 id="broker">broker</h3>
<p>处理流程如下</p>
<pre><code>  SendMessageProcessor#processRequest -&gt; SendMessageWorker#receive -&gt; MessageStoreWrapper#putMessage -&gt; DefaultStorage#appendMessage -&gt; MessageLog#putMessage
  -&gt; LogSegment#append -&gt; RawMessageAppender#doAppend  
</code></pre><p>broker使用LogManager维护log, 提供最新的log进行mmap方式写入.</p>
<p>除了正常的消息投递, broker还会异步构建consumer queue. 相关处理流程如下:</p>
<pre><code>Dispatcher#run...processLog  -&gt;  FixedExecOrderEventBus#post -&gt; BuildConsumerLogEventListener#onEvent -&gt; ConsumerLog#putMessageLogOffset -&gt; LogSegment#append -&gt; ConsumerLogMessageAppender#doAppend
</code></pre><p>入口是在 ActionLogIterateService内部类 Dispatcher中, 通过不断读取 message Log构建consumer log.</p>
<h2 id="扩容怎么处理">扩容怎么处理</h2>
<p>扩容过程中, 会收到发送失败的异常，然后拉取最新的路由列表, 重新发送</p>
]]></content>
        </item>
        
        <item>
            <title>Qmq Broker</title>
            <link>https://xujianhai.fun/posts/qmq-broker/</link>
            <pubDate>Sat, 23 Mar 2019 09:34:43 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/qmq-broker/</guid>
            <description>debug debug参照github, 不赘述
目录结构 启动情况的结构:
├── actionlog ├── checkpoint └── messagelog 只生产的情况下的结构:
├── actionlog ├── checkpoint │ └── message-checkpoint.00000000000000000228 ├── consumerlog │ └── order.changed │ └── 00000000000000000000 └── messagelog └── 00000000000000000000 有消费的情况下: 目录结构
├── actionlog │ └── 00000000000000000000 ├── checkpoint │ ├── action-checkpoint.00000000000000000747 │ └── message-checkpoint.00000000000000000684 ├── consumerlog │ └── order.changed │ └── 00000000000000000000 ├── messagelog │ └── 00000000000000000000 └── pulllog └── snow4young-2.local@@9eb6e93294de88e6e8fa84468e4ebaf7 └── ordercenter@order.changed └── 00000000000000000000 messageLog: 所有消息的统一的存储文件 consumerLog: 每个消息队列的索引文件, 消息内容在messageLog中, consumerLog 只是索引记录 pullLog: 记录consumer已经ack的位置</description>
            <content type="html"><![CDATA[<h2 id="debug">debug</h2>
<p>debug参照<a href="https://github.com/qunarcorp/qmq/blob/master/docs/cn/debug.md">github</a>, 不赘述</p>
<h2 id="目录结构">目录结构</h2>
<p>启动情况的结构:</p>
<pre><code>├── actionlog
├── checkpoint
└── messagelog
</code></pre><p>只生产的情况下的结构:</p>
<pre><code>├── actionlog
├── checkpoint
│   └── message-checkpoint.00000000000000000228
├── consumerlog
│   └── order.changed
│       └── 00000000000000000000
└── messagelog
    └── 00000000000000000000
</code></pre><p>有消费的情况下: 目录结构</p>
<pre><code>├── actionlog
│   └── 00000000000000000000
├── checkpoint
│   ├── action-checkpoint.00000000000000000747
│   └── message-checkpoint.00000000000000000684
├── consumerlog
│   └── order.changed
│       └── 00000000000000000000
├── messagelog
│   └── 00000000000000000000
└── pulllog
    └── snow4young-2.local@@9eb6e93294de88e6e8fa84468e4ebaf7
        └── ordercenter@order.changed
            └── 00000000000000000000
</code></pre><p>messageLog: 所有消息的统一的存储文件
consumerLog: 每个消息队列的索引文件, 消息内容在messageLog中, consumerLog 只是索引记录
pullLog: 记录consumer已经ack的位置<br>
actionLog: 所有的ack消息写入统一的文件
checkpoint: 记录日志行为, 作为快照, 机器crash之后进行恢复. 启动的时候, 会进行fix.
action-checkpoint: actionLog的快照, 会定时清理
message-checkpoint: message的快照, 会定时清理</p>
<h2 id="快照">快照</h2>
<p>SnapshotStore: action/message的快照存储层实现
CheckpointManager: action/message 快照的逻辑实现
通过实现了crash之后的恢复</p>
<h2 id="延迟消息">延迟消息</h2>
<p>基于时间轮实现的延时消息、定时消息.</p>
<h2 id="扩容">扩容</h2>
<p>扩容场景，1. consumer提高并行度 2. 机器负载高, 需要扩容broker</p>
<ul>
<li>qmq中, consumer的并行度是和物理queue绑定的, 为了提高并行度, 只需要扩容物理queue的数量, 但是为了保证扩容的有序性, qmq提出了逻辑queue的概念, qmq中, 逻辑queue通过range方式映射到物理queue的, 为了保证后期的可扩容台特性, 逻辑queue建议大一些, 然后在mysql配置range映射到实际的物理queue</li>
<li>broker扩容, 创建好broker之后, 只需要在mysql中迁移queue和broker的关系就可以.</li>
<li>无论是consumer提高并行度, 还是broker扩容, qmq都保证了有序性，通过版本号+命令消息 提供了保障.</li>
</ul>
<h2 id="高可用-机器挂了">高可用-机器挂了</h2>
<p>Master-Slave模式, 手动切换
参照官方<a href="https://github.com/qunarcorp/qmq/blob/master/docs/cn/ha.md">文章</a></p>
]]></content>
        </item>
        
        <item>
            <title>Kafka Pre Pressure</title>
            <link>https://xujianhai.fun/posts/kafka-pre-pressure/</link>
            <pubDate>Thu, 21 Mar 2019 22:23:41 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-pre-pressure/</guid>
            <description>因为对kafka做了新特性, 需要进行压测, 工作如下:
 机器选择 kafka配置 jmx 指标 prometheus grafana  详细 1.机器选择
根据 kafka官网配置, 我申请了如下的机器配置
CPU:48c RAM:256G HDD:10T*8 Nic:25G   faq: 为什么要这么大内存?
 因为kafka通过mmap读写文件, 为了保证写入的数据能够及时、快速的被读取, 依赖OS Cache, 将mmap写入的数据缓存在系统中, 这样, 读取的数据就会在内存中, 而不是磁盘上. 在高并发写+大量partition部署的时候, 为了保证及时、高效的读取, 系统的缓存需要很大. 在线上, 也经常由于 partition数量过多[申请的topic很多, 每个topic的partition混布在不同的broker上,单个broker上的partition负载过多], 会导致读取性能下降, consumer延时升高.    2.Kafka配置
参照 https://docs.confluent.io/current/kafka/deployment.html, 配置gc参数
if [ &amp;quot;x$KAFKA_HEAP_OPTS&amp;quot; = &amp;quot;x&amp;quot; ]; then export KAFKA_HEAP_OPTS=&amp;quot;-Xmx6G -Xms6G&amp;quot; export KAFKA_JVM_PERFORMANCE_OPTS=&amp;quot;-server -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true&amp;quot; fi 3.</description>
            <content type="html"><![CDATA[<p>因为对kafka做了新特性, 需要进行压测, 工作如下:</p>
<ol>
<li>机器选择</li>
<li>kafka配置</li>
<li>jmx 指标</li>
<li>prometheus</li>
<li>grafana</li>
</ol>
<h2 id="详细">详细</h2>
<p>1.机器选择</p>
<p>根据 <a href="https://docs.confluent.io/current/kafka/deployment.html">kafka官网</a>配置, 我申请了如下的机器配置</p>
<pre><code>CPU:48c RAM:256G HDD:10T*8 Nic:25G
</code></pre><ul>
<li>
<p>faq: 为什么要这么大内存?</p>
<ul>
<li>因为kafka通过mmap读写文件, 为了保证写入的数据能够及时、快速的被读取, 依赖OS Cache, 将mmap写入的数据缓存在系统中, 这样, 读取的数据就会在内存中, 而不是磁盘上. 在高并发写+大量partition部署的时候, 为了保证及时、高效的读取, 系统的缓存需要很大. 在线上, 也经常由于 partition数量过多[申请的topic很多, 每个topic的partition混布在不同的broker上,单个broker上的partition负载过多], 会导致读取性能下降, consumer延时升高.</li>
</ul>
</li>
</ul>
<p>2.Kafka配置</p>
<p>参照 <a href="https://docs.confluent.io/current/kafka/deployment.html,">https://docs.confluent.io/current/kafka/deployment.html,</a> 配置gc参数</p>
<pre><code>if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then
    export KAFKA_HEAP_OPTS=&quot;-Xmx6G -Xms6G&quot;
    export KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true&quot;
fi
</code></pre><p>3.Jmx 指标</p>
<p>在kafka的启动脚本 kafka-server-start.sh 中, 添加如下信息</p>
<pre><code>export JMX_PORT=9999
export KAFKA_OPTS=&quot; -javaagent:$base_dir/jmx_prometheus_javaagent-0.11.0.jar=9990:$base_dir/kafka-agent.yaml&quot;
</code></pre><p>将<a href="%22https://github.com/prometheus/jmx_exporter%22">依赖的jar包</a>下载到bin目录下, 同时, 在bin目录下创建配置文件 kafka-agent.yaml</p>
<pre><code>hostPort: 127.0.0.1:9999 # 这里9999为设置的jmx端口
lowercaseOutputName: true
</code></pre><p>关于魔数:</p>
<ul>
<li>9999: kafka jmx堆外暴露的端口, jmx_exporter从9999端口从kafka获取jmx数据</li>
<li>9990: jmx_exporter对外暴露的端口, prometheus从 9990端口拉取数据</li>
</ul>
<p>根据脚本启动后, 可以通过 http://localhost:9990/metrics 进行验证</p>
<p>4.Prometheus</p>
<p>根据官网安装: <a href="https://prometheus.io/docs/prometheus/latest/getting_started/">https://prometheus.io/docs/prometheus/latest/getting_started/</a>
配置文件添加如下配置</p>
<pre><code>scrape_configs:
  .....  
  - job_name: 'kafka'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
    - targets: ['KAFKA_IP:9990']
</code></pre><p>通过浏览器 http://{PROMETHEUS_IP:9090}/graph?g0.range_input=1h&amp;g0.expr=jvm_gc_collection_seconds_sum&amp;g0.tab=0 进行访问</p>
<p>5.Grafana</p>
<ul>
<li>根据官网安装: <a href="http://docs.grafana.org/installation/debian/">http://docs.grafana.org/installation/debian/</a></li>
<li>bin目录下直接启动就可以: ./grafana-server</li>
<li>记得在grafana 上添加 prometheus 数据源</li>
<li>我的grafana图是自己配置的, 可以<a href="https://grafana.com/dashboards?search=kafka">参考官网其他的grafana配置</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq Broker Consume</title>
            <link>https://xujianhai.fun/posts/rocketmq-broker-consume/</link>
            <pubDate>Wed, 13 Mar 2019 23:11:04 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-broker-consume/</guid>
            <description>流程分析-消息接收: consumer 收消息 MQConsumerInner有DefaultMQPushConsumerImpl&amp;amp;DefaultMQPullConsumerImpl两种模式, 同时区分有序性和并发两种, 这里先看 push&amp;amp;并发的实现:
DefaultMQPushConsumer#start -&amp;gt; DefaultMQPushConsumerImpl#start -&amp;gt; MQClientInstance#start -&amp;gt; RebalanceService#start -&amp;gt; #run -&amp;gt; MQClientInstance#doRebalance -&amp;gt; RebalanceImpl#doRebalance ...#updateProcessQueueTableInRebalance -&amp;gt; RebalancePushImpl#dispatchPullRequest -&amp;gt; DefaultMQPushConsumerImpl#executePullRequestImmediately -&amp;gt; PullMessageService#executePullRequestImmediately -&amp;gt; #run -&amp;gt; #pullMessage -&amp;gt; DefaultMQPushConsumerImpl#pullMessage -&amp;gt; PullAPIWrapper#pullKernelImpl -&amp;gt; MQClientAPIImpl#pullMessage -&amp;gt; ConsumeMessageConcurrentlyService#submitConsumeRequest -&amp;gt; ConsumeRequest#run -&amp;gt; MessageListenerConcurrently#consumeMessage 总结:
 push模式中, 通过rebalance定期触发消费消息 pull模式中, 需要手动拉取. 无论是push模式还是pull模式, 最终都是调用 PullAPIWrapper#pullKernelImpl 实现的  broker 接受消息 PullMessageProcessor:
 PullMessageProcessor#processRequest -&amp;gt; DefaultMessageStore#getMessage -&amp;gt; ConsumeQueue.getIndexBuffer + CommitLog#getMessage 逻辑: 先从ConsumeQueue查找索引内容, 再到 CommitLog 中读取文件内容.</description>
            <content type="html"><![CDATA[<h2 id="流程分析-消息接收">流程分析-消息接收:</h2>
<h3 id="consumer-收消息">consumer 收消息</h3>
<p>MQConsumerInner有DefaultMQPushConsumerImpl&amp;DefaultMQPullConsumerImpl两种模式,  同时区分有序性和并发两种, 这里先看 push&amp;并发的实现:</p>
<pre><code>DefaultMQPushConsumer#start -&gt; DefaultMQPushConsumerImpl#start -&gt; MQClientInstance#start -&gt; RebalanceService#start -&gt; #run -&gt; MQClientInstance#doRebalance -&gt; RebalanceImpl#doRebalance  ...#updateProcessQueueTableInRebalance -&gt; RebalancePushImpl#dispatchPullRequest -&gt; DefaultMQPushConsumerImpl#executePullRequestImmediately -&gt; PullMessageService#executePullRequestImmediately -&gt; #run -&gt; #pullMessage  -&gt; DefaultMQPushConsumerImpl#pullMessage -&gt; PullAPIWrapper#pullKernelImpl -&gt; MQClientAPIImpl#pullMessage -&gt; ConsumeMessageConcurrentlyService#submitConsumeRequest -&gt; ConsumeRequest#run -&gt; MessageListenerConcurrently#consumeMessage
</code></pre><p><strong>总结:</strong></p>
<ul>
<li>push模式中, 通过rebalance定期触发消费消息</li>
<li>pull模式中, 需要手动拉取.</li>
<li>无论是push模式还是pull模式, 最终都是调用 PullAPIWrapper#pullKernelImpl 实现的</li>
</ul>
<h3 id="broker-接受消息">broker 接受消息</h3>
<p>PullMessageProcessor:</p>
<pre><code>    PullMessageProcessor#processRequest -&gt; DefaultMessageStore#getMessage -&gt; ConsumeQueue.getIndexBuffer  + CommitLog#getMessage 
</code></pre><p><strong>逻辑:</strong>
先从ConsumeQueue查找索引内容, 再到 CommitLog 中读取文件内容.</p>
<p><strong>特点:</strong></p>
<ul>
<li>
<p>zeroCopy的支持:</p>
<p>消费消息,下发消息的时候: 通过继承Netty FileRegion的实现 ManyMessageTransfer, 最终实现 文件传输的zerocopy 传输, 和kafka类似, 也支持 []byte 的拷贝, rocketmq 通过参数配置</p>
</li>
<li>
<p>阻塞通知机制</p>
<p>当consumer消费消息的时候, 没有消息存在的情况下, broker 会阻塞起调用, 当consume queue有消息的时候, 在通知consumer. 超时时间 5000 ms.</p>
</li>
</ul>
<h3 id="faq-如何确定消息关系">faq: 如何确定消息关系</h3>
<p>实现接口 AllocateMessageQueueStrategy, 默认实现有</p>
<ul>
<li>AllocateMachineRoomNearby、</li>
<li>AllocateMessageQueueAveragely、</li>
<li>AllocateMessageQueueAveragelyByCircle、</li>
<li>AllocateMessageQueueByConfig、</li>
<li>AllocateMessageQueueByMachineRoom、</li>
<li>AllocateMessageQueueConsistentHash,</li>
<li>默认是 AllocateMessageQueueAveragely, 注意, 只有Clustering模式才需要负载均衡, 广播模式中 每个consumer订阅所有的 queue.</li>
</ul>
<h3 id="faq-rebalance">faq: rebalance</h3>
<p>RebalanceService定期执行, 调用路径如下</p>
<pre><code>RebalanceService#run -&gt; MQClientInstance#doRebalance -&gt; MQConsumerInner#doRebalance -&gt; RebalanceImpl#doRebalance...#rebalanceByTopic...#updateProcessQueueTableInRebalance...#messageQueueChanged
</code></pre><h2 id="faq-offset管理">faq: offset管理</h2>
<p>consumer offset 在broker中是存储在 本地磁盘上的, 并且只保存 每个queue最新的offset. 保存的操作不是实时的, 是通过定时任务执行的. 具体实现可以参看 BrokerController#initialize.</p>
<pre><code>            this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
                @Override
                public void run() {
                    try {
                        BrokerController.this.consumerOffsetManager.persist();
                    } catch (Throwable e) {
                        log.error(&quot;schedule persist consumerOffset error.&quot;, e);
                    }
                }
            }, 1000 * 10, this.brokerConfig.getFlushConsumerOffsetInterval(), TimeUnit.MILLISECONDS);

</code></pre><p><strong>简述:</strong></p>
<ul>
<li>rebalance通过consumer端定期调用执行的.</li>
<li>rebalance过程中, 会重新进行 queue分配, 根据比较和上次分配结果的差异, 判断是否需要处理</li>
<li>在 pull 模式中, 当发生 rebalance 的时候, 会取消不存在queue的PullTaskImpl, 同时对新的queue,添加PullTaskImpl, 参看类 MQPullConsumerScheduleService</li>
<li>在 push模式中, 触发 PullMessageService 立即执行 新分配的的queue的PullRequest</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq Broker Send</title>
            <link>https://xujianhai.fun/posts/rocketmq-broker-send/</link>
            <pubDate>Wed, 13 Mar 2019 23:07:35 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-broker-send/</guid>
            <description>流程分析-消息发送: producer 发送消息 DefaultMQProducer#send -&amp;gt; DefaultMQProducerImpl#send...#sendDefaultImpl -&amp;gt; #sendKernelImpl -&amp;gt; MQClientAPIImpl#sendMessage -&amp;gt; 不同情况讨论 分类情况:
 invokeOneway: NettyRemotingClient#invokeOneway, netty channel发送. sendMessageAsync: netty channel 异步发送, 通过回调处理结果 sendMessageSync: NettyRemotingClient#invokeSync, netty channel 阻塞等待结果  broker 发送消息 SendMessageProcessor: 负责处理消息的发送 普通消息的发送:
SendMessageProcessor#processRequest -&amp;gt; #sendMessage -&amp;gt; DefaultMessageStore#putMessage -&amp;gt; CommitLog#putMessage -&amp;gt; MappedFile#appendMessage 批量消息的发送:
SendMessageProcessor#processRequest -&amp;gt; #sendBatchMessage-&amp;gt; DefaultMessageStore#putMessages -&amp;gt; CommitLog#putMessages -&amp;gt; MappedFile#appendMessages -&amp;gt; DefaultAppendMessageCallback#doAppend 值得注意的是, 无论是 SendMessage 还是 putMessages, 最后都会执行下面两个函数
handleDiskFlush(result, putMessageResult, msg); handleHA(result, putMessageResult, msg); handleDiskFlush主要是处理磁盘刷新策略的, 主要有同步模式和异步模式, 有三种实现
 CommitRealTimeService: 异步刷盘 FlushRealTimeService: 异步刷盘 GroupCommitService: 同步刷盘  ha机制处理: 进行主从同步</description>
            <content type="html"><![CDATA[<h2 id="流程分析-消息发送">流程分析-消息发送:</h2>
<h3 id="producer-发送消息">producer 发送消息</h3>
<pre><code>DefaultMQProducer#send -&gt; DefaultMQProducerImpl#send...#sendDefaultImpl -&gt; #sendKernelImpl -&gt; MQClientAPIImpl#sendMessage -&gt; 不同情况讨论
</code></pre><p>分类情况:</p>
<ul>
<li>invokeOneway: NettyRemotingClient#invokeOneway, netty channel发送.</li>
<li>sendMessageAsync: netty channel 异步发送, 通过回调处理结果</li>
<li>sendMessageSync: NettyRemotingClient#invokeSync, netty channel 阻塞等待结果</li>
</ul>
<h3 id="broker-发送消息">broker 发送消息</h3>
<p>SendMessageProcessor: 负责处理消息的发送
普通消息的发送:</p>
<pre><code>SendMessageProcessor#processRequest -&gt; #sendMessage -&gt; DefaultMessageStore#putMessage -&gt; CommitLog#putMessage -&gt; MappedFile#appendMessage
</code></pre><p>批量消息的发送:</p>
<pre><code>SendMessageProcessor#processRequest -&gt; #sendBatchMessage-&gt; DefaultMessageStore#putMessages -&gt; CommitLog#putMessages -&gt; MappedFile#appendMessages -&gt; DefaultAppendMessageCallback#doAppend 
</code></pre><p>值得注意的是, 无论是 SendMessage 还是 putMessages, 最后都会执行下面两个函数</p>
<pre><code>handleDiskFlush(result, putMessageResult, msg);
handleHA(result, putMessageResult, msg);
</code></pre><p>handleDiskFlush主要是处理磁盘刷新策略的, 主要有同步模式和异步模式, 有三种实现</p>
<ul>
<li>CommitRealTimeService: 异步刷盘</li>
<li>FlushRealTimeService: 异步刷盘</li>
<li>GroupCommitService: 同步刷盘</li>
</ul>
<p>ha机制处理: 进行主从同步<br>
其他相关概念:
dlq: 死信队列</p>
<h3 id="问题">问题</h3>
<ol>
<li>producer 如何决定向哪个broker发送?</li>
</ol>
<ul>
<li>可以指定messageQueue, 向这个queue所在的brokerName的第一台broker addr发送</li>
<li>通过实现 MessageQueueSelector 实现消息的发送策略. 默认实现有 SelectMessageQueueByHash、SelectMessageQueueByMachineRoom、SelectMessageQueueByRandom</li>
<li>默认情况下使用 MQFaultStrategy 方式, 正常情况下是 round robin 方式发送的, 添加了 检活 容错的逻辑处理</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq Broker</title>
            <link>https://xujianhai.fun/posts/rocketmq-broker/</link>
            <pubDate>Mon, 11 Mar 2019 21:52:08 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-broker/</guid>
            <description>BrokerController分析
主要的类和作用  NettyRemotingServer: 网络协议处理 NettyRequestProcessor及其子类: 负责请求的处理 DefaultMessageStore: 存储层, 负责写入消息到CommitLog, 异步写入ConsummeQueue, 异步索引消息 CommitLog: 消息数据的写入和读取. 所有的消息都写入一个文件里. ConsumeQueue: 消费者消息索引的写入和读取, ConsumeQueue对应着一个文件, 是 topic+queueId 维度的, 只有索引, 没有实际的数据. ReputMessageService: 负责从CommitLog中读取数据, 调用Dispatcher链进行处理, ConsummeQueue 和 IndexFile 的构建就是通过注册到Dispatcher链实现的 MappedFile: 通过mapp方式实现的文件的读写 HAService: 主从同步的处理 ManyMessageTransfer/OneMessageTransfer/QueryMessageTransfer: 通过继承Netty FileRegion的方式实现zero copy BrokerOuterAPI: broker 和 namesrv 交互的组件 ConsumerOffsetManager: offset的管理  特点  所有的topic的消息都是放在一个 CommitLog 里面的 ConsumeQueue 的构建是异步进行的, 是 topic+queueId 维度的. 消费支持 zero copy. 可以自定义MessageStore的实现  </description>
            <content type="html"><![CDATA[<p>BrokerController分析</p>
<h2 id="主要的类和作用">主要的类和作用</h2>
<ul>
<li>NettyRemotingServer: 网络协议处理</li>
<li>NettyRequestProcessor及其子类: 负责请求的处理</li>
<li>DefaultMessageStore: 存储层, 负责写入消息到CommitLog, 异步写入ConsummeQueue, 异步索引消息</li>
<li>CommitLog: 消息数据的写入和读取. 所有的消息都写入一个文件里.</li>
<li>ConsumeQueue: 消费者消息索引的写入和读取, ConsumeQueue对应着一个文件, 是 topic+queueId 维度的, 只有索引, 没有实际的数据.</li>
<li>ReputMessageService: 负责从CommitLog中读取数据, 调用Dispatcher链进行处理, ConsummeQueue 和 IndexFile 的构建就是通过注册到Dispatcher链实现的</li>
<li>MappedFile: 通过mapp方式实现的文件的读写</li>
<li>HAService: 主从同步的处理</li>
<li>ManyMessageTransfer/OneMessageTransfer/QueryMessageTransfer: 通过继承Netty FileRegion的方式实现zero copy</li>
<li>BrokerOuterAPI: broker 和 namesrv 交互的组件</li>
<li>ConsumerOffsetManager: offset的管理</li>
</ul>
<h2 id="特点">特点</h2>
<ol>
<li>所有的topic的消息都是放在一个 CommitLog 里面的</li>
<li>ConsumeQueue 的构建是异步进行的, 是 topic+queueId 维度的.</li>
<li>消费支持 zero copy.</li>
<li>可以自定义MessageStore的实现</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq Namesrv</title>
            <link>https://xujianhai.fun/posts/rocketmq-namesrv/</link>
            <pubDate>Mon, 11 Mar 2019 21:20:32 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-namesrv/</guid>
            <description>namesrv 负责 broker和topic的管理, 实现比较简单. 数据存放在内存中, 也支持配置的文件存储方式支持.
namesrv 的主要模块有 RouteInfoManager 和 KVConfigManager, 就是分别维护 broker/topic 管理和配置的管理.
因为太简单了, 不赘述.</description>
            <content type="html"><![CDATA[<p>namesrv 负责 broker和topic的管理, 实现比较简单. 数据存放在内存中, 也支持配置的文件存储方式支持.</p>
<p>namesrv 的主要模块有 RouteInfoManager 和 KVConfigManager, 就是分别维护 broker/topic  管理和配置的管理.</p>
<p>因为太简单了, 不赘述.</p>
]]></content>
        </item>
        
        <item>
            <title>Rocketmq Debug</title>
            <link>https://xujianhai.fun/posts/rocketmq-debug/</link>
            <pubDate>Mon, 11 Mar 2019 16:40:08 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/rocketmq-debug/</guid>
            <description>这里记录下启动 rocketmq 的启动 和 消息的收发
rocketmq-namesrv 环境变量添加:
ROCKETMQ_HOME ${YOUR_DOWNLOAD_PATH}/rocketmq/distribution run 按妞运行服务, 启动成功后, 控制台会输出:
The Name Server boot success. serializeType=JSON rocketmq-broker program arguments:
-n localhost:9876 环境变量参数
ROCKETMQ_HOME ${YOUR_DOWNLOAD_PATH}/rocketmq/distribution run 按妞运行服务, 启动成功后, 控制台会输出:
The broker[${YOUR_MAC_NAME}, ${YOUR_IP}:${YOUR_PORT}] boot success. serializeType=JSON and name server is localhost:9876 rocketmq-consumer 环境变量参数
NAMESRV_ADDR=localhost:9876 run 按妞运行服务, 启动成功后, 控制台会输出:
Consumer Started. rocketmq-producer 环境变量参数
NAMESRV_ADDR=localhost:9876 run 按妞运行服务, 启动成功后, 控制台输出大量SendResult日志如下:
SendResult [sendStatus=SEND_OK, msgId=0A5E54923F6218B4AAC236E079DA0000, offsetMsgId=0A5E549200002A9F0000000000000000, messageQueue=MessageQueue [topic=TopicTest, brokerName=${YOUR_NAME}, queueId=1], queueOffset=0] ....... ....... 此时, consumer 也会输出 接收到的日志内容:</description>
            <content type="html"><![CDATA[<p>这里记录下启动 rocketmq 的启动 和 消息的收发</p>
<h2 id="rocketmq-namesrv">rocketmq-namesrv</h2>
<p>环境变量添加:</p>
<pre><code>ROCKETMQ_HOME  ${YOUR_DOWNLOAD_PATH}/rocketmq/distribution
</code></pre><p>run 按妞运行服务, 启动成功后, 控制台会输出:</p>
<pre><code>The Name Server boot success. serializeType=JSON
</code></pre><h2 id="rocketmq-broker">rocketmq-broker</h2>
<p>program arguments:</p>
<pre><code>-n localhost:9876
</code></pre><p>环境变量参数</p>
<pre><code>ROCKETMQ_HOME  ${YOUR_DOWNLOAD_PATH}/rocketmq/distribution
</code></pre><p>run 按妞运行服务, 启动成功后, 控制台会输出:</p>
<pre><code>The broker[${YOUR_MAC_NAME}, ${YOUR_IP}:${YOUR_PORT}] boot success. serializeType=JSON and name server is localhost:9876 
</code></pre><h2 id="rocketmq-consumer">rocketmq-consumer</h2>
<p>环境变量参数</p>
<pre><code>NAMESRV_ADDR=localhost:9876
</code></pre><p>run 按妞运行服务, 启动成功后, 控制台会输出:</p>
<pre><code>Consumer Started.
</code></pre><h2 id="rocketmq-producer">rocketmq-producer</h2>
<p>环境变量参数</p>
<pre><code>NAMESRV_ADDR=localhost:9876
</code></pre><p>run 按妞运行服务, 启动成功后, 控制台输出大量SendResult日志如下:</p>
<pre><code>SendResult [sendStatus=SEND_OK, msgId=0A5E54923F6218B4AAC236E079DA0000, offsetMsgId=0A5E549200002A9F0000000000000000, messageQueue=MessageQueue [topic=TopicTest, brokerName=${YOUR_NAME}, queueId=1], queueOffset=0]
.......
.......
</code></pre><p>此时, consumer 也会输出 接收到的日志内容:</p>
<pre><code>ConsumeMessageThread_1 Receive New Messages: [MessageExt [queueId=0, storeSize=178, queueOffset=0, sysFlag=0, bornTimestamp=1552290281783, bornHost=/${YOUR_BORN_IP_PORT}, storeTimestamp=1552290281784, storeHost=/${YOUR_SOTRE_IP_PORT}, msgId=0A5E549200002A9F0000000000000216, commitLogOffset=534, bodyCRC=1032136437, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic='TopicTest', flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=45, CONSUME_START_TIME=1552290399156, UNIQ_KEY=0A5E54923F6218B4AAC236E07D370003, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 51], transactionId='null'}]] 
....
....
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Kafka Proto</title>
            <link>https://xujianhai.fun/posts/kafka-proto/</link>
            <pubDate>Sat, 09 Mar 2019 21:56:40 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-proto/</guid>
            <description>debug方式启动服务端后, 通过在 KafkaApis#handle 设置断点, 可以发现启动过程中, broker 接受了三个请求:
UpdateMetadata -&amp;gt; LeaderAndIsr -&amp;gt; UpdateMetadata 但是, 后来通过调试分析发现, 只有当存在topic的时候, broker才会接收到三个请求, 没有topic的情况下, 其实, broker只会接受到一个请求.
没有topic的处理 没有topic的情况下, Kafka broker 只会接受到一个请求: UpdateMetadata. 执行流程如下:
 KafkaController#startup -&amp;gt; Startup#process -&amp;gt; KafkaController#elect -&amp;gt; #onControllerFailover -&amp;gt; #sendUpdateMetadataRequest -&amp;gt; brokerRequestBatch#addUpdateMetadataRequestForBrokers 描述: Kafka Controller 启动后, 会竞选 leader, 成功后会立即触发发送一次 UpdateMetadata.
broker接受到请求后, 处理流程:
 如果有删除的partition, 删除partition 完成topic的延时任务  有topic的处理 有topic的情况下, Kafka broker 会接受到三个请求: UpdateMetadata、LeaderAndIsr、UpdateMetadata. 通过debug, 执行流程如下:
  UpdateMetadata:
同上. KafkaController 竞选leader成功后发送请求给Broker
  LeaderAndIsr:</description>
            <content type="html"><![CDATA[<p><a href="/post/kafka-debbug">debug方式</a>启动服务端后, 通过在 KafkaApis#handle 设置断点, 可以发现启动过程中, broker 接受了三个请求:</p>
<pre><code>UpdateMetadata -&gt; LeaderAndIsr -&gt; UpdateMetadata
</code></pre><p>但是, 后来通过调试分析发现, 只有当存在topic的时候, broker才会接收到三个请求, 没有topic的情况下, 其实, broker只会接受到一个请求.</p>
<h2 id="没有topic的处理">没有topic的处理</h2>
<p>没有topic的情况下, Kafka broker 只会接受到一个请求: UpdateMetadata. 执行流程如下:</p>
<pre><code> KafkaController#startup -&gt; Startup#process -&gt; KafkaController#elect -&gt; #onControllerFailover -&gt; #sendUpdateMetadataRequest -&gt; brokerRequestBatch#addUpdateMetadataRequestForBrokers
</code></pre><p>描述: Kafka Controller 启动后, 会竞选 leader, 成功后会立即触发发送一次 UpdateMetadata.</p>
<p>broker接受到请求后, 处理流程:</p>
<ul>
<li>如果有删除的partition, 删除partition</li>
<li>完成topic的延时任务</li>
</ul>
<h2 id="有topic的处理">有topic的处理</h2>
<p>有topic的情况下, Kafka broker 会接受到三个请求: UpdateMetadata、LeaderAndIsr、UpdateMetadata. 通过debug, 执行流程如下:</p>
<ol>
<li>
<p>UpdateMetadata:</p>
<p>同上. KafkaController 竞选leader成功后发送请求给Broker</p>
</li>
<li>
<p>LeaderAndIsr:</p>
<p>启动的时候, ReplicaStateMachine 发送 LeaderAndIsr 的请求给Broker.</p>
</li>
<li>
<p>UpdateMetadata:</p>
</li>
</ol>
<pre><code>KafkaController#startup -&gt; Startup#process -&gt; KafkaController#elect -&gt;KafkaController#onControllerFailover -&gt; ReplicaStateMachine#startup -&gt; #handleStateChanges -&gt; #doHandleStateChanges +  ControllerBrokerRequestBatch#sendRequestsToBrokers -&gt; #sendRequestsToBrokers
</code></pre><p>简述: 启动的时候, ReplicaStateMachine 在发送完 LeaderAndIsr 的请求, 再继续发送 UpdateMetadata 请求.</p>
<h2 id="总结">总结</h2>
<p>仔细观察发现: 其实, 无论有没有topic, 整体的流程是一样的. 只是 <a href="/post/kafka-replica">ReplicaStateMachine</a> 会处理topic存在的情况.</p>
<h2 id="faq">faq:</h2>
<ol>
<li>为什么有topic的时候, 会多出来两个请求?</li>
</ol>
<p>根据阅读的理解, 多处理的两个请求, 只是处理topic-partition的一个行为, 以为只有一个topic、一个partition和一个broker, 所以只有两个请求. 一个是需要通知新副本关于 leader&amp;ISR 信息, 二是 通知每一个broker这个partition的的更新元数据请求</p>
<p>相关的注释内容:</p>
<blockquote>
<ul>
<li>OnlineReplica,OfflineReplica -&gt; OnlineReplica</li>
<li>&ndash;send LeaderAndIsr request with current leader and isr to the new replica and UpdateMetadata request for the</li>
<li>partition to every live broker</li>
</ul>
</blockquote>
<p>核心处理逻辑</p>
<pre><code>private def doHandleStateChanges(replicaId: Int, partitions: Seq[TopicPartition], targetState: ReplicaState,
                                   callbacks: Callbacks): Unit = {
      .......
      case OnlineReplica =&gt;
        validReplicas.foreach { replica =&gt;
          val partition = replica.topicPartition
          replicaState(replica) match {
            case NewReplica =&gt;
              val assignment = controllerContext.partitionReplicaAssignment(partition)
              if (!assignment.contains(replicaId)) {
                controllerContext.updatePartitionReplicaAssignment(partition, assignment :+ replicaId)
              }
            case _ =&gt;
              controllerContext.partitionLeadershipInfo.get(partition) match {
                case Some(leaderIsrAndControllerEpoch) =&gt;
                  controllerBrokerRequestBatch.addLeaderAndIsrRequestForBrokers(Seq(replicaId),
                    replica.topicPartition,
                    leaderIsrAndControllerEpoch,
                    controllerContext.partitionReplicaAssignment(partition), isNew = false)
                case None =&gt;
              }
          }
          logSuccessfulTransition(replicaId, partition, replicaState(replica), OnlineReplica)
          replicaState.put(replica, OnlineReplica)
        }
    
}
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Kafka Debug</title>
            <link>https://xujianhai.fun/posts/kafka-debug/</link>
            <pubDate>Sun, 03 Mar 2019 21:59:31 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-debug/</guid>
            <description> 修改配置文件: config/server.properties, 替换 log.dirs 的参数 在 build.gradle 的 core 模块添加以下依赖, 保证日志的输出  compile libs.slf4jlog4j  在 intellij 的启动参数中添加 config/server.properties 启动Debug  </description>
            <content type="html"><![CDATA[<ul>
<li>修改配置文件: config/server.properties, 替换 log.dirs 的参数</li>
<li>在 build.gradle 的 core 模块添加以下依赖, 保证日志的输出</li>
</ul>
<pre><code>compile libs.slf4jlog4j 
</code></pre><ul>
<li>在 intellij 的启动参数中添加 config/server.properties</li>
<li>启动Debug</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Kafka Producer</title>
            <link>https://xujianhai.fun/posts/kafka-producer/</link>
            <pubDate>Sun, 03 Mar 2019 21:41:08 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-producer/</guid>
            <description>kafka Producer的实现比较简单, 按照 sarama 的go语言实现, 简单说下:
AsyncProducer -&amp;gt; topicProducer -&amp;gt; partitionProducer -&amp;gt; brokerProducer -&amp;gt; Broker 无论是 哪一种Producer的实现, 都是通过 channel 实现异步的send 和 dispatcher. 为了避免 broker 元数据发生变化, 消息发送给 错误的broker, 在 启动的时候, 会获取producer leader进行消息发送, 同时如果发送失败, 就会 关闭当前的broker[客户端broker的概念, 对应着 kafka producer的一个实例].
producer 发送消息到哪一个Partition，可以自定义partition策略, 默认的有 randomPartitioner、roundRobinPartitioner、hashPartitioner.</description>
            <content type="html"><![CDATA[<p>kafka Producer的实现比较简单, 按照 sarama 的go语言实现, 简单说下:</p>
<pre><code>AsyncProducer -&gt; topicProducer -&gt; partitionProducer -&gt; brokerProducer -&gt; Broker  
</code></pre><p>无论是 哪一种Producer的实现, 都是通过 channel 实现异步的send 和 dispatcher.
为了避免 broker 元数据发生变化, 消息发送给 错误的broker, 在 启动的时候, 会获取producer leader进行消息发送, 同时如果发送失败, 就会 关闭当前的broker[客户端broker的概念, 对应着 kafka producer的一个实例].</p>
<p>producer 发送消息到哪一个Partition，可以自定义partition策略, 默认的有 randomPartitioner、roundRobinPartitioner、hashPartitioner.</p>
]]></content>
        </item>
        
        <item>
            <title>Kafka Consumer</title>
            <link>https://xujianhai.fun/posts/kafka-consumer/</link>
            <pubDate>Sun, 03 Mar 2019 21:40:53 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-consumer/</guid>
            <description>kafka Consumer的实现比较简单, 类似 kafka producer, 按照 sarama 的go语言实现, 简单说下:
Consumer -&amp;gt; partitionConsumer -&amp;gt; brokerConsumer -&amp;gt; Broker partition 会发送MetadataRequest请求到kafka Broker 获取 topic partition的leader, 创建相应的Broker对象.</description>
            <content type="html"><![CDATA[<p>kafka Consumer的实现比较简单, 类似 kafka producer, 按照 sarama 的go语言实现, 简单说下:</p>
<pre><code>Consumer -&gt; partitionConsumer -&gt; brokerConsumer -&gt; Broker  
</code></pre><p>partition 会发送MetadataRequest请求到kafka Broker 获取 topic partition的leader, 创建相应的Broker对象.</p>
]]></content>
        </item>
        
        <item>
            <title>Kafka Consumer Proto</title>
            <link>https://xujianhai.fun/posts/kafka-consumer-proto/</link>
            <pubDate>Sun, 03 Mar 2019 21:40:47 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-consumer-proto/</guid>
            <description>Debug  参照debug文章在intellij启动Kafka  2.terminal 发送请求:
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test 服务端的交互 通过分析发现交互的请求协议如下
API_VERSIONS METADATA FindCoordinator ApiVersions JoinGroup SyncGroup OffsetFetch ApiVersion ListOffsets Fetch ... Heartbeat ... OffsetCommit ...Fetch... 细节分析   API_VERSIONS
返回 broker 是否支持client version
  METADATA
获取broker信息, 以及相应的partition信息
  FindCoordinator
获取coordinator的信息
  ApiVersions
同上
  JoinGroup
添加到 group 中, 等待运算
  SyncGroup
client leader 同步运算结果, 并返回
  OffsetFetch
v0 从zk读, 后面的版本通过 GroupCoordinator#handleFetchOffsets 获取offset</description>
            <content type="html"><![CDATA[<h2 id="debug">Debug</h2>
<ol>
<li>参照<a href="/post/kafka-debug">debug文章</a>在intellij启动Kafka</li>
</ol>
<p>2.terminal 发送请求:</p>
<pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test
</code></pre><h2 id="服务端的交互">服务端的交互</h2>
<p>通过分析发现交互的请求协议如下</p>
<pre><code>API_VERSIONS METADATA FindCoordinator ApiVersions JoinGroup SyncGroup OffsetFetch ApiVersion ListOffsets Fetch ... Heartbeat ... OffsetCommit ...Fetch...
</code></pre><h2 id="细节分析">细节分析</h2>
<ol>
<li>
<p>API_VERSIONS</p>
<p>返回 broker 是否支持client version</p>
</li>
<li>
<p>METADATA</p>
<p>获取broker信息, 以及相应的partition信息</p>
</li>
<li>
<p>FindCoordinator</p>
<p>获取coordinator的信息</p>
</li>
<li>
<p>ApiVersions</p>
<p>同上</p>
</li>
<li>
<p>JoinGroup</p>
<p>添加到 group 中, 等待运算</p>
</li>
<li>
<p>SyncGroup</p>
<p>client leader 同步运算结果, 并返回</p>
</li>
<li>
<p>OffsetFetch</p>
<p>v0 从zk读, 后面的版本通过 GroupCoordinator#handleFetchOffsets 获取offset</p>
</li>
<li>
<p>ApiVersion</p>
<p>同上</p>
</li>
<li>
<p>ListOffsets</p>
<p>获取Offset</p>
</li>
<li>
<p>Fetch</p>
</li>
</ol>
<pre><code>    ReplicaManager#fetchMessages -&gt;  #readFromLocalLog -&gt; Partition#readRecords -&gt;  Log#read -&gt; LogSegment#read -&gt; 通过网络层发送文件内容
</code></pre><ol start="11">
<li>
<p>Heartbeat</p>
<p>更新member信息</p>
</li>
<li>
<p>OffsetCommit</p>
</li>
</ol>
<pre><code>    v0: 直接写入 zookeeper 
    后面的版本: GroupCoordinator#handleCommitOffsets -&gt; #doCommitOffsets -&gt; GroupManager#storeOffsets -&gt; #appendForGroup -&gt; ReplicaManager#appendRecords -&gt; #appendToLocalLog -&gt; Partition#appendRecordsToLeader -&gt; Log#appendAsLeader -&gt; #append -&gt; LogSegment#append -&gt; FileRecords#append -&gt; MemoryRecords#writeFullyTo 
</code></pre><p>除了早期的版本, 直接写入zk, 后面的版本, 是直接写入 kafka 的分布式文件上的</p>
<h2 id="faq">faq</h2>
<ol>
<li>为什么发送处理那么多次 API_VERSIONS</li>
<li>OffsetFetch和ListOffsets?</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Kafka Producer Proto</title>
            <link>https://xujianhai.fun/posts/kafka-producer-proto/</link>
            <pubDate>Sun, 03 Mar 2019 21:40:41 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-producer-proto/</guid>
            <description>Debug   参照debug文章在intellij中启动Kafka
  terminal 发送请求:
bin/kafka-console-producer.sh &amp;ndash;broker-list localhost:9092 &amp;ndash;topic test
  服务端的交互 通过分析发现交互的请求协议如下
ApiVersions Metadata ApiVersions Produce.... 细节分析   ApiVersions
返回 broker 是否支持client version
  Metadata
获取broker信息, 以及相应的partition信息
  ApiVersions
同上
  Produce 主要的流程
   ReplicaManager#appendRecords -&amp;gt; #appendToLocalLog -&amp;gt; Partition#appendRecordsToLeader -&amp;gt; Log#appendAsLeader -&amp;gt; #append -&amp;gt; LogSegment#append -&amp;gt; FileRecords#append -&amp;gt; MemoryRecords#writeFullyTo faq  为什么两次 ApiVersins ?  </description>
            <content type="html"><![CDATA[<h2 id="debug">Debug</h2>
<ol>
<li>
<p>参照<a href="/post/kafka-debug">debug文章</a>在intellij中启动Kafka</p>
</li>
<li>
<p>terminal 发送请求:</p>
<p>bin/kafka-console-producer.sh &ndash;broker-list localhost:9092 &ndash;topic test</p>
</li>
</ol>
<h2 id="服务端的交互">服务端的交互</h2>
<p>通过分析发现交互的请求协议如下</p>
<pre><code>ApiVersions Metadata ApiVersions Produce....
</code></pre><h2 id="细节分析">细节分析</h2>
<ol>
<li>
<p>ApiVersions</p>
<p>返回 broker 是否支持client version</p>
</li>
<li>
<p>Metadata</p>
<p>获取broker信息, 以及相应的partition信息</p>
</li>
<li>
<p>ApiVersions</p>
<p>同上</p>
</li>
<li>
<p>Produce
主要的流程</p>
</li>
</ol>
<pre><code>    ReplicaManager#appendRecords -&gt; #appendToLocalLog -&gt; Partition#appendRecordsToLeader -&gt; Log#appendAsLeader -&gt; #append -&gt; LogSegment#append -&gt; FileRecords#append -&gt; MemoryRecords#writeFullyTo  
</code></pre><h2 id="faq">faq</h2>
<ol>
<li>为什么两次 ApiVersins ?</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Kafka Broker</title>
            <link>https://xujianhai.fun/posts/kafka-broker/</link>
            <pubDate>Sun, 03 Mar 2019 21:40:29 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/kafka-broker/</guid>
            <description>Kafka 的服务端 主要有以下几个部分组成:
  GroupCoordinator: 负责consumer的成员管理和offset管理, 一个集群有多个 GroupCoordinator, consumer group 根据 group names被分派到 consumer group 的一个partition, 作为这个consumer group的 coordinator.
  KafkaController: 负责监听 zk 处理 topic/parititon/broker 等信息, 一个集群只有一个 KafkaController
  SocketServer: 自定义的socketServer, 1 Acceptor &amp;amp; n Processor.
  KafkaApis: 负责处理相应的client&amp;amp;follower的请求和响应
  TimingWheel: 时间轮的实现, kafka broker中 DelayedJoin DelayedHeartbeat 等延迟等待的事件的实现
  Log: 日志层的管理, 包括多个 LogSegment
   /**
 An append-only log for storing messages.  The log is a sequence of LogSegments, each with a base offset denoting the first message in the segment.</description>
            <content type="html"><![CDATA[<p>Kafka 的服务端 主要有以下几个部分组成:</p>
<ol>
<li>
<p>GroupCoordinator: 负责consumer的成员管理和offset管理, 一个集群有多个 GroupCoordinator, consumer group 根据 group names被分派到 consumer group 的一个partition, 作为这个consumer group的 coordinator.</p>
</li>
<li>
<p>KafkaController: 负责监听 zk 处理 topic/parititon/broker 等信息, 一个集群只有一个 KafkaController</p>
</li>
<li>
<p>SocketServer: 自定义的socketServer, 1 Acceptor &amp; n Processor.</p>
</li>
<li>
<p>KafkaApis: 负责处理相应的client&amp;follower的请求和响应</p>
</li>
<li>
<p>TimingWheel: 时间轮的实现, kafka broker中 DelayedJoin DelayedHeartbeat 等延迟等待的事件的实现</p>
</li>
<li>
<p>Log: 日志层的管理, 包括多个 LogSegment</p>
</li>
</ol>
<blockquote>
<p>/**</p>
<ul>
<li>An append-only log for storing messages.</li>
<li></li>
<li>The log is a sequence of LogSegments, each with a base offset denoting the first message in the segment.</li>
<li>&hellip;&hellip;.
*/</li>
</ul>
</blockquote>
<ol start="6">
<li>
<p>ReplicaStateMachine: 负责副本的管理. kafka Controller 竞选成为leader后, 会启动 ReplicaStateMachine</p>
</li>
<li>
<p>PartitionStateMachine: 负责partition的管理</p>
</li>
</ol>
<h2 id="faq">faq:</h2>
<ol>
<li>关于zero copy 是怎么回事?
Kafka实现中, consumer fetch的时候, 不是copy 文件的内容在发送出去的, 而是 只是引用 ByteBuffer 对象发送 socket, 最终调用java的 FileChannelImpl#transferTo0 的native实现.</li>
</ol>
<pre><code>JNIEXPORT jlong JNICALL
Java_sun_nio_ch_FileChannelImpl_transferTo0(JNIEnv *env, jobject this,
                                            jobject srcFDO,
                                            jlong position, jlong count,
                                            jobject dstFDO)
{
    jint srcFD = fdval(env, srcFDO);
    jint dstFD = fdval(env, dstFDO);

#if defined(__linux__)
    off64_t offset = (off64_t)position;
    jlong n = sendfile64(dstFD, srcFD, &amp;offset, (size_t)count);  // 关键
    if (n &lt; 0) {
        if (errno == EAGAIN)
            return IOS_UNAVAILABLE;
        if ((errno == EINVAL) &amp;&amp; ((ssize_t)count &gt;= 0))
            return IOS_UNSUPPORTED_CASE;
        if (errno == EINTR) {
            return IOS_INTERRUPTED;
        }
        JNU_ThrowIOExceptionWithLastError(env, &quot;Transfer failed&quot;);
        return IOS_THROWN;
    }
    return n;
#elif defined (__solaris__)
    .....
</code></pre><p>在linux系统中, 最终使用 <a href="https://linux.die.net/man/2/sendfile64">sendfile64 指令</a></p>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Broker</title>
            <link>https://xujianhai.fun/posts/pulsar-broker/</link>
            <pubDate>Sun, 03 Mar 2019 19:46:10 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-broker/</guid>
            <description>这里分析Broker的主要实现.
impl  基于Netty实现网络层, ServerCnx.java 是网络层和Broker交互的媒介 Consumer: 消费者, Producer: 生产者, 负责消息投递状态的管理,producer统计. Topic: 负责管理 Producer&amp;amp;Subscription, 以及消息的存储 Subscription: 负责管理一组consumer. 支持 Exclusive/Failover/Shared 三种订阅模型, 每个订阅有自己的cursor和Dispatcher. Dispatcher: 负责消息的分发以及消费者的管理[添加、删除] ManagedCursor: cursor管理 ManagedLedger: 负责数据的存储, 主要是 消息数据 和 cursor数据  流程   消息的生产/消费 参照 生产流程分析 和 消费流程分析；这里简单描述:
Producer send message -&amp;gt; Network -&amp;gt; Netty Server -&amp;gt; Topic -&amp;gt; Subscription -&amp;gt; Dispacher -&amp;gt; Consumer -&amp;gt; Netty Server -&amp;gt; Network -&amp;gt; Consumer   消息redelivery
后面补充！
  消息去重</description>
            <content type="html"><![CDATA[<p>这里分析Broker的主要实现.</p>
<h2 id="impl">impl</h2>
<ul>
<li>基于Netty实现网络层, ServerCnx.java 是网络层和Broker交互的媒介</li>
<li>Consumer: 消费者,</li>
<li>Producer: 生产者, 负责消息投递状态的管理,producer统计.</li>
<li>Topic: 负责管理 Producer&amp;Subscription, 以及消息的存储</li>
<li>Subscription: 负责管理一组consumer. 支持 Exclusive/Failover/Shared 三种订阅模型, 每个订阅有自己的cursor和Dispatcher.</li>
<li>Dispatcher: 负责消息的分发以及消费者的管理[添加、删除]</li>
<li>ManagedCursor: cursor管理</li>
<li>ManagedLedger: 负责数据的存储, 主要是 消息数据 和 cursor数据</li>
</ul>
<h2 id="流程">流程</h2>
<ol>
<li>
<p>消息的生产/消费
参照 <a href="/post/pulsar-broker-producer-proto">生产流程分析</a> 和 <a href="/post/pulsar-broker-consumer-proto">消费流程分析</a>；这里简单描述:</p>
<pre><code>Producer send message -&gt; Network -&gt; Netty Server -&gt; Topic -&gt; Subscription -&gt; Dispacher -&gt; Consumer -&gt; Netty Server -&gt; Network -&gt; Consumer 
</code></pre></li>
<li>
<p>消息redelivery</p>
<p>后面补充！</p>
</li>
<li>
<p>消息去重</p>
<p>通过比较 message.sequenceId 和 内存中相应Producer的 lastSequenceIdPushed 的大小, 判定 消息是否发送过. 在 Producer 中, 消息是递增的. 无论是 Broker 还是 client Producer 的实现中, sequenceId 都只在内存中进行维护, 不进行存储.</p>
</li>
<li>
<p>Cursor compaction</p>
</li>
<li>
<p>Topic compaction</p>
</li>
<li>
<p>分层存储</p>
</li>
<li>
<p>注册和高可用</p>
</li>
</ol>
<h2 id="faq">faq</h2>
<ol>
<li>topic是按照什么规则分配给 broker的呢? 又是什么时候分配的呢?</li>
</ol>
<ul>
<li>在topic创建的时候是不会分配的. 可以参照 TopicsImpl#createPartitionedTopic, 创建完topic, 会在zk上存储 /admin/partitioned-topics/namespace/domain/topicName -&gt; partitions 的json数据, 可以通过脚本实现topic的创建</li>
<li>除次之外, 在 PARTITIONED_METADATA 交互协议中, 如果topic不存在, 就作为不是partition的topic, 直接返回partition=0的响应, 不会拒绝请求, 也不会去创建topic.</li>
<li>在Lookup交互协议的时候, 尝试获取 topic 的 own broker 地址的时候, 如果topic没有 owner broker, 会进行选举最低负载的broker. 方法可以参看 NamespaceService#findBrokerServiceUrl 和 #searchForCandidateBroker.</li>
<li>值得注意的是, own broker不是根据topic粒度确定的, 而是以 NamespaceBundle 的资源粒度确定的, 多个topic在一个NamespaceBundle上, 作为整体调度, 比如broker负载过重的时候, 可以将部分NamespaceBundle迁移到其他低负载的broker上.</li>
</ul>
<ol start="2">
<li>xxx</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Broker Producer Proto</title>
            <link>https://xujianhai.fun/posts/pulsar-broker-producer-proto/</link>
            <pubDate>Sun, 03 Mar 2019 18:35:39 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-broker-producer-proto/</guid>
            <description>Debug 1.环境准备: 参照 之前的方式
2.terminal req:
 先发送produce请求 bin/pulsar-client produce my-topic &amp;ndash;messages &amp;ldquo;hello-pulsar1&amp;rdquo; bin/pulsar-client produce my-topic &amp;ndash;messages &amp;ldquo;hello-pulsar2&amp;rdquo;  服务端的交互 通过之前的调试方法, producer 和 broker 之间的交互协议:
CONNECT PARTITIONED_METADATA LOOKUP CONNECT PRODUCER SEND CLOSE_PRODUCER   Connect: lookup服务 进行权限校验
  PARTITIONED_METADATA: lookup服务获取topic的partition数量, zk查找 /admin/partitioned-topics/namespace/domain/topicName.
  LOOKUP: 获取topicName的broker地址
  CONNECT: producer连接 进行权限校验
  PRODUCER: 检查topic的权限
  SEND: 发送消息
Producer#publishMessage -&amp;gt; PersistentTopic#publishMessage -&amp;gt; ManagedLedgerImpl#asyncAddEntry -&amp;gt; OpAddEntry#initiate -&amp;gt; LedgerHandle#asyncAddEntry[bookkeeper] -&amp;gt; OpAddEntry#addComplete -&amp;gt; EntryCache#insert + PersistentTopic#addComplete[flush response] + ManagedLedgerImpl#notifyCursors[唤醒等待的读] -&amp;gt; ManagedCursorImpl#notifyEntriesAvailable -&amp;gt; ManagedLedgerImpl#asyncReadEntries -&amp;gt; 参照consumer   CLOSE_PRODUCER: 通知broker关闭该producer</description>
            <content type="html"><![CDATA[<h2 id="debug">Debug</h2>
<p>1.环境准备:
参照 <a href="/post/pulsar-broker-consumer-proto">之前</a>的方式</p>
<p>2.terminal req:</p>
<ul>
<li>先发送produce请求
bin/pulsar-client produce my-topic &ndash;messages &ldquo;hello-pulsar1&rdquo;
bin/pulsar-client produce my-topic &ndash;messages &ldquo;hello-pulsar2&rdquo;</li>
</ul>
<h2 id="服务端的交互">服务端的交互</h2>
<p>通过之前的<a href="/post/pulsar-client-proto">调试方法</a>, producer 和 broker 之间的交互协议:</p>
<pre><code>CONNECT  PARTITIONED_METADATA LOOKUP  CONNECT PRODUCER SEND CLOSE_PRODUCER 
</code></pre><ol>
<li>
<p>Connect: lookup服务 进行权限校验</p>
</li>
<li>
<p>PARTITIONED_METADATA: lookup服务获取topic的partition数量, zk查找 /admin/partitioned-topics/namespace/domain/topicName.</p>
</li>
<li>
<p>LOOKUP: 获取topicName的broker地址</p>
</li>
<li>
<p>CONNECT: producer连接 进行权限校验</p>
</li>
<li>
<p>PRODUCER: 检查topic的权限</p>
</li>
<li>
<p>SEND: 发送消息</p>
<pre><code>Producer#publishMessage -&gt; PersistentTopic#publishMessage -&gt; ManagedLedgerImpl#asyncAddEntry -&gt; OpAddEntry#initiate -&gt; LedgerHandle#asyncAddEntry[bookkeeper] -&gt; OpAddEntry#addComplete -&gt;   EntryCache#insert + PersistentTopic#addComplete[flush response] + ManagedLedgerImpl#notifyCursors[唤醒等待的读] -&gt; ManagedCursorImpl#notifyEntriesAvailable -&gt; ManagedLedgerImpl#asyncReadEntries -&gt; 参照consumer 
</code></pre></li>
<li>
<p>CLOSE_PRODUCER: 通知broker关闭该producer</p>
</li>
</ol>
<h2 id="faq">faq:</h2>
<ol>
<li>为什么执行Connect两次
<ul>
<li>前三次的客户端协议: Connect PARTITIONED_METADATA LOOKUP 是和 client.conf 中填写的brokerServiceUrl的地址进行交互的, brokerServiceUrl充当了类似 管理服务器.</li>
<li>后面三次的客户端协议, 是和topic的owner broker进行交互的. 这里因为是没有区分partition的producer, 所以只有一个CONNECT 和 PRODUCER, 不然, 有几个partition就有几个 CONNECT 和 PRODUCER.
具体的参照: ClientCnx#channelActive 和 ProducerImpl#connectionOpened</li>
</ul>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Client Producer</title>
            <link>https://xujianhai.fun/posts/pulsar-client-producer/</link>
            <pubDate>Sun, 03 Mar 2019 14:43:01 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-client-producer/</guid>
            <description>这篇文章会对 producer 进行分析.
入口 这里通过org.apache.pulsar.client.tutorial.SampleProducer#main进行分析.
producer client impl 主要有 ProducerImpl 和 PartitionedProducerImpl 两种,
 ProducerImpl  实例化后, 会创建连接, 触发CONNECT、PRODUCER command 简单的直接发送消息   PartitionedProducerImpl  实例化的时候, 遍历partition实例化producer, 并监听partition变化. 发送消息时, 通过partition router选择指定的producer进行发送    初始化 先弄清几个概念, broker有两种角色, 如下
 充当Lookup服务的broker topic owner broker    在充当Lookup服务的broker, 可以通过提供 域名 + http服务 实现元数据的管理, 也可以通过 broker的二进制协议实现, 无论是 http协议服务, 还是二进制协议服务, 都只是服务的交互方式, 真正的元数据是存在zk中的
  topic owner broker 只做一件事情, 就是负责处理topic生产的消息, 进行消息的存储和分发.
  所以, producer创建的时候, 先连接到Lookup服务, 通过Lookup服务查询到topic的owner broker, 然后连接到 topic owner broker进行消息的发送.</description>
            <content type="html"><![CDATA[<p>这篇文章会对 producer 进行分析.</p>
<h2 id="入口">入口</h2>
<p>这里通过<a href="https://github.com/apache/pulsar/blob/master/pulsar-client/src/test/java/org/apache/pulsar/client/tutorial/SampleProducer.java">org.apache.pulsar.client.tutorial.SampleProducer#main</a>进行分析.</p>
<h2 id="producer-client-impl">producer client impl</h2>
<p>主要有 ProducerImpl 和  PartitionedProducerImpl 两种,</p>
<ul>
<li>ProducerImpl
<ul>
<li>实例化后, 会创建连接, 触发CONNECT、PRODUCER command</li>
<li>简单的直接发送消息</li>
</ul>
</li>
<li>PartitionedProducerImpl
<ul>
<li>实例化的时候, 遍历partition实例化producer, 并监听partition变化.</li>
<li>发送消息时, 通过partition router选择指定的producer进行发送</li>
</ul>
</li>
</ul>
<h2 id="初始化">初始化</h2>
<p>先弄清几个概念, broker有两种角色, 如下</p>
<ol>
<li>充当Lookup服务的broker</li>
<li>topic owner broker</li>
</ol>
<ul>
<li>
<p>在充当Lookup服务的broker, 可以通过提供 域名 + http服务 实现元数据的管理, 也可以通过 broker的二进制协议实现, 无论是 http协议服务, 还是二进制协议服务, 都只是服务的交互方式, 真正的元数据是存在zk中的</p>
</li>
<li>
<p>topic owner broker 只做一件事情, 就是负责处理topic生产的消息, 进行消息的存储和分发.</p>
</li>
</ul>
<p>所以, producer创建的时候, 先连接到Lookup服务, 通过Lookup服务查询到topic的owner broker, 然后连接到 topic owner broker进行消息的发送.</p>
<h2 id="faq">faq</h2>
<ol>
<li>topic partition发生变更了怎么办?
producer无法感知, 需要重启.</li>
</ol>
<h2 id="总结">总结</h2>
<ol>
<li>无</li>
</ol>
<h2 id="不足">不足</h2>
<ol>
<li>无</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Function</title>
            <link>https://xujianhai.fun/posts/pulsar-function/</link>
            <pubDate>Sun, 03 Mar 2019 10:07:06 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-function/</guid>
            <description>不赘述, 有空补</description>
            <content type="html"><![CDATA[<p>不赘述, 有空补</p>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Proxy</title>
            <link>https://xujianhai.fun/posts/pulsar-proxy/</link>
            <pubDate>Sun, 03 Mar 2019 10:06:30 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-proxy/</guid>
            <description>proxy的作用  The Pulsar proxy is an optional gateway that you can run in front of the brokers in a Pulsar cluster. We recommend running a Pulsar proxy in cases when direction connections between clients and Pulsar brokers are either infeasible, undesirable, or both, for example when running Pulsar in a cloud environment or on Kubernetes or an analogous platform.
 根据官方的描述, proxy可以更好的解耦client和broker.
proxy impl  主要的类:   ProxyConnection: 处理proxy server的请求, ProxyConnectionPool: 后端broker client的连接池, LookupProxyHandler频繁使用 LookupProxyHandler: 负责 namespace topic的查找工作 DirectProxyHandler: broker client的处理器, 内部类 ProxyBackendHandler 负责协议相关 BrokerDiscoveryProvider: 负责从 zk上获取 topic metadata  消息收发流程  总结如下图:</description>
            <content type="html"><![CDATA[<h2 id="proxy的作用">proxy的作用</h2>
<blockquote>
<p>The Pulsar proxy is an optional gateway that you can run in front of the brokers in a Pulsar cluster. We recommend running a Pulsar proxy in cases when direction connections between clients and Pulsar brokers are either infeasible, undesirable, or both, for example when running Pulsar in a cloud environment or on Kubernetes or an analogous platform.</p>
</blockquote>
<p>根据<a href="https://pulsar.apache.org/docs/en/administration-proxy/">官方</a>的描述, proxy可以更好的解耦client和broker.</p>
<h2 id="proxy-impl">proxy impl</h2>
<ol>
<li>主要的类:</li>
</ol>
<ul>
<li>ProxyConnection: 处理proxy server的请求,</li>
<li>ProxyConnectionPool: 后端broker client的连接池, LookupProxyHandler频繁使用</li>
<li>LookupProxyHandler: 负责 namespace topic的查找工作</li>
<li>DirectProxyHandler: broker client的处理器, 内部类 ProxyBackendHandler 负责协议相关</li>
<li>BrokerDiscoveryProvider: 负责从 zk上获取 topic metadata</li>
</ul>
<ol start="2">
<li>消息收发流程</li>
</ol>
<p>总结如下图:</p>
<p>Netty Server &lt;-&gt; ProxyConnection &lt;-&gt; DirectProxyHandler &lt;-&gt; Broker Server</p>
<h2 id="总结">总结</h2>
<ol>
<li>无</li>
</ol>
<h2 id="不足">不足</h2>
<ol>
<li>ProxyConnection 和 ProxyConnectionPool 容易混淆, 其实, ProxyConnectionPool 并不是  ProxyConnection 的 pool.</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Cleint Consumer</title>
            <link>https://xujianhai.fun/posts/pulsar-client-consumer/</link>
            <pubDate>Sun, 03 Mar 2019 09:59:33 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-client-consumer/</guid>
            <description>这篇文章会对 consumer 进行分析.
入口 这里通过org.apache.pulsar.client.tutorial.SampleConsumer#main进行分析.
consumer 实现 通过debug分析可以发现, consumer底层只有 ConsumerImpl、ZeroQueueConsumerImpl、 MultiTopicsConsumerImpl 和 PatternMultiTopicsConsumerImpl 四种. 其中, ConsumerImpl是基础的实现, 其他是基于ConsumerImpl进行的封装和组合处理. consumer和broker之间的协议交互通过 ClientCnx 进行处理, 协议分析参照链接.
这里针对四种consumer实现的差异进行说明:
 ConsumerImpl  实例化最后, 会调用方法grabCnx, 触发了连接的初始化工作: 创建连接, 发送 CONNECT command消息, 发送SUBSCRIBE command消息, 发送Flow command消息, 最终实现注册和服务端消息的推送. 服务端推送的消息会放到 incomingMessages 队列. 消息通过显式的调用 internalReceive() 从incomingMessages队列中取消息进行消费.   ZeroQueueConsumerImpl  继承ConsumerImpl进行实现, 但是重写了方法 canEnqueueMessage() 和 internalReceive() 以及其他方法 (其他的过于琐碎,不进行讨论) 重写 internalReceive(), 每次调用会清空 incomingMessages, 没有及时处理的消息就没有了. 然后主动Flow command 请求一条消息, 等待获取消息进行处理 重写 canEnqueueMessage(), 当 listener 方式处理消息, 直接回调, 那么, 消息就不会丢失   MultiTopicsConsumerImpl  多个consumer的集合.</description>
            <content type="html"><![CDATA[<p>这篇文章会对 consumer 进行分析.</p>
<h2 id="入口">入口</h2>
<p>这里通过<a href="https://github.com/apache/pulsar/blob/master/pulsar-client/src/test/java/org/apache/pulsar/client/tutorial/SampleConsumer.java">org.apache.pulsar.client.tutorial.SampleConsumer#main</a>进行分析.</p>
<h2 id="consumer-实现">consumer 实现</h2>
<p>通过debug分析可以发现, consumer底层只有 ConsumerImpl、ZeroQueueConsumerImpl、 MultiTopicsConsumerImpl 和 PatternMultiTopicsConsumerImpl 四种. 其中, ConsumerImpl是基础的实现, 其他是基于ConsumerImpl进行的封装和组合处理.  consumer和broker之间的协议交互通过 ClientCnx 进行处理, 协议分析参照<a href="/post/pulsar-broker-consumer-proto">链接</a>.</p>
<p>这里针对四种consumer实现的差异进行说明:</p>
<ul>
<li>ConsumerImpl
<ul>
<li>实例化最后, 会调用方法grabCnx, 触发了连接的初始化工作: 创建连接, 发送 CONNECT command消息, 发送SUBSCRIBE command消息, 发送Flow command消息, 最终实现注册和服务端消息的推送.</li>
<li>服务端推送的消息会放到 incomingMessages 队列.</li>
<li>消息通过显式的调用 internalReceive() 从incomingMessages队列中取消息进行消费.</li>
</ul>
</li>
<li>ZeroQueueConsumerImpl
<ul>
<li>继承ConsumerImpl进行实现, 但是重写了方法 canEnqueueMessage() 和 internalReceive() 以及其他方法 (其他的过于琐碎,不进行讨论)</li>
<li>重写 internalReceive(), 每次调用会清空 incomingMessages, 没有及时处理的消息就没有了. 然后主动Flow command 请求一条消息, 等待获取消息进行处理</li>
<li>重写 canEnqueueMessage(), 当 listener 方式处理消息, 直接回调, 那么, 消息就不会丢失</li>
</ul>
</li>
<li>MultiTopicsConsumerImpl
<ul>
<li>多个consumer的集合.</li>
<li>初始化的时候, 就会直接启动消息的接收.</li>
</ul>
</li>
<li>PatternMultiTopicsConsumerImpl
<ul>
<li>继承 MultiTopicsConsumerImpl, 定时监听topic的变化,进行sub/unsub</li>
</ul>
</li>
</ul>
<h2 id="主要方法分析">主要方法分析</h2>
<h3 id="subscribe">subscribe</h3>
<pre><code>调用链: ConsumerBuilderImpl#subscribe -&gt; PulsarClientImpl#subscribeAsync -&gt; 分类讨论
</code></pre><ol>
<li>
<p>pattern模式</p>
<p>获取namespace下的所有topic, 然后进行过滤, 实例化 PatternMultiTopicsConsumerImpl</p>
</li>
<li>
<p>单topic模式</p>
</li>
</ol>
<pre><code>调用链: PulsarClientImpl#singleTopicSubscribeAsync -&gt; #doSingleTopicSubscribeAsync -&gt; ConsumerImpl#newConsumerImpl / MultiTopicsConsumerImpl#createPartitionedConsumer
</code></pre><p>单topic中, 根据 topic partition &gt; 1 ? MultiTopicsConsumerImpl : ConsumerImpl 进行处理, 在 MultiTopicsConsumerImpl 实现中, 每个 topic partition 都是 一个topic, 会生成一个 ZeroQueueConsumerImpl/ConsumerImpl, 并进行 Flow协议进行消息接收, 当消息到来的时候, 会将消息放到 incomingMessages队列, 等待 receive或者 listener函数异步处理. 而在 ConsumerImpl实现中, 实例化后不做任何处理</p>
<ol start="3">
<li>
<p>多topic模式</p>
<p>实例化 MultiTopicsConsumerImpl</p>
</li>
</ol>
<h3 id="receive">receive</h3>
<p>通过上面的consumer底层分析, 可以知道, receive 其实是通过 queue 传递给上层获取的. 不做赘述.</p>
<h2 id="总结">总结</h2>
<ol>
<li>无</li>
</ol>
<h2 id="不足">不足</h2>
<ol>
<li>ZeroQueueConsumerImpl 实现不够简洁, 继承ConsumerImpl,导致很多多余的逻辑. 尤其是 incomingMessages 队列, 在 ZeroQueueConsumerImpl 是多余的存储</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Broker Consumer Proto</title>
            <link>https://xujianhai.fun/posts/pulsar-broker-consumer-proto/</link>
            <pubDate>Sat, 02 Mar 2019 21:40:57 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-broker-consumer-proto/</guid>
            <description>Debug 1.环境准备: 在 org.apache.pulsar.common.api.PulsarDecoder#channelRead 中对每一个 case 打点, 启动debug.
2.terminal req:
 先发送produce请求, 积累数据, 同时避免 consumer污染debug流程 bin/pulsar-client produce my-topic &amp;ndash;messages &amp;ldquo;hello-pulsar1&amp;rdquo; bin/pulsar-client produce my-topic &amp;ndash;messages &amp;ldquo;hello-pulsar2&amp;rdquo; 发送consume请求, 关注调试 bin/pulsar-client consume my-topic -t Shared -s demo-sub2 -n 0  服务端的交互 经过debug, 发现consumer主要通过一下几个流程:
CONNECT PARTITIONED_METADATA LOOKUP CONNECT SUBSCRIBE FLOW ACK CLOASE_CONSUMER 通过代码, 进行分析如下: (ps: 箭头表示调用关系)
1.CONNECT: lookup服务连接, 进行权限校验
2.PARTITIONED_METADATA: 获取topic的partition数量
3.CONNECT: consumer连接, 进行权限校验
4.LOOKUP: 获取topicName的broker地址
5.SUBSCRIBE: 订阅主题
ServerCnx#handleSubscribe -&amp;gt; PersistentTopic#subscribe -&amp;gt; PersistentSubscription#addConsumer -&amp;gt; PersistentDispatcherMultipleConsumers#addConsumer 6.</description>
            <content type="html"><![CDATA[<h2 id="debug">Debug</h2>
<p>1.环境准备:
在 org.apache.pulsar.common.api.PulsarDecoder#channelRead 中对每一个 case 打点, 启动<a href="/post/pulsar-debug">debug</a>.</p>
<p>2.terminal req:</p>
<ul>
<li>先发送produce请求, 积累数据, 同时避免 consumer污染debug流程
bin/pulsar-client produce my-topic &ndash;messages &ldquo;hello-pulsar1&rdquo;
bin/pulsar-client produce my-topic &ndash;messages &ldquo;hello-pulsar2&rdquo;</li>
<li>发送consume请求, 关注调试
bin/pulsar-client consume  my-topic  -t Shared -s demo-sub2 -n 0</li>
</ul>
<h2 id="服务端的交互">服务端的交互</h2>
<p>经过debug, 发现consumer主要通过一下几个流程:</p>
<pre><code>CONNECT  PARTITIONED_METADATA  LOOKUP CONNECT SUBSCRIBE FLOW ACK CLOASE_CONSUMER
</code></pre><p>通过代码, 进行分析如下: (ps: 箭头表示调用关系)</p>
<p>1.CONNECT: lookup服务连接, 进行权限校验</p>
<p>2.PARTITIONED_METADATA: 获取topic的partition数量</p>
<p>3.CONNECT: consumer连接, 进行权限校验</p>
<p>4.LOOKUP: 获取topicName的broker地址</p>
<p>5.SUBSCRIBE: 订阅主题</p>
<pre><code>ServerCnx#handleSubscribe -&gt; PersistentTopic#subscribe -&gt; PersistentSubscription#addConsumer -&gt; PersistentDispatcherMultipleConsumers#addConsumer 
</code></pre><p>6.FLOW: 消费消息</p>
<pre><code>ServerCnx#handleFlow -&gt; Consumer#flowPermits -&gt; PersistentSubscription#consumerFlow -&gt; PersistentDispatcherMultipleConsumers#consumerFlow...readMoreEntries -&gt; ManagedCursorImpl#asyncReadEntriesOrWait...#asyncReadEntries -&gt; ManagedLedgerImpl#asyncReadEntries... -&gt;  EntryCacheImpl#asyncReadEntry -&gt; bookkeeper#ReadHandle.. -&gt; 层层callback -&gt; OpReadEntry#readEntriesComplete...#checkReadCompletion -&gt;PersistentDispatcherMultipleConsumers#readEntriesComplete-&gt; Consumer#sendMessages
</code></pre><p>7.ACK: ack消息</p>
<pre><code>Consumer.messageAcked  -&gt; PersistentSubscription.acknowledgeMessage -&gt; ManagedCursorImpl.asyncDelete -&gt; LegerHandle.asyncAddEntry 
</code></pre><p>8.CLOASE_CONSUMER: 关闭consumer</p>
<pre><code>handleCloseConsumer -&gt; 从consumers中删除, Consumer.close -&gt; PersistentSubscription.removeConsumer -&gt; XXXDispatcher.removeConsumer 
</code></pre><h2 id="细节分析">细节分析</h2>
<p>从pulsar的<a href="https://pulsar.apache.org/docs/en/concepts-messaging/">订阅模式</a>中得知, 订阅模式有 exclusive, shared, and failover三种. 上面的流程主要针对 shared mode的流程分析, 不同订阅模式的区别在于4、5、6,这里统一展开:</p>
<ol>
<li>exclusive
只允许一个Consumer订阅, 使用 ActiveConsumer 进行存储, 分发的时候 也只分发给ActiveConsumer. 参照类 org.apache.pulsar.broker.service.persistent.PersistentDispatcherSingleActiveConsumer.</li>
<li>failover
允许多个Consumer订阅, 但是只有一个 Active Consumer, 整体处理逻辑和 exclusive mode类似, 使用的也是同一个类: org.apache.pulsar.broker.service.persistent.PersistentDispatcherSingleActiveConsumer. 但是多了一个 active consumer 的选择逻辑:
参照类 AbstractDispatcherSingleActiveConsumer#pickAndScheduleActiveConsumer</li>
</ol>
<pre><code>protected boolean pickAndScheduleActiveConsumer() {
        ....
        int index = partitionIndex % consumers.size(); 
        Consumer prevConsumer = ACTIVE_CONSUMER_UPDATER.getAndSet(this, consumers.get(index));

        Consumer activeConsumer = ACTIVE_CONSUMER_UPDATER.get(this);
        ....
}
</code></pre><p>如果只是单独的topic, 进行 failover mode, 那么, index只可能等于0, 这段代码多余, 但是如果是 partitioned topic consumer, 那么, consumer数量 &lt; partition index, consumer 不断增加, 会导致 active consumer 不断变化, 实现中有延迟通知active consumer的实现.
3. shared
允许多个Consumer订阅, topic消息不顺序，那么, 消息是怎么分配到 consumer上的呢? 当消息需要分派的时候, 会调用方法 getNextConsumer, 如下: PersistentDispatcherMultipleConsumers#readEntriesComplete</p>
<pre><code>@Override
public synchronized void readEntriesComplete(List&lt;Entry&gt; entries, Object ctx) {
....
while (entriesToDispatch &gt; 0 &amp;&amp; totalAvailablePermits &gt; 0 &amp;&amp; isAtleastOneConsumerAvailable()) {
            Consumer c = getNextConsumer();
            ....
            if (messagesForC &gt; 0) {
                ....
                SendMessageInfo sentMsgInfo = c.sendMessages(entries.subList(start, start + messagesForC));
                ....
            }
            ....
}
....  
}
</code></pre><h2 id="faq">faq</h2>
<ol>
<li>什么时候会触发rebalance逻辑? rebalance逻辑是怎么实现的?</li>
</ol>
<ul>
<li>broker socket关闭的时候, 会触发 producer/consumer 都关闭.</li>
<li>心跳超时, 会将consumer进行剔除, failover 会触发 consumer选举, shared模式也是.</li>
</ul>
<ol start="2">
<li>为什么 CONNECT  两次？
<ul>
<li>答案和<a href="/post/producer-broker-producer-proto">之前</a>一样, consumer先和配置文件中brokerUrl进行连接, 获取到了topic的地址后, 连接到topic owner broker, 如果订阅了多个topic, 就会有相应的多个CONNECT 和SUBSCRIBE协议请求。</li>
</ul>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Debug</title>
            <link>https://xujianhai.fun/posts/pulsar-debug/</link>
            <pubDate>Sat, 02 Mar 2019 17:22:50 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-debug/</guid>
            <description> 首先, 在 ~/.m2/settings.xml中添加远程仓库配置:   &amp;lt;repositories&amp;gt; ... &amp;lt;repository&amp;gt; &amp;lt;id&amp;gt;central&amp;lt;/id&amp;gt; &amp;lt;layout&amp;gt;default&amp;lt;/layout&amp;gt; &amp;lt;url&amp;gt;https://repo1.maven.org/maven2&amp;lt;/url&amp;gt; &amp;lt;/repository&amp;gt; &amp;lt;repository&amp;gt; &amp;lt;snapshots&amp;gt; &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt; &amp;lt;/snapshots&amp;gt; &amp;lt;id&amp;gt;bintray-yahoo-maven&amp;lt;/id&amp;gt; &amp;lt;name&amp;gt;bintray&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://yahoo.bintray.com/maven&amp;lt;/url&amp;gt; &amp;lt;/repository&amp;gt; .... &amp;lt;/repositories&amp;gt; 这里使用Intellij进行调试. 相关配置:   启动类: org.apache.pulsar.PulsarStandaloneStarter 启动参数: &amp;ndash;config conf/standalone.conf &amp;ndash;no-functions-worker   intellij 启动成功后, 本地使用 consumer&amp;amp;producer测试.
  启动consumer: bin/pulsar-client consume my-topic -t Shared -s demo-sub2 -n 1
  启动produer: bin/pulsar-client produce my-topic &amp;ndash;messages &amp;ldquo;hello-pulsar&amp;rdquo; producer启动成功后, consumer控制面上会有 消息提示.
----- got message ----- hello-pulsar     </description>
            <content type="html"><![CDATA[<ol>
<li>首先, 在 ~/.m2/settings.xml中添加远程仓库配置:</li>
</ol>
<pre><code> &lt;repositories&gt;
 ...
        &lt;repository&gt;
          &lt;id&gt;central&lt;/id&gt;
          &lt;layout&gt;default&lt;/layout&gt;
          &lt;url&gt;https://repo1.maven.org/maven2&lt;/url&gt;
        &lt;/repository&gt;
        &lt;repository&gt;
          &lt;snapshots&gt;
            &lt;enabled&gt;false&lt;/enabled&gt;
          &lt;/snapshots&gt;
          &lt;id&gt;bintray-yahoo-maven&lt;/id&gt;
          &lt;name&gt;bintray&lt;/name&gt;
          &lt;url&gt;https://yahoo.bintray.com/maven&lt;/url&gt;
        &lt;/repository&gt;
....
&lt;/repositories&gt;
</code></pre><ol start="2">
<li>这里使用Intellij进行调试. 相关配置:</li>
</ol>
<ul>
<li>启动类: org.apache.pulsar.PulsarStandaloneStarter</li>
<li>启动参数:  &ndash;config conf/standalone.conf &ndash;no-functions-worker</li>
</ul>
<ol start="3">
<li>
<p>intellij 启动成功后, 本地使用 consumer&amp;producer测试.</p>
<ul>
<li>
<p>启动consumer: bin/pulsar-client consume  my-topic  -t Shared -s demo-sub2 -n 1</p>
</li>
<li>
<p>启动produer: bin/pulsar-client produce my-topic &ndash;messages &ldquo;hello-pulsar&rdquo;
producer启动成功后, consumer控制面上会有 消息提示.</p>
<pre><code>----- got message -----
hello-pulsar
</code></pre></li>
</ul>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Pulsar Intro</title>
            <link>https://xujianhai.fun/posts/pulsar-intro/</link>
            <pubDate>Sat, 02 Mar 2019 17:06:25 +0800</pubDate>
            
            <guid>https://xujianhai.fun/posts/pulsar-intro/</guid>
            <description>pulsar是 yahoo开发并开源的 分布式 pub-sub消息. 相比于 kafka/rocketmq, 最大的差异就是 存储上的不同. kafka 和 rocketmq 存储在 本地机器上, pulsar却是存储在 bookkeeper, 一种容错、低延迟、容错的存储服务, 专门为 append-only 类型进行优化. 这样的做法，可以实现存储层和计算层隔离, Broker 动态变化不依赖于数据的迁移，方便了扩容缩容.
pulsar实现上的主要特点:
 消息内容的数据写入到 bookkeeper. 封装的数据结构: ManagedLedgerImpl 消费的cursor 写入到 bookkeeper. 封装的数据结构: ManagedCursorImpl 消费的订阅关系 维护在 zk中. 封装的数据结构: MetaStoreImplZookeeper consumer类型: Exclusive/Failover 主要处理逻辑封装在 AbstractDispatcherSingleActiveConsumer, Shared 封装逻辑主要在 PersistentDispatcherMultipleConsumers. 支持partitioned topic 基于Netty的网络处理逻辑.  </description>
            <content type="html"><![CDATA[<p>pulsar是 yahoo开发并开源的 分布式 pub-sub消息. 相比于 kafka/rocketmq, 最大的差异就是 存储上的不同. kafka 和 rocketmq 存储在 本地机器上, pulsar却是存储在 <a href="https://github.com/apache/bookkeeper">bookkeeper</a>, 一种容错、低延迟、容错的存储服务, 专门为 append-only 类型进行优化. 这样的做法，可以实现存储层和计算层隔离, Broker 动态变化不依赖于数据的迁移，方便了扩容缩容.</p>
<p>pulsar实现上的主要特点:</p>
<ul>
<li>消息内容的数据写入到 bookkeeper. 封装的数据结构: ManagedLedgerImpl</li>
<li>消费的cursor 写入到 bookkeeper. 封装的数据结构: ManagedCursorImpl</li>
<li>消费的订阅关系 维护在 zk中. 封装的数据结构: MetaStoreImplZookeeper
consumer类型: Exclusive/Failover 主要处理逻辑封装在 AbstractDispatcherSingleActiveConsumer, Shared 封装逻辑主要在 PersistentDispatcherMultipleConsumers.</li>
<li>支持partitioned topic</li>
<li>基于Netty的网络处理逻辑.</li>
</ul>
]]></content>
        </item>
        
    </channel>
</rss>
