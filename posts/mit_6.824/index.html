<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Zero xu ">
<meta name="description" content="序 记录学习 mit 6.824 课程的经历
MapReduce 1 目标: 将任务拆解成map 和 reduce 两个阶段, 进行 大规模的数据处理, 比如 页面爬取、词频统计
模型 如上图, map/reduce 架构会将用户输入切成若干份数据输入(map的个数), 由map进行处理, 按照论文的说法, map读取文件是本地读取操作, map计算后得到的结果, 会按照 hash到若干份(reduce个数)本地文件存储, 并将存储位置上报给 master, master启动reduce worker, reduce worker会远程获取 每个map机器上的文件, 本地计算后输出到 gfs(分布式文件存储)上. 为了更好的性能和效果, 在map输出后以及reduce输入前, 会有一个combiner任务, 对map的结果进行预处理, 减少网络传输, 但是和reduce不同, combiner 的输出结果是存储在磁盘上的.
分布式场景下, 容易出现一些坏的机器导致map/reduce 执行慢, 对此, map/reduce 架构会重新执行任务. 对于执行完宕机的场景, map会触发重新执行(结果存放在本地), reduce 不需要重新执行(结果存放在gfs)
map/reduce的场景中, 需要处理 热点倾斜的问题, 因为会出现大量数据集中在一台reduce机器上, 对于这种问题, 需要自定义良好的 partition 函数, 将数据尽可能的平均打散
hadoop 架构
将论文中 partition/combiner 的抽象成 shuffle, 进行分区、排序、分割, 将属于同一划分（分区）的输出合并在一起并写在磁盘上，最终得到一个分区有序的文件." />
<meta name="keywords" content="homepage, blog, science, informatics, development, programming, mit" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://xujianhai.fun/posts/mit_6.824/" />


    <title>
        
            Mit_6 :: zero.xu blog  — Hello Friend
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://xujianhai.fun/main.min.5dcefbf8102eb536dd3e2de53ffebfa58599ab2435c241a0db81728a5e015f2e.css">




<meta itemprop="name" content="Mit_6">
<meta itemprop="description" content="序 记录学习 mit 6.824 课程的经历
MapReduce 1 目标: 将任务拆解成map 和 reduce 两个阶段, 进行 大规模的数据处理, 比如 页面爬取、词频统计
模型 如上图, map/reduce 架构会将用户输入切成若干份数据输入(map的个数), 由map进行处理, 按照论文的说法, map读取文件是本地读取操作, map计算后得到的结果, 会按照 hash到若干份(reduce个数)本地文件存储, 并将存储位置上报给 master, master启动reduce worker, reduce worker会远程获取 每个map机器上的文件, 本地计算后输出到 gfs(分布式文件存储)上. 为了更好的性能和效果, 在map输出后以及reduce输入前, 会有一个combiner任务, 对map的结果进行预处理, 减少网络传输, 但是和reduce不同, combiner 的输出结果是存储在磁盘上的.
分布式场景下, 容易出现一些坏的机器导致map/reduce 执行慢, 对此, map/reduce 架构会重新执行任务. 对于执行完宕机的场景, map会触发重新执行(结果存放在本地), reduce 不需要重新执行(结果存放在gfs)
map/reduce的场景中, 需要处理 热点倾斜的问题, 因为会出现大量数据集中在一台reduce机器上, 对于这种问题, 需要自定义良好的 partition 函数, 将数据尽可能的平均打散
hadoop 架构
将论文中 partition/combiner 的抽象成 shuffle, 进行分区、排序、分割, 将属于同一划分（分区）的输出合并在一起并写在磁盘上，最终得到一个分区有序的文件.">
<meta itemprop="datePublished" content="2020-06-06T10:23:10&#43;08:00" />
<meta itemprop="dateModified" content="2020-06-06T10:23:10&#43;08:00" />
<meta itemprop="wordCount" content="1008">
<meta itemprop="image" content="https://xujianhai.fun/"/>



<meta itemprop="keywords" content="mit," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://xujianhai.fun/"/>

<meta name="twitter:title" content="Mit_6"/>
<meta name="twitter:description" content="序 记录学习 mit 6.824 课程的经历
MapReduce 1 目标: 将任务拆解成map 和 reduce 两个阶段, 进行 大规模的数据处理, 比如 页面爬取、词频统计
模型 如上图, map/reduce 架构会将用户输入切成若干份数据输入(map的个数), 由map进行处理, 按照论文的说法, map读取文件是本地读取操作, map计算后得到的结果, 会按照 hash到若干份(reduce个数)本地文件存储, 并将存储位置上报给 master, master启动reduce worker, reduce worker会远程获取 每个map机器上的文件, 本地计算后输出到 gfs(分布式文件存储)上. 为了更好的性能和效果, 在map输出后以及reduce输入前, 会有一个combiner任务, 对map的结果进行预处理, 减少网络传输, 但是和reduce不同, combiner 的输出结果是存储在磁盘上的.
分布式场景下, 容易出现一些坏的机器导致map/reduce 执行慢, 对此, map/reduce 架构会重新执行任务. 对于执行完宕机的场景, map会触发重新执行(结果存放在本地), reduce 不需要重新执行(结果存放在gfs)
map/reduce的场景中, 需要处理 热点倾斜的问题, 因为会出现大量数据集中在一台reduce机器上, 对于这种问题, 需要自定义良好的 partition 函数, 将数据尽可能的平均打散
hadoop 架构
将论文中 partition/combiner 的抽象成 shuffle, 进行分区、排序、分割, 将属于同一划分（分区）的输出合并在一起并写在磁盘上，最终得到一个分区有序的文件."/>





    <meta property="article:published_time" content="2020-06-06 10:23:10 &#43;0800 CST" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://xujianhai.fun/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">zero.xu|blog</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://xujianhai.fun/about/">About</a></li><li><a href="https://xujianhai.fun/posts/">Posts</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>5 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://xujianhai.fun/posts/mit_6.824/">Mit_6</a>
            </h1>

            

            <div class="post-content">
                <h2 id="序">序</h2>
<p>记录学习 mit 6.824 课程的经历</p>
<h2 id="mapreduce-1">MapReduce <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h2>
<h3 id="目标">目标:</h3>
<p>将任务拆解成map 和 reduce 两个阶段, 进行 大规模的数据处理, 比如 页面爬取、词频统计</p>
<h3 id="模型">模型</h3>
<p><img src="https://xujianhai.fun/mapreduce.png" alt="mapreduce" title=""></p>
<p>如上图, map/reduce 架构会将用户输入切成若干份数据输入(map的个数), 由map进行处理, 按照论文的说法, map读取文件是本地读取操作, map计算后得到的结果, 会按照 hash到若干份(reduce个数)本地文件存储, 并将存储位置上报给 master, master启动reduce worker, reduce worker会远程获取 每个map机器上的文件, 本地计算后输出到 gfs(分布式文件存储)上. 为了更好的性能和效果, 在map输出后以及reduce输入前, 会有一个combiner任务, 对map的结果进行预处理, 减少网络传输, 但是和reduce不同, combiner 的输出结果是存储在磁盘上的.</p>
<p>分布式场景下, 容易出现一些坏的机器导致map/reduce 执行慢, 对此, map/reduce 架构会重新执行任务. 对于执行完宕机的场景, map会触发重新执行(结果存放在本地), reduce 不需要重新执行(结果存放在gfs)</p>
<p>map/reduce的场景中, 需要处理 热点倾斜的问题, 因为会出现大量数据集中在一台reduce机器上, 对于这种问题, 需要自定义良好的 partition 函数, 将数据尽可能的平均打散</p>
<h3 id="hadoop">hadoop</h3>
<p>架构</p>
<p>将论文中 partition/combiner 的抽象成 shuffle, 进行分区、排序、分割, 将属于同一划分（分区）的输出合并在一起并写在磁盘上，最终得到一个分区有序的文件.</p>
<h3 id="问题">问题</h3>
<ol>
<li>难以实时计算(处理存储在磁盘上的离线数据)</li>
<li>不能流式计算(处理的数据源是静态的文件, 流式计算处理的数据是动态的, storm/spark)</li>
<li>难以 DAG(任务输入和输出依赖的有向无环图, mapreduce的计算结果都会写入磁盘, 会造成大量频繁io, 性能低下, spark)</li>
</ol>
<h2 id="gfs--2">GFS  <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></h2>
<h3 id="架构">架构</h3>
<p><img src="https://xujianhai.fun/gfs_arch.png" alt="arch" title=""></p>
<p>根据论文的描述, 核心组件是 master 和 chunkserver, master 负责管理chunk replica(创建、替换、复制、负载均衡, 通过chunk version number区分最新的chunk)、chunk 选主(lease机制)、GC(通过心跳进行懒删除文件)、文件管理(目录的形式组织文件), chunkserver 负责存储 chunk, chunk是存储的最小单位, 一个文件会被划分成多个chunk, chunk默认是 64MB.</p>
<p>master的数据特点: 通过operation log 记录所有请求操作, 并定期将内存状态存储到checkpoint, 加快恢复时间.
chunkserver的数据特点: chunk内部会划分成 64KB 的 block, 每个block做checksum校验, 避免损坏的数据.</p>
<p>在高可用方面, master 也是多副本的, 不过以standby的形式存在, 当master挂了, 通过DNS切换通知客户端. chunkserver本身的多副本机制确保了高可用.</p>
<p>需要注意的是, master 持久化的元数据并不包括 chunk-&gt;chunkservers 的映射, 这个是通过master-chunkserver 之间的心跳上报实现的. (rocketmq中topic-&gt;brokers的映射也是这样, kafka则将这些信息维护在了zk上)</p>
<h3 id="写入">写入</h3>
<p><img src="https://xujianhai.fun/gfs_write.png" alt="write" title=""></p>
<p>写入比较特殊, 根据论文, 写入其实是个两阶段提交, 如图所示, 首先, 客户端从master获取到元数据(primary+secondary replica)并缓存起来, 然后客户端向所有的chunkserver发送写请求, chunkserver 将请求写入到自己的LRU缓存(注意这里没落盘), 确保全部写入成功后(失败会重试), client 请求primary提交数据, primary 会分配一个连续的序列号, 然后将将之前的修改存储起来, 然后primary请求所有secondary进行落盘, 如果部分replica落盘失败, primary返回错误给client, client会重试保证最终成功避免数据的不一致.</p>
<p>但是我比较疑惑的是, 写入是指写入到指定位置(offset) 还是指定的chunk 吗? append 的语意比较清晰, 就是追加到文件末尾.</p>
<p>append语意比write略微复杂, append 需要保证写入的chunk有足够的空间容纳数据, 否则会对当前chunk进行补齐(primary会同步secondary进行同样的操作), 然后client重试下一个chunk.</p>
<p>gfs并没有支持更新.</p>
<p>需要注意的时候, 数据的一致性完全依赖客户端的重试. 这样选主之后, 即使primary不是最新的, 也可以通过重试补齐. 有些暴力, 因为很多业务并不需要全部成功, 可以容忍失败, 希望尽快的返回错误, 因此现在基本上使用 raft 的方式同步, 但是相比retry, 复杂了些, 而且raft 至少需要三副本.</p>
<h3 id="hdfs">hdfs</h3>
<p>架构</p>
<p>开源有hdfs, hdfs存在以下问题:</p>
<ol>
<li>不适合低延迟数据访问, hdfs 的设计目标是高吞吐量</li>
<li>无法存储大量小文件 (参考facebook hystack)</li>
<li>不支持多用户写入和随机修改 (hdfs的一个文件只有一个写入者, 并且是追加写)</li>
</ol>
<h3 id="hbase">hbase</h3>
<h3 id="新版cfs">新版cfs</h3>
<p>google 对gfs做了一次升级, 降低master单点故障, 但是思路很神奇, 最终是将元数据存储在chubby+bigtable上. 更多参考</p>
<ul>
<li><a href="https://levy.at/blog/22">https://levy.at/blog/22</a></li>
</ul>
<h2 id="primary-backup--3">primary-backup  <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></h2>
<p>根据论文内容, 这个思路是基于虚拟机的, 通过底层 hypervisor进行数据的同步, 如下:</p>
<p><img src="https://xujianhai.fun/mit-backup.png" alt="backup" title=""></p>
<p>primary会通过 log channel 将数据同步给 backup, backup receive之后primary才会响应给client, 这里存在几个问题:</p>
<ol>
<li>logchannel 同步带宽</li>
<li>backup receive 速度, 太慢影响primary的处理响应, 太快backup来不及处理导致lag增大(这样backup成为leader后, 需要先消费完这些数据才能成为leader, 灾备时间被延迟了)</li>
<li>非确定性的事件太多, 因为是机器级别的复制, 存在计时等干扰的非确定性因素同步</li>
<li>引入了 共享磁盘/非共享磁盘的 复杂度. 磁盘用来脑裂</li>
<li>存在重复输出和丢失输出(告诉客户端存在, 实际上backup并没与同步到)</li>
</ol>
<p>参考</p>
<ul>
<li><a href="https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf">https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf</a></li>
</ul>
<h2 id="raft-4">raft <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></h2>
<p>以可理解性为目标, 解耦了 leader election、log replication、safety 三个方面, 如下:</p>
<ul>
<li>strong leader: 写从leader流向 follower</li>
<li>leader election: 随机timer</li>
<li>membership change: joint consensus/ 联合一致性</li>
</ul>
<p>基于 有序的replicated log 的复制状态机(replicated state machine). 一致性模块 确保 replicated log的一致性. 安全(非拜占庭问题下)、可用、不依赖时间. 更多的参考之前的学习: <a href="https://xujianhai.fun/posts/raft-paper">paper</a></p>
<p>在实践上, 建议参考 <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<p>进一步阅读:
Oki and Liskov’s Viewstamped Replication [29, 22]
chubby
spanner</p>
<h2 id="zookeeper-6">zookeeper <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></h2>
<p>目标: 提供构建复杂协调原语的简单、高性能的内核</p>
<p>这里的论文<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> 只是讲述了 zookeeper基于zab构建的协调服务. 对外提供的语意: lock,read/write lock, double barier, leader election, group membership, configuraiton management, zookeeper 抽象了znode存储数据(最大1MB), 参照 文件系统的 层级结构 组织znode. 并提供了 Regular 和 Ephemeral 两种 znode. 文章中重点讲述了 zookeeper 对外的 client api: create/delete/exists/get/getdata/setdata/getChildren/sync, 其中, getData、exits、getChildren 提供了 watch 模式, 用于当数据变更的时候进行通知, 但是需要注意的是, 数据获取的这几种方式并不是连接leader获取最新的数据, 默认是获取任意server本地的数据, 这样虽然提供了并发性能, 但是存在”stale data“/过期数据的问题,  支持如果想获取最新数据, zookeeper 提供了 sync 的api, 在调用read之前调用sync, sync会保证将之前pending write执行完, 这样再read就是最新的数据了. 按照论文的说法, sync+leader 应该是在leader上的, 根据读操作 FIFO 的特性, sync就不需要 zab原子广播, 只需要本地前面的操作同步完就可以了, 但是需要确保leader还持有租约, zookeeper 的解决方案比较直接, 通过前一个zab广播协议进行leader确认, 如果不是leader的话广播就会失败, 这里存在一个特别点, 如果sync+read之前有写操作, 直接依赖之前的写操作的广播协议, 如果没有写操作, 则需要一个 <code>null transaction</code>的原子广播执行.</p>
<p>这里谈到了读操作, 其实zookeeper提供的操作是 线性写、FIFO read, 线性写必须是通过leader执行, 通过zab原子广播到其他节点, 因此节点越多写操作越慢. 在内部存储上, zookeeper 用内存存储状态, replay log(wal) 存储提交的操作, 并且周期性的生成快照. 在快照这一块, zookeeper 并没有使用锁, 而是深度遍历生成快照, 也就意味着快照内数据的时间点是不一致的, 但是因为数据的状态改变是幂等的, 因此可以重复apply.</p>
<p>和之前的论文不同的地方是, 这篇论文还重点举例了 zookeeper的几个应用: fetching service、indexer service、message broker.</p>
<p>奇怪的是, 这篇文章并没有讲述 zab 协议的具体细节.</p>
<p>异步 线性 zookeeper</p>
<p><a href="https://www.cnblogs.com/yeyang/p/11420920.html">https://www.cnblogs.com/yeyang/p/11420920.html</a> chubby 有论文吗？</p>
<p>其他参考阅读(TODO):</p>
<ul>
<li>zab 论文: <a href="http://dl.acm.org/citation.cfm?id=2056409">http://dl.acm.org/citation.cfm?id=2056409</a> 或者 <a href="https://github.com/jayknoxqu/pasox_zab_protocol/blob/master/2012-deSouzaMedeiros.pdf">https://github.com/jayknoxqu/pasox_zab_protocol/blob/master/2012-deSouzaMedeiros.pdf</a> 和 <a href="https://distributedalgorithm.wordpress.com/2015/06/20/architecture-of-zab-zookeeper-atomic-broadcast-protocol/">https://distributedalgorithm.wordpress.com/2015/06/20/architecture-of-zab-zookeeper-atomic-broadcast-protocol/</a></li>
<li>线性和时序: <a href="http://www.bailis.org/blog/linearizability-versus-serializability/">http://www.bailis.org/blog/linearizability-versus-serializability/</a></li>
<li>wait-free定义参考: <a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf">https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf</a></li>
<li>zk集群动态配置: <a href="https://zookeeper.apache.org/doc/r3.5.3-beta/zookeeperReconfig.html">https://zookeeper.apache.org/doc/r3.5.3-beta/zookeeperReconfig.html</a></li>
<li>zk集群动态配置: <a href="https://www.usenix.org/system/files/conference/atc12/atc12-final74.pdf">https://www.usenix.org/system/files/conference/atc12/atc12-final74.pdf</a></li>
<li>zab 和 paxos 的思考: <a href="https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab+vs.+Paxos">https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab+vs.+Paxos</a></li>
</ul>
<h2 id="craq7">CRAQ<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></h2>
<p>架构参考:</p>
<p><img src="https://xujianhai.fun/chain-replica.png" alt="chain-replica" title=""></p>
<p>全称是: Chain Replication with Apportioned Queries: 分摊查询的级联复制, 相比CR, 通过分摊查询压力到其他节点提升了整个架构的读吞吐量. 传统的CR模型, head负责write, tail负责read, 写入从head不断下沉到tail, tail 是整个commit的保证, tail commit之后逐级反馈到head, 然后head发送响应到client. 因为tail 是最终commit的保证, 因此从tail读取肯定是已经提交的最新的数据. 那么 CRAQ 模型将读取压力分摊到其他节点, 怎么保证读取的是最新的数据呢？ 根据初始的CQ模型, write是级联下沉的, 当节点收到write但是还没收到commit, 那么就会标记数据为 dirty, 当client read定向到这个节点, 这个节点会请求 tail 节点获取最新的数据, 然后相应给客户端; 如果被请求的节点没有标记dirty, 那么节点直接返回数据. 这样就避免了<code>stale read</code>.</p>
<p>适配全球化进程, craq 还支持了 多数据中心、多Chain, 但是和raft不一样, 本身不解决协调问题, 而是依赖zk解决 group membership(成员管理)、元数据存储、以及节点变更通知. 但是论文并没有讨论相关细节. 论文中还谈到了 广播协议, 避免逐级传播的网络开销, 加快write和commit流程. 比较遗憾的是, 论文中提到 大对象的原子更新用小事务完成, 不过没有完成. 在节点变更上, 略微有些复杂, 甚至有些地方不是很理解, 实现的话需要认真思考下</p>
<p>在功能上, 除了 对象存储常规的 read/write, 还支持 prepend/append、incre/decr、test-and.</p>
<p>在压测结果上, 整个效果还是很理想的.</p>
<h2 id="aurora-8">AURORA <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></h2>
<p>参考论文中的良心架构:</p>
<p><img src="https://xujianhai.fun/aurora.png" alt="aurora" title=""></p>
<p>aurora 是 amazon 提供的 OLAP关系型数据库, 在mysql 5.6 分支上改造, 将存储层和计算层进行了分离. aurora 重点在于 <code>the log is the database</code>, 通过同步虚拟分段的redo log 进行存储层之间的数据同步, 减少mysql mirror模型下大量的数据同步, 需要注意的是, aurora 用的是6路复制(3个az, 每个az两个副本). 在一致性层面, 通过quorum达成了最终一致性, 并通过 gossip 进行数据的补齐. 本质上是个 一写多读 的方案. 比较特别的是, 还用了s3做备份</p>
<p>在架构上, 计算层是 Aurora MySQL/instance, 存储层是 Storage Node. 在工程实践上, 计算层只是简单的 fork 了 mysql源码并简单了修改.</p>
<p>分层的设计中, instance(database tire)负责: query processor, transactions,
locking, buffer cache, access methods and undo management; storage node负责: redo logging, durable storage, crash recovery, and backup/restore. 也就是将存储层进行了沉降.</p>
<p>写入流程:  storage node将接受到的 redo log record(这个是个啥?)添加到内存队列, 持久化到磁盘上后ack. 在apply的时候, 会检查是否存在lag, 如果存在lag,  则通过gossip协议进行补齐. 最终将记录放到新的数据页, 并周期性同步到s3, 并周期性的 gc老版本 和 校验 crc. 这里只有内存队列排队和ack 两个步骤是同步的, 其他是异步的, 所以性能会有所提升. 可以明显的发现, 这里调用写入和写入page其实是两个分离的行为, 那么是否存在着 客户端写入被ack了, 但是读取的时候可能读取到的是 <code>stale data</code>/过期的数据.</p>
<p>读流程: 根据分层设计, instance(database layer) 负责 buffer cache, 因此读取大部分情况下是读取 database layer, 只有当数据页找不到的时候, 才会产生 storage IO 请求. 因此缓存数据, 所以存在空间不够需要驱逐的场景, 传统数据库是刷新dirty page, 但是在 aurora, 则是刷新 <code>PAGE LSN</code> 大于vdl的page, 因为读取都是读取当前vdl的数据, 因此大于vdl的数据则是没有意义的. 问题, 如果都是小于等于vdl的, 怎么驱逐呢?  在read的时候, 使用 vdl 取代了读取快照作为 read point, 而不是通过read quorom来达成一致, database layer 是知道每个 segment 的 scl, 但是需要处理 segment 不能给出大于 read point的数据, 除此之外, database 可以计算出 Protection Group Min Read Point LSN(PGMRPL), 也就是read 请求的 read point 不可能小于 PGMRPL, 通过这个点, storage node 可以安全的回收</p>
<p>关于复制, 在传统的镜像复制/mysql mirror中, Primary/Backup Instance 之间复制需要 Log、Binlog、Data、FRMFiles(元数据), aurora 只需要同步 redo log 就可以了.</p>
<p>分段的设计中, database volumn 被分成 固定大小的(10GB)的 segment, 通过复制构成 pg(protection groups), 通过分段减少 mttr(平均恢复时间), 因为 mttf 是不可控制的.</p>
<p>更艳遇事务, 多个事务之间的干扰处理 以及 事务处理没怎么说. 但是只有一个 writer, 模型应该相对简单</p>
<p>对于恢复的处理, 常规的wal+快照, 同时也有undo log回滚未提交的日志. 也会增加一个 epoch 避免崩溃恢复被打断. 除此之外, instance(database layer) 需要重新构建运行时状态, 比如数据缓冲和一些元数据(vdl). 对于undo, aurora因为设计在database层, 所以 database 进行执行.</p>
<h3 id="几个概念">几个概念:</h3>
<ul>
<li>LSN: 日志序列号, 每个日志记录的唯一ID</li>
<li>SCL/Segment Complete LSN: segment PG已经收到的最大的LSN. 用来gossip阶段数据补齐的. 每个storage node 不一样. 读取的时候, 需要选择 scl &gt; read point 的.</li>
<li>VCL/Volumn Complete LSN: 存储层上之前的日志记录的&quot;可用&rdquo;, 在恢复期间, 大于VCL的日志记录都会被截断. 在恢复期间, storage server 上 已经被 quorum reads 存储的最大的LSN. 可能还未提交. Storage layer的概念</li>
<li>CPL/Cosistemcy Point LSN: database layer 的概念. 每个mtrs(mini transaction, 每个database layer的transaction会被分成多个mini transaction)产生的最后一个log record的LSN.</li>
<li>VDL/Volumn Durable LSN: database layer的概念. 最大的CPL. 恢复阶段通过read quorum确定. 小于 VCL. write流程中, database layer 收到 storage layer的ack的时候, 就会增长</li>
</ul>
<p>恢复的时候, database 和  storage沟通 创建每个pg的 durable point来创建 vdl.</p>
<h3 id="几个问题">几个问题:</h3>
<ul>
<li>mysql mirror 的模型: 暂时没找到资料, 论文中说 需要同步binlog redolog data 和 double write, 按照之前的经验, 我们使用的binlog同步复制策略并没与这么多数据需要同步. 不是很理解</li>
<li>redo log: 记录页的变更记录, innodb 的wal. 因此 ddl 变更 对应的redo log也是很大的</li>
<li>看样子底层存储使用的是 amazon block store,这个不就有一致性能力吗? 为什么还需要复制?</li>
<li>事务的处理: ? 仅仅说了在 database layer将事务拆分成多个 mtrs, 最终commit</li>
<li>事务隔离级别的保证: 支持可重复读</li>
<li>quorum需要保证写入失败的场景下, 通过gossip不会写入所有副本, 不然冗余数据被提交了呀.</li>
<li>storage tier 负责生成数据页, 那么有个问题, database tire/instance 没有数据有怎么生成 redo log 呢? 因此是 database layer 会从 storage node 捞取数据上去, 并且只有在 vdl</li>
<li>读取空间不足, 如果都是 page lsn &lt;= vdl, evit alg 怎么实践?</li>
<li>存在写入成功后, 不一定立即能够读取到, 因为ack 是先写入wal的, storage node可能还没有处理, 怎么解决的?</li>
</ul>
<p>更多参考:</p>
<ul>
<li><a href="https://www.percona.com/blog/2015/11/16/amazon-aurora-looking-deeper/">https://www.percona.com/blog/2015/11/16/amazon-aurora-looking-deeper/</a></li>
<li><a href="http://www.yunweipai.com/archives/18411.html">http://www.yunweipai.com/archives/18411.html</a></li>
</ul>
<h2 id="frangipani-9">Frangipani <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></h2>
<h2 id="参考">参考:</h2>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>作者: Jeffrey Dean, Sanjay Ghemawat: <a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>作者: Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: <a href="https://pdos.csail.mit.edu/6.824/papers/gfs.pdf">The Google File System</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>作者: Daniel J. Scales, Mike Nelson, and Ganesh Venkitachalam: <a href="https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf">The Design of a Practical System for Fault-Tolerant Virtual Machines</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>作者: Diego Ongaro, John Ousterhout: <a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf">In Search of an Understandable Consensus Algorithm (Extended Version)</a> <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>作者: Diego Ongar: <a href="https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf">CONSENSUS: BRIDGING THEORY AND PRACTICE</a> <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>作者: Patrick Hunt, Mahadev Konar, Flavio P. Junqueira, Benjamin Reed: <a href="https://pdos.csail.mit.edu/6.824/papers/zookeeper.pdf">ZooKeeper: Wait-free coordination for Internet-scale systems</a> <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>作者: Jeff Terrace, Michael J. Freedman:<a href="https://pdos.csail.mit.edu/6.824/papers/craq.pdf">Object Storage on CRAQ
High-throughput chain replication for read-mostly workloads</a> <a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>作者: Amazon Web Services: <a href="https://pdos.csail.mit.edu/6.824/papers/aurora.pdf">Amazon Aurora: Design Considerations for High
Throughput Cloud-Native Relational Databases</a> <a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>作者: Chandramohan A. Thekkath, Timothy Mann, Edward K. Lee: <a href="https://pdos.csail.mit.edu/6.824/papers/thekkath-frangipani.pdf">Frangipani: A Scalable Distributed File System</a> <a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://xujianhai.fun/tags/mit">mit</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>1008 Words</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2020-06-06 10:23 &#43;0800</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://xujianhai.fun/posts/python_tool/">
                                <span class="button__icon">←</span>
                                <span class="button__text">Python_tool</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://xujianhai.fun/posts/kafka_group_coordinator/">
                                <span class="button__text">Kafka_group_coordinator</span>
                                <span class="button__icon">→</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
                <span><a href="https://xujianhai.fun/">Zero xu</a></span>
            
            <span></span>
            <span> <a href="https://xujianhai.fun/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span>
            <span>Made with &#10084; by <a href="https://github.com/rhazdon">rhazdon</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="https://xujianhai.fun/bundle.min.2d5469329143160ae2456a69c3c76dc2d0a3b212b46afe291a51bd68650ed6f8697e001dab54f1c272c77ce08092a8c55e5bb4314e0ee334aab4b927ec896638.js" integrity="sha512-LVRpMpFDFgriRWppw8dtwtCjshK0av4pGlG9aGUO1vhpfgAdq1TxwnLHfOCAkqjFXlu0MU4O4zSqtLkn7IlmOA=="></script>



    </body>
</html>
